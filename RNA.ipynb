{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bPVTxb4akHi"
      },
      "source": [
        "**Vamos experimentar agora a Rede Neural Artificial?**\n",
        "Veja:\n",
        "https://scikit-learn.org/stable/modules/neural_networks_supervised.html# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "fpe0EYaXiIPm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install yellowbrick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ru9xg6QIaceV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mkdILomXTwn",
        "outputId": "e42868f6-592c-43b8-cbfa-bbea68845a8b"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "STeZ46Y4bKfl"
      },
      "outputs": [],
      "source": [
        "#novos pre-processamentos\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Carregando o dataset\n",
        "path = '/Users/caiolima/Documents/inteligência artificial/lista 7/breast-cancer.csv'\n",
        "dataFrame = pd.read_csv(path)\n",
        "\n",
        "# Separando características e rótulo\n",
        "X = dataFrame.iloc[:, :-1]\n",
        "y = dataFrame.iloc[:, -1]\n",
        "\n",
        "# Identificação e tratamento de outliers - exemplo usando Z-score\n",
        "from scipy import stats\n",
        "z_scores = np.abs(stats.zscore(X.select_dtypes(include=[np.number])))\n",
        "outlier_rows = np.where(z_scores > 3)[0]\n",
        "X = X.drop(outlier_rows)\n",
        "y = y.drop(outlier_rows)\n",
        "\n",
        "# Conversão de variáveis nominais para numéricas\n",
        "# Vamos assumir que X possui algumas colunas categóricas. Identifique essas colunas e as transforme.\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Criação de transformadores para colunas numéricas e categóricas\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Preenchimento de valores faltantes para numéricos\n",
        "    ('scaler', StandardScaler())  # Normalização de características numéricas\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Preenchimento de valores faltantes para categóricos\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Conversão de categóricos para numéricos\n",
        "])\n",
        "\n",
        "# Combinando os transformadores\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Aplicando o pré-processamento\n",
        "X = preprocessor.fit_transform(X)\n",
        "\n",
        "# Balanceamento do dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Eliminação de redundância (se necessário, usando por exemplo, eliminação recursiva de características)\n",
        "\n",
        "# Agora dividimos o conjunto de dados já pré-processado em treino e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bCxFBVNFt22"
      },
      "source": [
        "**Vamos treinar com a rede neural?**\n",
        "\n",
        "**Experimente a RNA com os parâmetros default. A rede convergiu? quantas épocas?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVW22XucaswH",
        "outputId": "2c08bca3-ed7c-4d92-c4a9-8030b70af513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier()"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definição do espaço de hiperparâmetros para a busca\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (20,), (30,), (10, 10), (20, 10), (30, 15)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],  # Regularização L2\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "}\n",
        "\n",
        "\n",
        "# Criando o objeto Grid Search com o modelo MLPClassifier e o espaço de hiperparâmetros\n",
        "grid_search = GridSearchCV(MLPClassifier(max_iter=2000), param_grid, n_jobs=-1, cv=3, scoring='accuracy')\n",
        "\n",
        "# Executando o Grid Search no conjunto de dados de treino\n",
        "grid_search.fit(X_treino, y_treino)\n",
        "\n",
        "# Imprimindo os melhores parâmetros encontrados\n",
        "print(\"Melhores parâmetros encontrados pelo Grid Search:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = MLPClassifier(\n",
        "    max_iter=2000,  # Aumentar se necessário\n",
        "    **best_params # Desempacota os melhores parâmetros encontrados\n",
        ")\n",
        "best_model.fit(X_treino, y_treino)\n",
        "\n",
        "# Avaliando o melhor modelo encontrado no conjunto de teste\n",
        "best_model = grid_search.best_estimator_\n",
        "previsoes = best_model.predict(X_teste)\n",
        "acuracia = accuracy_score(y_teste, previsoes)\n",
        "print(f\"Acurácia do melhor modelo: {acuracia}\")\n",
        "\n",
        "\n",
        "modelo = MLPClassifier(\n",
        "    max_iter=2000,  # Aumentando o número de iterações\n",
        "    learning_rate_init=0.005,  # Ajustando a taxa de aprendizado\n",
        "    solver='adam',  # Garantindo que estamos usando o otimizador 'adam'\n",
        "    verbose=True  # Para imprimir mensagens durante o treinamento\n",
        ")\n",
        "\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Yh14LUHfN-"
      },
      "source": [
        "**Depois execute novamente com os ajustes. Veja agora os erros a cada época.. estabeleça o verbose para true **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QRptikHHepQ",
        "outputId": "f0106c94-1301-44e9-be60-e108bf82bebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.69577205\n",
            "Iteration 2, loss = 0.68066804\n",
            "Iteration 3, loss = 0.66872945\n",
            "Iteration 4, loss = 0.65669064\n",
            "Iteration 5, loss = 0.64677642\n",
            "Iteration 6, loss = 0.63753823\n",
            "Iteration 7, loss = 0.62851702\n",
            "Iteration 8, loss = 0.62098742\n",
            "Iteration 9, loss = 0.61366021\n",
            "Iteration 10, loss = 0.60701451\n",
            "Iteration 11, loss = 0.60068516\n",
            "Iteration 12, loss = 0.59486090\n",
            "Iteration 13, loss = 0.58920706\n",
            "Iteration 14, loss = 0.58400559\n",
            "Iteration 15, loss = 0.57962014\n",
            "Iteration 16, loss = 0.57469789\n",
            "Iteration 17, loss = 0.57019631\n",
            "Iteration 18, loss = 0.56630702\n",
            "Iteration 19, loss = 0.56244514\n",
            "Iteration 20, loss = 0.55883408\n",
            "Iteration 21, loss = 0.55603194\n",
            "Iteration 22, loss = 0.55194066\n",
            "Iteration 23, loss = 0.54880242\n",
            "Iteration 24, loss = 0.54563843\n",
            "Iteration 25, loss = 0.54272236\n",
            "Iteration 26, loss = 0.53970141\n",
            "Iteration 27, loss = 0.53667165\n",
            "Iteration 28, loss = 0.53363884\n",
            "Iteration 29, loss = 0.53074274\n",
            "Iteration 30, loss = 0.52795635\n",
            "Iteration 31, loss = 0.52501574\n",
            "Iteration 32, loss = 0.52214989\n",
            "Iteration 33, loss = 0.51928635\n",
            "Iteration 34, loss = 0.51655908\n",
            "Iteration 35, loss = 0.51379313\n",
            "Iteration 36, loss = 0.51093265\n",
            "Iteration 37, loss = 0.50806745\n",
            "Iteration 38, loss = 0.50532479\n",
            "Iteration 39, loss = 0.50260691\n",
            "Iteration 40, loss = 0.49989842\n",
            "Iteration 41, loss = 0.49710995\n",
            "Iteration 42, loss = 0.49443562\n",
            "Iteration 43, loss = 0.49161546\n",
            "Iteration 44, loss = 0.48895708\n",
            "Iteration 45, loss = 0.48637771\n",
            "Iteration 46, loss = 0.48353695\n",
            "Iteration 47, loss = 0.48100031\n",
            "Iteration 48, loss = 0.47826233\n",
            "Iteration 49, loss = 0.47556148\n",
            "Iteration 50, loss = 0.47286070\n",
            "Iteration 51, loss = 0.47027666\n",
            "Iteration 52, loss = 0.46759310\n",
            "Iteration 53, loss = 0.46498632\n",
            "Iteration 54, loss = 0.46231468\n",
            "Iteration 55, loss = 0.45956095\n",
            "Iteration 56, loss = 0.45707701\n",
            "Iteration 57, loss = 0.45460470\n",
            "Iteration 58, loss = 0.45156462\n",
            "Iteration 59, loss = 0.44908651\n",
            "Iteration 60, loss = 0.44660618\n",
            "Iteration 61, loss = 0.44434433\n",
            "Iteration 62, loss = 0.44145812\n",
            "Iteration 63, loss = 0.43862826\n",
            "Iteration 64, loss = 0.43601299\n",
            "Iteration 65, loss = 0.43368522\n",
            "Iteration 66, loss = 0.43159032\n",
            "Iteration 67, loss = 0.42872253\n",
            "Iteration 68, loss = 0.42606312\n",
            "Iteration 69, loss = 0.42334445\n",
            "Iteration 70, loss = 0.42062459\n",
            "Iteration 71, loss = 0.41835240\n",
            "Iteration 72, loss = 0.41532473\n",
            "Iteration 73, loss = 0.41269315\n",
            "Iteration 74, loss = 0.41033800\n",
            "Iteration 75, loss = 0.40770989\n",
            "Iteration 76, loss = 0.40533099\n",
            "Iteration 77, loss = 0.40282340\n",
            "Iteration 78, loss = 0.40030284\n",
            "Iteration 79, loss = 0.39756312\n",
            "Iteration 80, loss = 0.39517623\n",
            "Iteration 81, loss = 0.39247060\n",
            "Iteration 82, loss = 0.39034983\n",
            "Iteration 83, loss = 0.38753770\n",
            "Iteration 84, loss = 0.38523563\n",
            "Iteration 85, loss = 0.38292031\n",
            "Iteration 86, loss = 0.38031863\n",
            "Iteration 87, loss = 0.37788873\n",
            "Iteration 88, loss = 0.37537602\n",
            "Iteration 89, loss = 0.37289085\n",
            "Iteration 90, loss = 0.37074715\n",
            "Iteration 91, loss = 0.36841989\n",
            "Iteration 92, loss = 0.36560514\n",
            "Iteration 93, loss = 0.36340523\n",
            "Iteration 94, loss = 0.36088134\n",
            "Iteration 95, loss = 0.35852826\n",
            "Iteration 96, loss = 0.35638438\n",
            "Iteration 97, loss = 0.35377646\n",
            "Iteration 98, loss = 0.35148266\n",
            "Iteration 99, loss = 0.34892633\n",
            "Iteration 100, loss = 0.34657127\n",
            "Iteration 101, loss = 0.34437529\n",
            "Iteration 102, loss = 0.34213067\n",
            "Iteration 103, loss = 0.33976974\n",
            "Iteration 104, loss = 0.33740302\n",
            "Iteration 105, loss = 0.33518483\n",
            "Iteration 106, loss = 0.33297049\n",
            "Iteration 107, loss = 0.33073821\n",
            "Iteration 108, loss = 0.32855376\n",
            "Iteration 109, loss = 0.32636723\n",
            "Iteration 110, loss = 0.32456114\n",
            "Iteration 111, loss = 0.32200993\n",
            "Iteration 112, loss = 0.32017862\n",
            "Iteration 113, loss = 0.31812574\n",
            "Iteration 114, loss = 0.31591296\n",
            "Iteration 115, loss = 0.31361558\n",
            "Iteration 116, loss = 0.31158399\n",
            "Iteration 117, loss = 0.30947625\n",
            "Iteration 118, loss = 0.30735603\n",
            "Iteration 119, loss = 0.30541927\n",
            "Iteration 120, loss = 0.30330267\n",
            "Iteration 121, loss = 0.30122162\n",
            "Iteration 122, loss = 0.29929090\n",
            "Iteration 123, loss = 0.29738810\n",
            "Iteration 124, loss = 0.29560141\n",
            "Iteration 125, loss = 0.29342809\n",
            "Iteration 126, loss = 0.29147597\n",
            "Iteration 127, loss = 0.28951957\n",
            "Iteration 128, loss = 0.28765418\n",
            "Iteration 129, loss = 0.28560361\n",
            "Iteration 130, loss = 0.28390845\n",
            "Iteration 131, loss = 0.28202890\n",
            "Iteration 132, loss = 0.28001601\n",
            "Iteration 133, loss = 0.27833847\n",
            "Iteration 134, loss = 0.27636671\n",
            "Iteration 135, loss = 0.27449281\n",
            "Iteration 136, loss = 0.27290372\n",
            "Iteration 137, loss = 0.27098127\n",
            "Iteration 138, loss = 0.26907683\n",
            "Iteration 139, loss = 0.26730045\n",
            "Iteration 140, loss = 0.26549686\n",
            "Iteration 141, loss = 0.26391118\n",
            "Iteration 142, loss = 0.26212530\n",
            "Iteration 143, loss = 0.26041783\n",
            "Iteration 144, loss = 0.25866991\n",
            "Iteration 145, loss = 0.25698285\n",
            "Iteration 146, loss = 0.25532711\n",
            "Iteration 147, loss = 0.25393676\n",
            "Iteration 148, loss = 0.25206741\n",
            "Iteration 149, loss = 0.25078284\n",
            "Iteration 150, loss = 0.24886800\n",
            "Iteration 151, loss = 0.24725046\n",
            "Iteration 152, loss = 0.24585083\n",
            "Iteration 153, loss = 0.24415209\n",
            "Iteration 154, loss = 0.24258872\n",
            "Iteration 155, loss = 0.24104045\n",
            "Iteration 156, loss = 0.23958460\n",
            "Iteration 157, loss = 0.23801039\n",
            "Iteration 158, loss = 0.23653980\n",
            "Iteration 159, loss = 0.23504705\n",
            "Iteration 160, loss = 0.23355789\n",
            "Iteration 161, loss = 0.23214108\n",
            "Iteration 162, loss = 0.23054499\n",
            "Iteration 163, loss = 0.22904320\n",
            "Iteration 164, loss = 0.22770635\n",
            "Iteration 165, loss = 0.22651685\n",
            "Iteration 166, loss = 0.22491908\n",
            "Iteration 167, loss = 0.22346596\n",
            "Iteration 168, loss = 0.22224076\n",
            "Iteration 169, loss = 0.22096123\n",
            "Iteration 170, loss = 0.21956885\n",
            "Iteration 171, loss = 0.21817696\n",
            "Iteration 172, loss = 0.21689517\n",
            "Iteration 173, loss = 0.21551129\n",
            "Iteration 174, loss = 0.21416212\n",
            "Iteration 175, loss = 0.21296051\n",
            "Iteration 176, loss = 0.21165372\n",
            "Iteration 177, loss = 0.21078793\n",
            "Iteration 178, loss = 0.20926554\n",
            "Iteration 179, loss = 0.20801057\n",
            "Iteration 180, loss = 0.20680751\n",
            "Iteration 181, loss = 0.20563994\n",
            "Iteration 182, loss = 0.20433657\n",
            "Iteration 183, loss = 0.20340928\n",
            "Iteration 184, loss = 0.20194933\n",
            "Iteration 185, loss = 0.20079364\n",
            "Iteration 186, loss = 0.19977244\n",
            "Iteration 187, loss = 0.19848567\n",
            "Iteration 188, loss = 0.19734830\n",
            "Iteration 189, loss = 0.19609720\n",
            "Iteration 190, loss = 0.19509502\n",
            "Iteration 191, loss = 0.19401824\n",
            "Iteration 192, loss = 0.19287040\n",
            "Iteration 193, loss = 0.19173693\n",
            "Iteration 194, loss = 0.19095004\n",
            "Iteration 195, loss = 0.18964438\n",
            "Iteration 196, loss = 0.18848065\n",
            "Iteration 197, loss = 0.18734329\n",
            "Iteration 198, loss = 0.18633141\n",
            "Iteration 199, loss = 0.18542300\n",
            "Iteration 200, loss = 0.18423919\n",
            "Iteration 201, loss = 0.18315211\n",
            "Iteration 202, loss = 0.18214815\n",
            "Iteration 203, loss = 0.18106888\n",
            "Iteration 204, loss = 0.18025633\n",
            "Iteration 205, loss = 0.17906980\n",
            "Iteration 206, loss = 0.17815973\n",
            "Iteration 207, loss = 0.17701702\n",
            "Iteration 208, loss = 0.17620876\n",
            "Iteration 209, loss = 0.17519972\n",
            "Iteration 210, loss = 0.17412853\n",
            "Iteration 211, loss = 0.17319814\n",
            "Iteration 212, loss = 0.17224094\n",
            "Iteration 213, loss = 0.17136569\n",
            "Iteration 214, loss = 0.17038387\n",
            "Iteration 215, loss = 0.16937675\n",
            "Iteration 216, loss = 0.16851555\n",
            "Iteration 217, loss = 0.16755290\n",
            "Iteration 218, loss = 0.16669058\n",
            "Iteration 219, loss = 0.16593457\n",
            "Iteration 220, loss = 0.16486578\n",
            "Iteration 221, loss = 0.16407205\n",
            "Iteration 222, loss = 0.16297158\n",
            "Iteration 223, loss = 0.16213798\n",
            "Iteration 224, loss = 0.16133954\n",
            "Iteration 225, loss = 0.16040061\n",
            "Iteration 226, loss = 0.15962544\n",
            "Iteration 227, loss = 0.15881572\n",
            "Iteration 228, loss = 0.15785887\n",
            "Iteration 229, loss = 0.15705513\n",
            "Iteration 230, loss = 0.15623203\n",
            "Iteration 231, loss = 0.15546040\n",
            "Iteration 232, loss = 0.15470346\n",
            "Iteration 233, loss = 0.15381778\n",
            "Iteration 234, loss = 0.15310596\n",
            "Iteration 235, loss = 0.15230680\n",
            "Iteration 236, loss = 0.15132944\n",
            "Iteration 237, loss = 0.15059318\n",
            "Iteration 238, loss = 0.14976476\n",
            "Iteration 239, loss = 0.14919632\n",
            "Iteration 240, loss = 0.14818145\n",
            "Iteration 241, loss = 0.14752112\n",
            "Iteration 242, loss = 0.14665146\n",
            "Iteration 243, loss = 0.14596118\n",
            "Iteration 244, loss = 0.14523726\n",
            "Iteration 245, loss = 0.14448883\n",
            "Iteration 246, loss = 0.14372552\n",
            "Iteration 247, loss = 0.14317169\n",
            "Iteration 248, loss = 0.14227144\n",
            "Iteration 249, loss = 0.14161489\n",
            "Iteration 250, loss = 0.14091074\n",
            "Iteration 251, loss = 0.14013942\n",
            "Iteration 252, loss = 0.13929498\n",
            "Iteration 253, loss = 0.13860550\n",
            "Iteration 254, loss = 0.13793402\n",
            "Iteration 255, loss = 0.13726296\n",
            "Iteration 256, loss = 0.13650725\n",
            "Iteration 257, loss = 0.13589762\n",
            "Iteration 258, loss = 0.13510957\n",
            "Iteration 259, loss = 0.13445151\n",
            "Iteration 260, loss = 0.13374457\n",
            "Iteration 261, loss = 0.13310668\n",
            "Iteration 262, loss = 0.13246640\n",
            "Iteration 263, loss = 0.13175709\n",
            "Iteration 264, loss = 0.13127334\n",
            "Iteration 265, loss = 0.13048586\n",
            "Iteration 266, loss = 0.12985877\n",
            "Iteration 267, loss = 0.12919406\n",
            "Iteration 268, loss = 0.12853044\n",
            "Iteration 269, loss = 0.12796467\n",
            "Iteration 270, loss = 0.12736985\n",
            "Iteration 271, loss = 0.12672777\n",
            "Iteration 272, loss = 0.12610370\n",
            "Iteration 273, loss = 0.12549947\n",
            "Iteration 274, loss = 0.12493667\n",
            "Iteration 275, loss = 0.12446865\n",
            "Iteration 276, loss = 0.12399086\n",
            "Iteration 277, loss = 0.12320302\n",
            "Iteration 278, loss = 0.12250831\n",
            "Iteration 279, loss = 0.12190690\n",
            "Iteration 280, loss = 0.12140324\n",
            "Iteration 281, loss = 0.12081765\n",
            "Iteration 282, loss = 0.12025677\n",
            "Iteration 283, loss = 0.11979526\n",
            "Iteration 284, loss = 0.11912829\n",
            "Iteration 285, loss = 0.11855017\n",
            "Iteration 286, loss = 0.11804247\n",
            "Iteration 287, loss = 0.11745641\n",
            "Iteration 288, loss = 0.11694622\n",
            "Iteration 289, loss = 0.11626998\n",
            "Iteration 290, loss = 0.11590190\n",
            "Iteration 291, loss = 0.11531279\n",
            "Iteration 292, loss = 0.11484238\n",
            "Iteration 293, loss = 0.11414272\n",
            "Iteration 294, loss = 0.11376063\n",
            "Iteration 295, loss = 0.11323334\n",
            "Iteration 296, loss = 0.11273817\n",
            "Iteration 297, loss = 0.11208958\n",
            "Iteration 298, loss = 0.11172307\n",
            "Iteration 299, loss = 0.11108000\n",
            "Iteration 300, loss = 0.11067175\n",
            "Iteration 301, loss = 0.11022501\n",
            "Iteration 302, loss = 0.10967913\n",
            "Iteration 303, loss = 0.10920471\n",
            "Iteration 304, loss = 0.10873070\n",
            "Iteration 305, loss = 0.10817817\n",
            "Iteration 306, loss = 0.10793491\n",
            "Iteration 307, loss = 0.10721401\n",
            "Iteration 308, loss = 0.10672189\n",
            "Iteration 309, loss = 0.10630700\n",
            "Iteration 310, loss = 0.10582222\n",
            "Iteration 311, loss = 0.10525340\n",
            "Iteration 312, loss = 0.10483252\n",
            "Iteration 313, loss = 0.10438619\n",
            "Iteration 314, loss = 0.10399960\n",
            "Iteration 315, loss = 0.10358949\n",
            "Iteration 316, loss = 0.10305969\n",
            "Iteration 317, loss = 0.10304195\n",
            "Iteration 318, loss = 0.10213426\n",
            "Iteration 319, loss = 0.10180981\n",
            "Iteration 320, loss = 0.10140752\n",
            "Iteration 321, loss = 0.10102809\n",
            "Iteration 322, loss = 0.10042457\n",
            "Iteration 323, loss = 0.09991774\n",
            "Iteration 324, loss = 0.09952447\n",
            "Iteration 325, loss = 0.09910657\n",
            "Iteration 326, loss = 0.09876291\n",
            "Iteration 327, loss = 0.09836519\n",
            "Iteration 328, loss = 0.09795153\n",
            "Iteration 329, loss = 0.09741759\n",
            "Iteration 330, loss = 0.09726025\n",
            "Iteration 331, loss = 0.09673438\n",
            "Iteration 332, loss = 0.09654203\n",
            "Iteration 333, loss = 0.09585578\n",
            "Iteration 334, loss = 0.09544163\n",
            "Iteration 335, loss = 0.09505306\n",
            "Iteration 336, loss = 0.09471137\n",
            "Iteration 337, loss = 0.09427580\n",
            "Iteration 338, loss = 0.09392604\n",
            "Iteration 339, loss = 0.09351193\n",
            "Iteration 340, loss = 0.09306601\n",
            "Iteration 341, loss = 0.09282668\n",
            "Iteration 342, loss = 0.09264076\n",
            "Iteration 343, loss = 0.09197698\n",
            "Iteration 344, loss = 0.09164568\n",
            "Iteration 345, loss = 0.09120594\n",
            "Iteration 346, loss = 0.09090483\n",
            "Iteration 347, loss = 0.09053597\n",
            "Iteration 348, loss = 0.09024517\n",
            "Iteration 349, loss = 0.08981484\n",
            "Iteration 350, loss = 0.08949754\n",
            "Iteration 351, loss = 0.08907210\n",
            "Iteration 352, loss = 0.08871298\n",
            "Iteration 353, loss = 0.08835074\n",
            "Iteration 354, loss = 0.08811548\n",
            "Iteration 355, loss = 0.08776223\n",
            "Iteration 356, loss = 0.08740366\n",
            "Iteration 357, loss = 0.08700662\n",
            "Iteration 358, loss = 0.08689025\n",
            "Iteration 359, loss = 0.08651186\n",
            "Iteration 360, loss = 0.08608959\n",
            "Iteration 361, loss = 0.08581657\n",
            "Iteration 362, loss = 0.08535158\n",
            "Iteration 363, loss = 0.08500868\n",
            "Iteration 364, loss = 0.08464806\n",
            "Iteration 365, loss = 0.08425929\n",
            "Iteration 366, loss = 0.08400622\n",
            "Iteration 367, loss = 0.08359613\n",
            "Iteration 368, loss = 0.08340220\n",
            "Iteration 369, loss = 0.08300173\n",
            "Iteration 370, loss = 0.08271490\n",
            "Iteration 371, loss = 0.08239832\n",
            "Iteration 372, loss = 0.08214199\n",
            "Iteration 373, loss = 0.08181951\n",
            "Iteration 374, loss = 0.08148785\n",
            "Iteration 375, loss = 0.08115059\n",
            "Iteration 376, loss = 0.08082938\n",
            "Iteration 377, loss = 0.08063513\n",
            "Iteration 378, loss = 0.08030009\n",
            "Iteration 379, loss = 0.08001946\n",
            "Iteration 380, loss = 0.07970124\n",
            "Iteration 381, loss = 0.07939728\n",
            "Iteration 382, loss = 0.07911517\n",
            "Iteration 383, loss = 0.07879950\n",
            "Iteration 384, loss = 0.07857100\n",
            "Iteration 385, loss = 0.07821754\n",
            "Iteration 386, loss = 0.07793049\n",
            "Iteration 387, loss = 0.07772536\n",
            "Iteration 388, loss = 0.07735449\n",
            "Iteration 389, loss = 0.07712861\n",
            "Iteration 390, loss = 0.07679932\n",
            "Iteration 391, loss = 0.07654541\n",
            "Iteration 392, loss = 0.07619675\n",
            "Iteration 393, loss = 0.07597605\n",
            "Iteration 394, loss = 0.07567781\n",
            "Iteration 395, loss = 0.07538678\n",
            "Iteration 396, loss = 0.07506251\n",
            "Iteration 397, loss = 0.07482709\n",
            "Iteration 398, loss = 0.07459802\n",
            "Iteration 399, loss = 0.07432509\n",
            "Iteration 400, loss = 0.07424645\n",
            "Iteration 401, loss = 0.07382085\n",
            "Iteration 402, loss = 0.07353731\n",
            "Iteration 403, loss = 0.07340743\n",
            "Iteration 404, loss = 0.07296807\n",
            "Iteration 405, loss = 0.07275077\n",
            "Iteration 406, loss = 0.07246361\n",
            "Iteration 407, loss = 0.07231441\n",
            "Iteration 408, loss = 0.07209687\n",
            "Iteration 409, loss = 0.07170615\n",
            "Iteration 410, loss = 0.07149519\n",
            "Iteration 411, loss = 0.07120561\n",
            "Iteration 412, loss = 0.07092006\n",
            "Iteration 413, loss = 0.07076561\n",
            "Iteration 414, loss = 0.07048983\n",
            "Iteration 415, loss = 0.07023362\n",
            "Iteration 416, loss = 0.07000310\n",
            "Iteration 417, loss = 0.06975838\n",
            "Iteration 418, loss = 0.06954463\n",
            "Iteration 419, loss = 0.06932393\n",
            "Iteration 420, loss = 0.06919268\n",
            "Iteration 421, loss = 0.06878675\n",
            "Iteration 422, loss = 0.06864193\n",
            "Iteration 423, loss = 0.06845624\n",
            "Iteration 424, loss = 0.06815445\n",
            "Iteration 425, loss = 0.06796197\n",
            "Iteration 426, loss = 0.06763454\n",
            "Iteration 427, loss = 0.06746893\n",
            "Iteration 428, loss = 0.06727051\n",
            "Iteration 429, loss = 0.06709683\n",
            "Iteration 430, loss = 0.06685057\n",
            "Iteration 431, loss = 0.06658966\n",
            "Iteration 432, loss = 0.06635037\n",
            "Iteration 433, loss = 0.06620449\n",
            "Iteration 434, loss = 0.06593876\n",
            "Iteration 435, loss = 0.06576465\n",
            "Iteration 436, loss = 0.06548678\n",
            "Iteration 437, loss = 0.06531950\n",
            "Iteration 438, loss = 0.06505217\n",
            "Iteration 439, loss = 0.06493834\n",
            "Iteration 440, loss = 0.06493373\n",
            "Iteration 441, loss = 0.06467700\n",
            "Iteration 442, loss = 0.06420793\n",
            "Iteration 443, loss = 0.06403690\n",
            "Iteration 444, loss = 0.06385673\n",
            "Iteration 445, loss = 0.06375995\n",
            "Iteration 446, loss = 0.06338081\n",
            "Iteration 447, loss = 0.06316960\n",
            "Iteration 448, loss = 0.06304500\n",
            "Iteration 449, loss = 0.06295490\n",
            "Iteration 450, loss = 0.06272684\n",
            "Iteration 451, loss = 0.06249006\n",
            "Iteration 452, loss = 0.06228953\n",
            "Iteration 453, loss = 0.06220128\n",
            "Iteration 454, loss = 0.06189806\n",
            "Iteration 455, loss = 0.06167678\n",
            "Iteration 456, loss = 0.06157345\n",
            "Iteration 457, loss = 0.06126426\n",
            "Iteration 458, loss = 0.06106415\n",
            "Iteration 459, loss = 0.06110395\n",
            "Iteration 460, loss = 0.06083509\n",
            "Iteration 461, loss = 0.06051038\n",
            "Iteration 462, loss = 0.06033306\n",
            "Iteration 463, loss = 0.06029232\n",
            "Iteration 464, loss = 0.06012679\n",
            "Iteration 465, loss = 0.05991865\n",
            "Iteration 466, loss = 0.05971183\n",
            "Iteration 467, loss = 0.05944376\n",
            "Iteration 468, loss = 0.05938513\n",
            "Iteration 469, loss = 0.05918475\n",
            "Iteration 470, loss = 0.05908262\n",
            "Iteration 471, loss = 0.05893857\n",
            "Iteration 472, loss = 0.05856166\n",
            "Iteration 473, loss = 0.05848422\n",
            "Iteration 474, loss = 0.05838080\n",
            "Iteration 475, loss = 0.05829089\n",
            "Iteration 476, loss = 0.05787021\n",
            "Iteration 477, loss = 0.05785375\n",
            "Iteration 478, loss = 0.05758502\n",
            "Iteration 479, loss = 0.05740305\n",
            "Iteration 480, loss = 0.05721450\n",
            "Iteration 481, loss = 0.05707387\n",
            "Iteration 482, loss = 0.05696409\n",
            "Iteration 483, loss = 0.05683266\n",
            "Iteration 484, loss = 0.05666645\n",
            "Iteration 485, loss = 0.05653652\n",
            "Iteration 486, loss = 0.05633943\n",
            "Iteration 487, loss = 0.05611722\n",
            "Iteration 488, loss = 0.05607423\n",
            "Iteration 489, loss = 0.05582743\n",
            "Iteration 490, loss = 0.05571556\n",
            "Iteration 491, loss = 0.05559299\n",
            "Iteration 492, loss = 0.05538409\n",
            "Iteration 493, loss = 0.05523411\n",
            "Iteration 494, loss = 0.05511611\n",
            "Iteration 495, loss = 0.05499839\n",
            "Iteration 496, loss = 0.05486261\n",
            "Iteration 497, loss = 0.05467708\n",
            "Iteration 498, loss = 0.05451753\n",
            "Iteration 499, loss = 0.05430503\n",
            "Iteration 500, loss = 0.05419152\n",
            "Iteration 501, loss = 0.05412020\n",
            "Iteration 502, loss = 0.05408809\n",
            "Iteration 503, loss = 0.05381803\n",
            "Iteration 504, loss = 0.05370825\n",
            "Iteration 505, loss = 0.05348699\n",
            "Iteration 506, loss = 0.05335363\n",
            "Iteration 507, loss = 0.05336936\n",
            "Iteration 508, loss = 0.05318916\n",
            "Iteration 509, loss = 0.05298223\n",
            "Iteration 510, loss = 0.05283516\n",
            "Iteration 511, loss = 0.05265852\n",
            "Iteration 512, loss = 0.05251575\n",
            "Iteration 513, loss = 0.05237185\n",
            "Iteration 514, loss = 0.05239487\n",
            "Iteration 515, loss = 0.05212090\n",
            "Iteration 516, loss = 0.05202104\n",
            "Iteration 517, loss = 0.05193014\n",
            "Iteration 518, loss = 0.05170296\n",
            "Iteration 519, loss = 0.05152728\n",
            "Iteration 520, loss = 0.05144202\n",
            "Iteration 521, loss = 0.05142765\n",
            "Iteration 522, loss = 0.05120600\n",
            "Iteration 523, loss = 0.05114418\n",
            "Iteration 524, loss = 0.05095738\n",
            "Iteration 525, loss = 0.05084593\n",
            "Iteration 526, loss = 0.05085537\n",
            "Iteration 527, loss = 0.05064354\n",
            "Iteration 528, loss = 0.05046990\n",
            "Iteration 529, loss = 0.05034943\n",
            "Iteration 530, loss = 0.05018995\n",
            "Iteration 531, loss = 0.05001538\n",
            "Iteration 532, loss = 0.04994459\n",
            "Iteration 533, loss = 0.04976068\n",
            "Iteration 534, loss = 0.04986239\n",
            "Iteration 535, loss = 0.04954999\n",
            "Iteration 536, loss = 0.04946724\n",
            "Iteration 537, loss = 0.04953977\n",
            "Iteration 538, loss = 0.04932492\n",
            "Iteration 539, loss = 0.04916026\n",
            "Iteration 540, loss = 0.04898331\n",
            "Iteration 541, loss = 0.04894009\n",
            "Iteration 542, loss = 0.04899748\n",
            "Iteration 543, loss = 0.04870834\n",
            "Iteration 544, loss = 0.04849676\n",
            "Iteration 545, loss = 0.04852884\n",
            "Iteration 546, loss = 0.04850696\n",
            "Iteration 547, loss = 0.04830089\n",
            "Iteration 548, loss = 0.04803412\n",
            "Iteration 549, loss = 0.04790810\n",
            "Iteration 550, loss = 0.04795633\n",
            "Iteration 551, loss = 0.04775240\n",
            "Iteration 552, loss = 0.04766862\n",
            "Iteration 553, loss = 0.04756342\n",
            "Iteration 554, loss = 0.04735981\n",
            "Iteration 555, loss = 0.04734860\n",
            "Iteration 556, loss = 0.04721343\n",
            "Iteration 557, loss = 0.04713512\n",
            "Iteration 558, loss = 0.04692131\n",
            "Iteration 559, loss = 0.04683630\n",
            "Iteration 560, loss = 0.04710248\n",
            "Iteration 561, loss = 0.04682491\n",
            "Iteration 562, loss = 0.04663701\n",
            "Iteration 563, loss = 0.04634218\n",
            "Iteration 564, loss = 0.04631737\n",
            "Iteration 565, loss = 0.04632582\n",
            "Iteration 566, loss = 0.04635723\n",
            "Iteration 567, loss = 0.04614335\n",
            "Iteration 568, loss = 0.04595677\n",
            "Iteration 569, loss = 0.04583777\n",
            "Iteration 570, loss = 0.04563538\n",
            "Iteration 571, loss = 0.04563171\n",
            "Iteration 572, loss = 0.04558940\n",
            "Iteration 573, loss = 0.04553107\n",
            "Iteration 574, loss = 0.04535095\n",
            "Iteration 575, loss = 0.04527096\n",
            "Iteration 576, loss = 0.04513682\n",
            "Iteration 577, loss = 0.04508634\n",
            "Iteration 578, loss = 0.04495205\n",
            "Iteration 579, loss = 0.04485354\n",
            "Iteration 580, loss = 0.04488599\n",
            "Iteration 581, loss = 0.04466561\n",
            "Iteration 582, loss = 0.04465703\n",
            "Iteration 583, loss = 0.04457743\n",
            "Iteration 584, loss = 0.04432840\n",
            "Iteration 585, loss = 0.04446474\n",
            "Iteration 586, loss = 0.04420400\n",
            "Iteration 587, loss = 0.04410260\n",
            "Iteration 588, loss = 0.04402771\n",
            "Iteration 589, loss = 0.04392469\n",
            "Iteration 590, loss = 0.04394983\n",
            "Iteration 591, loss = 0.04371817\n",
            "Iteration 592, loss = 0.04388280\n",
            "Iteration 593, loss = 0.04348091\n",
            "Iteration 594, loss = 0.04336496\n",
            "Iteration 595, loss = 0.04352190\n",
            "Iteration 596, loss = 0.04341343\n",
            "Iteration 597, loss = 0.04321211\n",
            "Iteration 598, loss = 0.04311164\n",
            "Iteration 599, loss = 0.04309709\n",
            "Iteration 600, loss = 0.04291730\n",
            "Iteration 601, loss = 0.04281867\n",
            "Iteration 602, loss = 0.04277337\n",
            "Iteration 603, loss = 0.04271672\n",
            "Iteration 604, loss = 0.04258064\n",
            "Iteration 605, loss = 0.04243039\n",
            "Iteration 606, loss = 0.04250685\n",
            "Iteration 607, loss = 0.04224466\n",
            "Iteration 608, loss = 0.04243234\n",
            "Iteration 609, loss = 0.04249355\n",
            "Iteration 610, loss = 0.04236382\n",
            "Iteration 611, loss = 0.04199687\n",
            "Iteration 612, loss = 0.04190229\n",
            "Iteration 613, loss = 0.04199723\n",
            "Iteration 614, loss = 0.04188033\n",
            "Iteration 615, loss = 0.04164375\n",
            "Iteration 616, loss = 0.04186816\n",
            "Iteration 617, loss = 0.04154099\n",
            "Iteration 618, loss = 0.04144841\n",
            "Iteration 619, loss = 0.04131966\n",
            "Iteration 620, loss = 0.04117563\n",
            "Iteration 621, loss = 0.04124290\n",
            "Iteration 622, loss = 0.04112601\n",
            "Iteration 623, loss = 0.04106873\n",
            "Iteration 624, loss = 0.04086900\n",
            "Iteration 625, loss = 0.04078388\n",
            "Iteration 626, loss = 0.04074734\n",
            "Iteration 627, loss = 0.04073234\n",
            "Iteration 628, loss = 0.04062764\n",
            "Iteration 629, loss = 0.04067201\n",
            "Iteration 630, loss = 0.04043995\n",
            "Iteration 631, loss = 0.04038982\n",
            "Iteration 632, loss = 0.04047851\n",
            "Iteration 633, loss = 0.04031023\n",
            "Iteration 634, loss = 0.04012253\n",
            "Iteration 635, loss = 0.04006889\n",
            "Iteration 636, loss = 0.04010045\n",
            "Iteration 637, loss = 0.03991589\n",
            "Iteration 638, loss = 0.03992325\n",
            "Iteration 639, loss = 0.04001100\n",
            "Iteration 640, loss = 0.03999531\n",
            "Iteration 641, loss = 0.03973602\n",
            "Iteration 642, loss = 0.03950296\n",
            "Iteration 643, loss = 0.03962578\n",
            "Iteration 644, loss = 0.03963550\n",
            "Iteration 645, loss = 0.03968769\n",
            "Iteration 646, loss = 0.03946220\n",
            "Iteration 647, loss = 0.03929825\n",
            "Iteration 648, loss = 0.03929530\n",
            "Iteration 649, loss = 0.03929723\n",
            "Iteration 650, loss = 0.03915816\n",
            "Iteration 651, loss = 0.03899190\n",
            "Iteration 652, loss = 0.03891712\n",
            "Iteration 653, loss = 0.03883471\n",
            "Iteration 654, loss = 0.03871871\n",
            "Iteration 655, loss = 0.03882624\n",
            "Iteration 656, loss = 0.03873106\n",
            "Iteration 657, loss = 0.03853992\n",
            "Iteration 658, loss = 0.03839238\n",
            "Iteration 659, loss = 0.03839873\n",
            "Iteration 660, loss = 0.03841145\n",
            "Iteration 661, loss = 0.03824853\n",
            "Iteration 662, loss = 0.03817403\n",
            "Iteration 663, loss = 0.03813987\n",
            "Iteration 664, loss = 0.03806864\n",
            "Iteration 665, loss = 0.03833694\n",
            "Iteration 666, loss = 0.03795916\n",
            "Iteration 667, loss = 0.03787766\n",
            "Iteration 668, loss = 0.03773598\n",
            "Iteration 669, loss = 0.03787283\n",
            "Iteration 670, loss = 0.03787607\n",
            "Iteration 671, loss = 0.03784274\n",
            "Iteration 672, loss = 0.03767956\n",
            "Iteration 673, loss = 0.03747777\n",
            "Iteration 674, loss = 0.03755054\n",
            "Iteration 675, loss = 0.03772447\n",
            "Iteration 676, loss = 0.03737580\n",
            "Iteration 677, loss = 0.03726422\n",
            "Iteration 678, loss = 0.03721891\n",
            "Iteration 679, loss = 0.03719495\n",
            "Iteration 680, loss = 0.03714652\n",
            "Iteration 681, loss = 0.03707206\n",
            "Iteration 682, loss = 0.03715412\n",
            "Iteration 683, loss = 0.03686370\n",
            "Iteration 684, loss = 0.03686002\n",
            "Iteration 685, loss = 0.03676719\n",
            "Iteration 686, loss = 0.03670736\n",
            "Iteration 687, loss = 0.03659615\n",
            "Iteration 688, loss = 0.03659184\n",
            "Iteration 689, loss = 0.03656766\n",
            "Iteration 690, loss = 0.03640607\n",
            "Iteration 691, loss = 0.03640144\n",
            "Iteration 692, loss = 0.03629620\n",
            "Iteration 693, loss = 0.03627575\n",
            "Iteration 694, loss = 0.03623513\n",
            "Iteration 695, loss = 0.03615272\n",
            "Iteration 696, loss = 0.03616766\n",
            "Iteration 697, loss = 0.03599109\n",
            "Iteration 698, loss = 0.03593440\n",
            "Iteration 699, loss = 0.03589962\n",
            "Iteration 700, loss = 0.03584911\n",
            "Iteration 701, loss = 0.03578138\n",
            "Iteration 702, loss = 0.03589199\n",
            "Iteration 703, loss = 0.03567994\n",
            "Iteration 704, loss = 0.03561680\n",
            "Iteration 705, loss = 0.03569410\n",
            "Iteration 706, loss = 0.03560002\n",
            "Iteration 707, loss = 0.03561194\n",
            "Iteration 708, loss = 0.03539691\n",
            "Iteration 709, loss = 0.03535177\n",
            "Iteration 710, loss = 0.03527185\n",
            "Iteration 711, loss = 0.03533252\n",
            "Iteration 712, loss = 0.03519033\n",
            "Iteration 713, loss = 0.03509627\n",
            "Iteration 714, loss = 0.03508973\n",
            "Iteration 715, loss = 0.03506753\n",
            "Iteration 716, loss = 0.03496624\n",
            "Iteration 717, loss = 0.03489261\n",
            "Iteration 718, loss = 0.03487162\n",
            "Iteration 719, loss = 0.03485916\n",
            "Iteration 720, loss = 0.03489585\n",
            "Iteration 721, loss = 0.03483699\n",
            "Iteration 722, loss = 0.03471710\n",
            "Iteration 723, loss = 0.03461035\n",
            "Iteration 724, loss = 0.03453062\n",
            "Iteration 725, loss = 0.03452424\n",
            "Iteration 726, loss = 0.03478192\n",
            "Iteration 727, loss = 0.03445274\n",
            "Iteration 728, loss = 0.03440531\n",
            "Iteration 729, loss = 0.03445627\n",
            "Iteration 730, loss = 0.03429537\n",
            "Iteration 731, loss = 0.03438338\n",
            "Iteration 732, loss = 0.03427525\n",
            "Iteration 733, loss = 0.03418546\n",
            "Iteration 734, loss = 0.03416572\n",
            "Iteration 735, loss = 0.03402160\n",
            "Iteration 736, loss = 0.03405196\n",
            "Iteration 737, loss = 0.03407067\n",
            "Iteration 738, loss = 0.03400286\n",
            "Iteration 739, loss = 0.03392748\n",
            "Iteration 740, loss = 0.03379972\n",
            "Iteration 741, loss = 0.03376023\n",
            "Iteration 742, loss = 0.03379955\n",
            "Iteration 743, loss = 0.03383991\n",
            "Iteration 744, loss = 0.03376701\n",
            "Iteration 745, loss = 0.03357481\n",
            "Iteration 746, loss = 0.03349710\n",
            "Iteration 747, loss = 0.03359966\n",
            "Iteration 748, loss = 0.03349962\n",
            "Iteration 749, loss = 0.03352368\n",
            "Iteration 750, loss = 0.03334162\n",
            "Iteration 751, loss = 0.03342692\n",
            "Iteration 752, loss = 0.03341747\n",
            "Iteration 753, loss = 0.03342768\n",
            "Iteration 754, loss = 0.03326049\n",
            "Iteration 755, loss = 0.03312345\n",
            "Iteration 756, loss = 0.03307003\n",
            "Iteration 757, loss = 0.03299295\n",
            "Iteration 758, loss = 0.03295844\n",
            "Iteration 759, loss = 0.03295341\n",
            "Iteration 760, loss = 0.03299806\n",
            "Iteration 761, loss = 0.03282983\n",
            "Iteration 762, loss = 0.03281950\n",
            "Iteration 763, loss = 0.03272810\n",
            "Iteration 764, loss = 0.03268639\n",
            "Iteration 765, loss = 0.03274363\n",
            "Iteration 766, loss = 0.03272358\n",
            "Iteration 767, loss = 0.03256101\n",
            "Iteration 768, loss = 0.03252753\n",
            "Iteration 769, loss = 0.03246763\n",
            "Iteration 770, loss = 0.03261192\n",
            "Iteration 771, loss = 0.03266267\n",
            "Iteration 772, loss = 0.03247237\n",
            "Iteration 773, loss = 0.03231524\n",
            "Iteration 774, loss = 0.03222691\n",
            "Iteration 775, loss = 0.03232670\n",
            "Iteration 776, loss = 0.03224827\n",
            "Iteration 777, loss = 0.03228044\n",
            "Iteration 778, loss = 0.03216531\n",
            "Iteration 779, loss = 0.03210021\n",
            "Iteration 780, loss = 0.03202442\n",
            "Iteration 781, loss = 0.03203629\n",
            "Iteration 782, loss = 0.03203424\n",
            "Iteration 783, loss = 0.03202331\n",
            "Iteration 784, loss = 0.03188520\n",
            "Iteration 785, loss = 0.03238823\n",
            "Iteration 786, loss = 0.03187597\n",
            "Iteration 787, loss = 0.03190004\n",
            "Iteration 788, loss = 0.03178784\n",
            "Iteration 789, loss = 0.03166342\n",
            "Iteration 790, loss = 0.03171280\n",
            "Iteration 791, loss = 0.03163731\n",
            "Iteration 792, loss = 0.03153427\n",
            "Iteration 793, loss = 0.03154896\n",
            "Iteration 794, loss = 0.03158221\n",
            "Iteration 795, loss = 0.03150785\n",
            "Iteration 796, loss = 0.03146047\n",
            "Iteration 797, loss = 0.03143316\n",
            "Iteration 798, loss = 0.03144639\n",
            "Iteration 799, loss = 0.03127572\n",
            "Iteration 800, loss = 0.03122192\n",
            "Iteration 801, loss = 0.03128221\n",
            "Iteration 802, loss = 0.03128176\n",
            "Iteration 803, loss = 0.03119472\n",
            "Iteration 804, loss = 0.03106095\n",
            "Iteration 805, loss = 0.03113596\n",
            "Iteration 806, loss = 0.03112793\n",
            "Iteration 807, loss = 0.03111704\n",
            "Iteration 808, loss = 0.03112196\n",
            "Iteration 809, loss = 0.03112175\n",
            "Iteration 810, loss = 0.03095936\n",
            "Iteration 811, loss = 0.03101707\n",
            "Iteration 812, loss = 0.03094861\n",
            "Iteration 813, loss = 0.03089864\n",
            "Iteration 814, loss = 0.03074075\n",
            "Iteration 815, loss = 0.03081695\n",
            "Iteration 816, loss = 0.03086227\n",
            "Iteration 817, loss = 0.03096052\n",
            "Iteration 818, loss = 0.03076134\n",
            "Iteration 819, loss = 0.03065663\n",
            "Iteration 820, loss = 0.03075999\n",
            "Iteration 821, loss = 0.03072713\n",
            "Iteration 822, loss = 0.03049048\n",
            "Iteration 823, loss = 0.03055897\n",
            "Iteration 824, loss = 0.03047157\n",
            "Iteration 825, loss = 0.03054755\n",
            "Iteration 826, loss = 0.03040387\n",
            "Iteration 827, loss = 0.03051432\n",
            "Iteration 828, loss = 0.03029367\n",
            "Iteration 829, loss = 0.03029261\n",
            "Iteration 830, loss = 0.03030121\n",
            "Iteration 831, loss = 0.03023133\n",
            "Iteration 832, loss = 0.03028780\n",
            "Iteration 833, loss = 0.03013744\n",
            "Iteration 834, loss = 0.03054092\n",
            "Iteration 835, loss = 0.03016195\n",
            "Iteration 836, loss = 0.03007056\n",
            "Iteration 837, loss = 0.02994274\n",
            "Iteration 838, loss = 0.02989636\n",
            "Iteration 839, loss = 0.02997175\n",
            "Iteration 840, loss = 0.02993413\n",
            "Iteration 841, loss = 0.02992543\n",
            "Iteration 842, loss = 0.02982181\n",
            "Iteration 843, loss = 0.02975391\n",
            "Iteration 844, loss = 0.02977943\n",
            "Iteration 845, loss = 0.02970947\n",
            "Iteration 846, loss = 0.02977729\n",
            "Iteration 847, loss = 0.02964419\n",
            "Iteration 848, loss = 0.02967984\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQZVMhEJXOC"
      },
      "source": [
        "**Faça outras alterações nos parâmetros**\n",
        "\n",
        "**4 entradas - 3 neurônios - 3 neurônios - 1**\n",
        "\n",
        "**Veja SoftMax para problemas multiclasse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Q1RssrJU9z",
        "outputId": "802b6904-3a64-4a71-fd3b-221c672f3524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.72428564\n",
            "Iteration 2, loss = 0.70977052\n",
            "Iteration 3, loss = 0.69622148\n",
            "Iteration 4, loss = 0.68423722\n",
            "Iteration 5, loss = 0.67276470\n",
            "Iteration 6, loss = 0.66133967\n",
            "Iteration 7, loss = 0.65213307\n",
            "Iteration 8, loss = 0.64332841\n",
            "Iteration 9, loss = 0.63452488\n",
            "Iteration 10, loss = 0.62665851\n",
            "Iteration 11, loss = 0.61941389\n",
            "Iteration 12, loss = 0.61196504\n",
            "Iteration 13, loss = 0.60567699\n",
            "Iteration 14, loss = 0.60003480\n",
            "Iteration 15, loss = 0.59428738\n",
            "Iteration 16, loss = 0.58936019\n",
            "Iteration 17, loss = 0.58432294\n",
            "Iteration 18, loss = 0.58023525\n",
            "Iteration 19, loss = 0.57585260\n",
            "Iteration 20, loss = 0.57182667\n",
            "Iteration 21, loss = 0.56824894\n",
            "Iteration 22, loss = 0.56483110\n",
            "Iteration 23, loss = 0.56148882\n",
            "Iteration 24, loss = 0.55805156\n",
            "Iteration 25, loss = 0.55465156\n",
            "Iteration 26, loss = 0.55152987\n",
            "Iteration 27, loss = 0.54854140\n",
            "Iteration 28, loss = 0.54541969\n",
            "Iteration 29, loss = 0.54252787\n",
            "Iteration 30, loss = 0.53976601\n",
            "Iteration 31, loss = 0.53685147\n",
            "Iteration 32, loss = 0.53429802\n",
            "Iteration 33, loss = 0.53153131\n",
            "Iteration 34, loss = 0.52849751\n",
            "Iteration 35, loss = 0.52584335\n",
            "Iteration 36, loss = 0.52312928\n",
            "Iteration 37, loss = 0.52045999\n",
            "Iteration 38, loss = 0.51771961\n",
            "Iteration 39, loss = 0.51510356\n",
            "Iteration 40, loss = 0.51233146\n",
            "Iteration 41, loss = 0.50953661\n",
            "Iteration 42, loss = 0.50696588\n",
            "Iteration 43, loss = 0.50428639\n",
            "Iteration 44, loss = 0.50174553\n",
            "Iteration 45, loss = 0.49930087\n",
            "Iteration 46, loss = 0.49655859\n",
            "Iteration 47, loss = 0.49407535\n",
            "Iteration 48, loss = 0.49114908\n",
            "Iteration 49, loss = 0.48838843\n",
            "Iteration 50, loss = 0.48566810\n",
            "Iteration 51, loss = 0.48346863\n",
            "Iteration 52, loss = 0.48075248\n",
            "Iteration 53, loss = 0.47804898\n",
            "Iteration 54, loss = 0.47559098\n",
            "Iteration 55, loss = 0.47279209\n",
            "Iteration 56, loss = 0.47024288\n",
            "Iteration 57, loss = 0.46759340\n",
            "Iteration 58, loss = 0.46515812\n",
            "Iteration 59, loss = 0.46249921\n",
            "Iteration 60, loss = 0.46011415\n",
            "Iteration 61, loss = 0.45728613\n",
            "Iteration 62, loss = 0.45472946\n",
            "Iteration 63, loss = 0.45212512\n",
            "Iteration 64, loss = 0.44944147\n",
            "Iteration 65, loss = 0.44699930\n",
            "Iteration 66, loss = 0.44446218\n",
            "Iteration 67, loss = 0.44168400\n",
            "Iteration 68, loss = 0.43930627\n",
            "Iteration 69, loss = 0.43655782\n",
            "Iteration 70, loss = 0.43400045\n",
            "Iteration 71, loss = 0.43141434\n",
            "Iteration 72, loss = 0.42889117\n",
            "Iteration 73, loss = 0.42632166\n",
            "Iteration 74, loss = 0.42364879\n",
            "Iteration 75, loss = 0.42113027\n",
            "Iteration 76, loss = 0.41842614\n",
            "Iteration 77, loss = 0.41616166\n",
            "Iteration 78, loss = 0.41339933\n",
            "Iteration 79, loss = 0.41082029\n",
            "Iteration 80, loss = 0.40832691\n",
            "Iteration 81, loss = 0.40604618\n",
            "Iteration 82, loss = 0.40339507\n",
            "Iteration 83, loss = 0.40074029\n",
            "Iteration 84, loss = 0.39831346\n",
            "Iteration 85, loss = 0.39568439\n",
            "Iteration 86, loss = 0.39327261\n",
            "Iteration 87, loss = 0.39092432\n",
            "Iteration 88, loss = 0.38846835\n",
            "Iteration 89, loss = 0.38599561\n",
            "Iteration 90, loss = 0.38349951\n",
            "Iteration 91, loss = 0.38111480\n",
            "Iteration 92, loss = 0.37883330\n",
            "Iteration 93, loss = 0.37665475\n",
            "Iteration 94, loss = 0.37434319\n",
            "Iteration 95, loss = 0.37170970\n",
            "Iteration 96, loss = 0.36922775\n",
            "Iteration 97, loss = 0.36679132\n",
            "Iteration 98, loss = 0.36456155\n",
            "Iteration 99, loss = 0.36214248\n",
            "Iteration 100, loss = 0.35994993\n",
            "Iteration 101, loss = 0.35758789\n",
            "Iteration 102, loss = 0.35522049\n",
            "Iteration 103, loss = 0.35302158\n",
            "Iteration 104, loss = 0.35079088\n",
            "Iteration 105, loss = 0.34848406\n",
            "Iteration 106, loss = 0.34626219\n",
            "Iteration 107, loss = 0.34401351\n",
            "Iteration 108, loss = 0.34178495\n",
            "Iteration 109, loss = 0.34012377\n",
            "Iteration 110, loss = 0.33757869\n",
            "Iteration 111, loss = 0.33525264\n",
            "Iteration 112, loss = 0.33313702\n",
            "Iteration 113, loss = 0.33093027\n",
            "Iteration 114, loss = 0.32892594\n",
            "Iteration 115, loss = 0.32687064\n",
            "Iteration 116, loss = 0.32466338\n",
            "Iteration 117, loss = 0.32261629\n",
            "Iteration 118, loss = 0.32049102\n",
            "Iteration 119, loss = 0.31845595\n",
            "Iteration 120, loss = 0.31636715\n",
            "Iteration 121, loss = 0.31447881\n",
            "Iteration 122, loss = 0.31227765\n",
            "Iteration 123, loss = 0.31037908\n",
            "Iteration 124, loss = 0.30834540\n",
            "Iteration 125, loss = 0.30643569\n",
            "Iteration 126, loss = 0.30446733\n",
            "Iteration 127, loss = 0.30246711\n",
            "Iteration 128, loss = 0.30056519\n",
            "Iteration 129, loss = 0.29882748\n",
            "Iteration 130, loss = 0.29674252\n",
            "Iteration 131, loss = 0.29473397\n",
            "Iteration 132, loss = 0.29282816\n",
            "Iteration 133, loss = 0.29093995\n",
            "Iteration 134, loss = 0.28935915\n",
            "Iteration 135, loss = 0.28728496\n",
            "Iteration 136, loss = 0.28541559\n",
            "Iteration 137, loss = 0.28351920\n",
            "Iteration 138, loss = 0.28170837\n",
            "Iteration 139, loss = 0.27991645\n",
            "Iteration 140, loss = 0.27825445\n",
            "Iteration 141, loss = 0.27657318\n",
            "Iteration 142, loss = 0.27454863\n",
            "Iteration 143, loss = 0.27285778\n",
            "Iteration 144, loss = 0.27155196\n",
            "Iteration 145, loss = 0.26961638\n",
            "Iteration 146, loss = 0.26773206\n",
            "Iteration 147, loss = 0.26608766\n",
            "Iteration 148, loss = 0.26465313\n",
            "Iteration 149, loss = 0.26286057\n",
            "Iteration 150, loss = 0.26120535\n",
            "Iteration 151, loss = 0.25944208\n",
            "Iteration 152, loss = 0.25816136\n",
            "Iteration 153, loss = 0.25640632\n",
            "Iteration 154, loss = 0.25476384\n",
            "Iteration 155, loss = 0.25320682\n",
            "Iteration 156, loss = 0.25150800\n",
            "Iteration 157, loss = 0.24991791\n",
            "Iteration 158, loss = 0.24833633\n",
            "Iteration 159, loss = 0.24690235\n",
            "Iteration 160, loss = 0.24545370\n",
            "Iteration 161, loss = 0.24376479\n",
            "Iteration 162, loss = 0.24221749\n",
            "Iteration 163, loss = 0.24084449\n",
            "Iteration 164, loss = 0.23945962\n",
            "Iteration 165, loss = 0.23795589\n",
            "Iteration 166, loss = 0.23642274\n",
            "Iteration 167, loss = 0.23497322\n",
            "Iteration 168, loss = 0.23352135\n",
            "Iteration 169, loss = 0.23223126\n",
            "Iteration 170, loss = 0.23067411\n",
            "Iteration 171, loss = 0.22927653\n",
            "Iteration 172, loss = 0.22790818\n",
            "Iteration 173, loss = 0.22669904\n",
            "Iteration 174, loss = 0.22523432\n",
            "Iteration 175, loss = 0.22372273\n",
            "Iteration 176, loss = 0.22257533\n",
            "Iteration 177, loss = 0.22106281\n",
            "Iteration 178, loss = 0.21972215\n",
            "Iteration 179, loss = 0.21857634\n",
            "Iteration 180, loss = 0.21720675\n",
            "Iteration 181, loss = 0.21585849\n",
            "Iteration 182, loss = 0.21456500\n",
            "Iteration 183, loss = 0.21329805\n",
            "Iteration 184, loss = 0.21211661\n",
            "Iteration 185, loss = 0.21085114\n",
            "Iteration 186, loss = 0.20951541\n",
            "Iteration 187, loss = 0.20839448\n",
            "Iteration 188, loss = 0.20698991\n",
            "Iteration 189, loss = 0.20593734\n",
            "Iteration 190, loss = 0.20467861\n",
            "Iteration 191, loss = 0.20337401\n",
            "Iteration 192, loss = 0.20248919\n",
            "Iteration 193, loss = 0.20114849\n",
            "Iteration 194, loss = 0.20005214\n",
            "Iteration 195, loss = 0.19878071\n",
            "Iteration 196, loss = 0.19763848\n",
            "Iteration 197, loss = 0.19644336\n",
            "Iteration 198, loss = 0.19534126\n",
            "Iteration 199, loss = 0.19441097\n",
            "Iteration 200, loss = 0.19312204\n",
            "Iteration 201, loss = 0.19213427\n",
            "Iteration 202, loss = 0.19109284\n",
            "Iteration 203, loss = 0.19010104\n",
            "Iteration 204, loss = 0.18875986\n",
            "Iteration 205, loss = 0.18784620\n",
            "Iteration 206, loss = 0.18663817\n",
            "Iteration 207, loss = 0.18564187\n",
            "Iteration 208, loss = 0.18461807\n",
            "Iteration 209, loss = 0.18374205\n",
            "Iteration 210, loss = 0.18290603\n",
            "Iteration 211, loss = 0.18166360\n",
            "Iteration 212, loss = 0.18060726\n",
            "Iteration 213, loss = 0.17969390\n",
            "Iteration 214, loss = 0.17868471\n",
            "Iteration 215, loss = 0.17770849\n",
            "Iteration 216, loss = 0.17677445\n",
            "Iteration 217, loss = 0.17585887\n",
            "Iteration 218, loss = 0.17487632\n",
            "Iteration 219, loss = 0.17392559\n",
            "Iteration 220, loss = 0.17290315\n",
            "Iteration 221, loss = 0.17220070\n",
            "Iteration 222, loss = 0.17159078\n",
            "Iteration 223, loss = 0.17028275\n",
            "Iteration 224, loss = 0.16938525\n",
            "Iteration 225, loss = 0.16848773\n",
            "Iteration 226, loss = 0.16749963\n",
            "Iteration 227, loss = 0.16673108\n",
            "Iteration 228, loss = 0.16600182\n",
            "Iteration 229, loss = 0.16500884\n",
            "Iteration 230, loss = 0.16406134\n",
            "Iteration 231, loss = 0.16316533\n",
            "Iteration 232, loss = 0.16242405\n",
            "Iteration 233, loss = 0.16167690\n",
            "Iteration 234, loss = 0.16055761\n",
            "Iteration 235, loss = 0.15970542\n",
            "Iteration 236, loss = 0.15900792\n",
            "Iteration 237, loss = 0.15824137\n",
            "Iteration 238, loss = 0.15737553\n",
            "Iteration 239, loss = 0.15639319\n",
            "Iteration 240, loss = 0.15589350\n",
            "Iteration 241, loss = 0.15481853\n",
            "Iteration 242, loss = 0.15415065\n",
            "Iteration 243, loss = 0.15317906\n",
            "Iteration 244, loss = 0.15244901\n",
            "Iteration 245, loss = 0.15163765\n",
            "Iteration 246, loss = 0.15081819\n",
            "Iteration 247, loss = 0.14998690\n",
            "Iteration 248, loss = 0.14929330\n",
            "Iteration 249, loss = 0.14857949\n",
            "Iteration 250, loss = 0.14782106\n",
            "Iteration 251, loss = 0.14704535\n",
            "Iteration 252, loss = 0.14629434\n",
            "Iteration 253, loss = 0.14559967\n",
            "Iteration 254, loss = 0.14480791\n",
            "Iteration 255, loss = 0.14391241\n",
            "Iteration 256, loss = 0.14326882\n",
            "Iteration 257, loss = 0.14246501\n",
            "Iteration 258, loss = 0.14172216\n",
            "Iteration 259, loss = 0.14103937\n",
            "Iteration 260, loss = 0.14025841\n",
            "Iteration 261, loss = 0.13986727\n",
            "Iteration 262, loss = 0.13890658\n",
            "Iteration 263, loss = 0.13829109\n",
            "Iteration 264, loss = 0.13760495\n",
            "Iteration 265, loss = 0.13682509\n",
            "Iteration 266, loss = 0.13636264\n",
            "Iteration 267, loss = 0.13549292\n",
            "Iteration 268, loss = 0.13484089\n",
            "Iteration 269, loss = 0.13418448\n",
            "Iteration 270, loss = 0.13379941\n",
            "Iteration 271, loss = 0.13279163\n",
            "Iteration 272, loss = 0.13225038\n",
            "Iteration 273, loss = 0.13178593\n",
            "Iteration 274, loss = 0.13107061\n",
            "Iteration 275, loss = 0.13053872\n",
            "Iteration 276, loss = 0.12978638\n",
            "Iteration 277, loss = 0.12909405\n",
            "Iteration 278, loss = 0.12846439\n",
            "Iteration 279, loss = 0.12781287\n",
            "Iteration 280, loss = 0.12722408\n",
            "Iteration 281, loss = 0.12672391\n",
            "Iteration 282, loss = 0.12600529\n",
            "Iteration 283, loss = 0.12530399\n",
            "Iteration 284, loss = 0.12477918\n",
            "Iteration 285, loss = 0.12423160\n",
            "Iteration 286, loss = 0.12361496\n",
            "Iteration 287, loss = 0.12300021\n",
            "Iteration 288, loss = 0.12249103\n",
            "Iteration 289, loss = 0.12185523\n",
            "Iteration 290, loss = 0.12134421\n",
            "Iteration 291, loss = 0.12071011\n",
            "Iteration 292, loss = 0.12014984\n",
            "Iteration 293, loss = 0.11962521\n",
            "Iteration 294, loss = 0.11906313\n",
            "Iteration 295, loss = 0.11858725\n",
            "Iteration 296, loss = 0.11797585\n",
            "Iteration 297, loss = 0.11736529\n",
            "Iteration 298, loss = 0.11683087\n",
            "Iteration 299, loss = 0.11630191\n",
            "Iteration 300, loss = 0.11576011\n",
            "Iteration 301, loss = 0.11524837\n",
            "Iteration 302, loss = 0.11472315\n",
            "Iteration 303, loss = 0.11422871\n",
            "Iteration 304, loss = 0.11378420\n",
            "Iteration 305, loss = 0.11344643\n",
            "Iteration 306, loss = 0.11271583\n",
            "Iteration 307, loss = 0.11222445\n",
            "Iteration 308, loss = 0.11185003\n",
            "Iteration 309, loss = 0.11124527\n",
            "Iteration 310, loss = 0.11074767\n",
            "Iteration 311, loss = 0.11020439\n",
            "Iteration 312, loss = 0.10977733\n",
            "Iteration 313, loss = 0.10927339\n",
            "Iteration 314, loss = 0.10877200\n",
            "Iteration 315, loss = 0.10830326\n",
            "Iteration 316, loss = 0.10787080\n",
            "Iteration 317, loss = 0.10745302\n",
            "Iteration 318, loss = 0.10691581\n",
            "Iteration 319, loss = 0.10639278\n",
            "Iteration 320, loss = 0.10589995\n",
            "Iteration 321, loss = 0.10539947\n",
            "Iteration 322, loss = 0.10510049\n",
            "Iteration 323, loss = 0.10456086\n",
            "Iteration 324, loss = 0.10413728\n",
            "Iteration 325, loss = 0.10364033\n",
            "Iteration 326, loss = 0.10325232\n",
            "Iteration 327, loss = 0.10273682\n",
            "Iteration 328, loss = 0.10239769\n",
            "Iteration 329, loss = 0.10181619\n",
            "Iteration 330, loss = 0.10159778\n",
            "Iteration 331, loss = 0.10101039\n",
            "Iteration 332, loss = 0.10093924\n",
            "Iteration 333, loss = 0.10025816\n",
            "Iteration 334, loss = 0.09969457\n",
            "Iteration 335, loss = 0.09970179\n",
            "Iteration 336, loss = 0.09921075\n",
            "Iteration 337, loss = 0.09865973\n",
            "Iteration 338, loss = 0.09836045\n",
            "Iteration 339, loss = 0.09782738\n",
            "Iteration 340, loss = 0.09743755\n",
            "Iteration 341, loss = 0.09697929\n",
            "Iteration 342, loss = 0.09657618\n",
            "Iteration 343, loss = 0.09610142\n",
            "Iteration 344, loss = 0.09576029\n",
            "Iteration 345, loss = 0.09526506\n",
            "Iteration 346, loss = 0.09491889\n",
            "Iteration 347, loss = 0.09452303\n",
            "Iteration 348, loss = 0.09413691\n",
            "Iteration 349, loss = 0.09382030\n",
            "Iteration 350, loss = 0.09336437\n",
            "Iteration 351, loss = 0.09294635\n",
            "Iteration 352, loss = 0.09261780\n",
            "Iteration 353, loss = 0.09220937\n",
            "Iteration 354, loss = 0.09185132\n",
            "Iteration 355, loss = 0.09149200\n",
            "Iteration 356, loss = 0.09114436\n",
            "Iteration 357, loss = 0.09086469\n",
            "Iteration 358, loss = 0.09049341\n",
            "Iteration 359, loss = 0.09000959\n",
            "Iteration 360, loss = 0.08968622\n",
            "Iteration 361, loss = 0.08937601\n",
            "Iteration 362, loss = 0.08910254\n",
            "Iteration 363, loss = 0.08882891\n",
            "Iteration 364, loss = 0.08833657\n",
            "Iteration 365, loss = 0.08796922\n",
            "Iteration 366, loss = 0.08762177\n",
            "Iteration 367, loss = 0.08735733\n",
            "Iteration 368, loss = 0.08701305\n",
            "Iteration 369, loss = 0.08660592\n",
            "Iteration 370, loss = 0.08632930\n",
            "Iteration 371, loss = 0.08593095\n",
            "Iteration 372, loss = 0.08581685\n",
            "Iteration 373, loss = 0.08532479\n",
            "Iteration 374, loss = 0.08501117\n",
            "Iteration 375, loss = 0.08485757\n",
            "Iteration 376, loss = 0.08446551\n",
            "Iteration 377, loss = 0.08426243\n",
            "Iteration 378, loss = 0.08378779\n",
            "Iteration 379, loss = 0.08342539\n",
            "Iteration 380, loss = 0.08314957\n",
            "Iteration 381, loss = 0.08288644\n",
            "Iteration 382, loss = 0.08249016\n",
            "Iteration 383, loss = 0.08218756\n",
            "Iteration 384, loss = 0.08201920\n",
            "Iteration 385, loss = 0.08167007\n",
            "Iteration 386, loss = 0.08131833\n",
            "Iteration 387, loss = 0.08099602\n",
            "Iteration 388, loss = 0.08080412\n",
            "Iteration 389, loss = 0.08038010\n",
            "Iteration 390, loss = 0.08010510\n",
            "Iteration 391, loss = 0.07980450\n",
            "Iteration 392, loss = 0.07957765\n",
            "Iteration 393, loss = 0.07927236\n",
            "Iteration 394, loss = 0.07897806\n",
            "Iteration 395, loss = 0.07869387\n",
            "Iteration 396, loss = 0.07831366\n",
            "Iteration 397, loss = 0.07804472\n",
            "Iteration 398, loss = 0.07776934\n",
            "Iteration 399, loss = 0.07748257\n",
            "Iteration 400, loss = 0.07720922\n",
            "Iteration 401, loss = 0.07690939\n",
            "Iteration 402, loss = 0.07668750\n",
            "Iteration 403, loss = 0.07639565\n",
            "Iteration 404, loss = 0.07609069\n",
            "Iteration 405, loss = 0.07583907\n",
            "Iteration 406, loss = 0.07559191\n",
            "Iteration 407, loss = 0.07531174\n",
            "Iteration 408, loss = 0.07509445\n",
            "Iteration 409, loss = 0.07487514\n",
            "Iteration 410, loss = 0.07450780\n",
            "Iteration 411, loss = 0.07431894\n",
            "Iteration 412, loss = 0.07399765\n",
            "Iteration 413, loss = 0.07380218\n",
            "Iteration 414, loss = 0.07351317\n",
            "Iteration 415, loss = 0.07323908\n",
            "Iteration 416, loss = 0.07304961\n",
            "Iteration 417, loss = 0.07277619\n",
            "Iteration 418, loss = 0.07249190\n",
            "Iteration 419, loss = 0.07218815\n",
            "Iteration 420, loss = 0.07219770\n",
            "Iteration 421, loss = 0.07196258\n",
            "Iteration 422, loss = 0.07163476\n",
            "Iteration 423, loss = 0.07129272\n",
            "Iteration 424, loss = 0.07105833\n",
            "Iteration 425, loss = 0.07093230\n",
            "Iteration 426, loss = 0.07051443\n",
            "Iteration 427, loss = 0.07039187\n",
            "Iteration 428, loss = 0.07006612\n",
            "Iteration 429, loss = 0.06985476\n",
            "Iteration 430, loss = 0.06959504\n",
            "Iteration 431, loss = 0.06950950\n",
            "Iteration 432, loss = 0.06923326\n",
            "Iteration 433, loss = 0.06895100\n",
            "Iteration 434, loss = 0.06866314\n",
            "Iteration 435, loss = 0.06852097\n",
            "Iteration 436, loss = 0.06826392\n",
            "Iteration 437, loss = 0.06802909\n",
            "Iteration 438, loss = 0.06786663\n",
            "Iteration 439, loss = 0.06757535\n",
            "Iteration 440, loss = 0.06736857\n",
            "Iteration 441, loss = 0.06715913\n",
            "Iteration 442, loss = 0.06695559\n",
            "Iteration 443, loss = 0.06695980\n",
            "Iteration 444, loss = 0.06654462\n",
            "Iteration 445, loss = 0.06633388\n",
            "Iteration 446, loss = 0.06623406\n",
            "Iteration 447, loss = 0.06595430\n",
            "Iteration 448, loss = 0.06563716\n",
            "Iteration 449, loss = 0.06565358\n",
            "Iteration 450, loss = 0.06533591\n",
            "Iteration 451, loss = 0.06516241\n",
            "Iteration 452, loss = 0.06483699\n",
            "Iteration 453, loss = 0.06460188\n",
            "Iteration 454, loss = 0.06443794\n",
            "Iteration 455, loss = 0.06423213\n",
            "Iteration 456, loss = 0.06415996\n",
            "Iteration 457, loss = 0.06380589\n",
            "Iteration 458, loss = 0.06373279\n",
            "Iteration 459, loss = 0.06340457\n",
            "Iteration 460, loss = 0.06320799\n",
            "Iteration 461, loss = 0.06307577\n",
            "Iteration 462, loss = 0.06286098\n",
            "Iteration 463, loss = 0.06272619\n",
            "Iteration 464, loss = 0.06250791\n",
            "Iteration 465, loss = 0.06231853\n",
            "Iteration 466, loss = 0.06211649\n",
            "Iteration 467, loss = 0.06193234\n",
            "Iteration 468, loss = 0.06183496\n",
            "Iteration 469, loss = 0.06151474\n",
            "Iteration 470, loss = 0.06139780\n",
            "Iteration 471, loss = 0.06118182\n",
            "Iteration 472, loss = 0.06119221\n",
            "Iteration 473, loss = 0.06086546\n",
            "Iteration 474, loss = 0.06087854\n",
            "Iteration 475, loss = 0.06042877\n",
            "Iteration 476, loss = 0.06035474\n",
            "Iteration 477, loss = 0.06011496\n",
            "Iteration 478, loss = 0.05996714\n",
            "Iteration 479, loss = 0.05970271\n",
            "Iteration 480, loss = 0.05981227\n",
            "Iteration 481, loss = 0.05954270\n",
            "Iteration 482, loss = 0.05927863\n",
            "Iteration 483, loss = 0.05904671\n",
            "Iteration 484, loss = 0.05887423\n",
            "Iteration 485, loss = 0.05867822\n",
            "Iteration 486, loss = 0.05862702\n",
            "Iteration 487, loss = 0.05839362\n",
            "Iteration 488, loss = 0.05821170\n",
            "Iteration 489, loss = 0.05810410\n",
            "Iteration 490, loss = 0.05786207\n",
            "Iteration 491, loss = 0.05773426\n",
            "Iteration 492, loss = 0.05757569\n",
            "Iteration 493, loss = 0.05740668\n",
            "Iteration 494, loss = 0.05727921\n",
            "Iteration 495, loss = 0.05722236\n",
            "Iteration 496, loss = 0.05693602\n",
            "Iteration 497, loss = 0.05680199\n",
            "Iteration 498, loss = 0.05672256\n",
            "Iteration 499, loss = 0.05662380\n",
            "Iteration 500, loss = 0.05647356\n",
            "Iteration 501, loss = 0.05615659\n",
            "Iteration 502, loss = 0.05600844\n",
            "Iteration 503, loss = 0.05593093\n",
            "Iteration 504, loss = 0.05599893\n",
            "Iteration 505, loss = 0.05566186\n",
            "Iteration 506, loss = 0.05543494\n",
            "Iteration 507, loss = 0.05523545\n",
            "Iteration 508, loss = 0.05516553\n",
            "Iteration 509, loss = 0.05504197\n",
            "Iteration 510, loss = 0.05483480\n",
            "Iteration 511, loss = 0.05462161\n",
            "Iteration 512, loss = 0.05451430\n",
            "Iteration 513, loss = 0.05442510\n",
            "Iteration 514, loss = 0.05424607\n",
            "Iteration 515, loss = 0.05407133\n",
            "Iteration 516, loss = 0.05390848\n",
            "Iteration 517, loss = 0.05376484\n",
            "Iteration 518, loss = 0.05373859\n",
            "Iteration 519, loss = 0.05351376\n",
            "Iteration 520, loss = 0.05347006\n",
            "Iteration 521, loss = 0.05333159\n",
            "Iteration 522, loss = 0.05313207\n",
            "Iteration 523, loss = 0.05314440\n",
            "Iteration 524, loss = 0.05289283\n",
            "Iteration 525, loss = 0.05271781\n",
            "Iteration 526, loss = 0.05256928\n",
            "Iteration 527, loss = 0.05259147\n",
            "Iteration 528, loss = 0.05253179\n",
            "Iteration 529, loss = 0.05226972\n",
            "Iteration 530, loss = 0.05206776\n",
            "Iteration 531, loss = 0.05195759\n",
            "Iteration 532, loss = 0.05181590\n",
            "Iteration 533, loss = 0.05164397\n",
            "Iteration 534, loss = 0.05154737\n",
            "Iteration 535, loss = 0.05143814\n",
            "Iteration 536, loss = 0.05133101\n",
            "Iteration 537, loss = 0.05121991\n",
            "Iteration 538, loss = 0.05102689\n",
            "Iteration 539, loss = 0.05099289\n",
            "Iteration 540, loss = 0.05083007\n",
            "Iteration 541, loss = 0.05063013\n",
            "Iteration 542, loss = 0.05050310\n",
            "Iteration 543, loss = 0.05056136\n",
            "Iteration 544, loss = 0.05038074\n",
            "Iteration 545, loss = 0.05022320\n",
            "Iteration 546, loss = 0.05007450\n",
            "Iteration 547, loss = 0.05000631\n",
            "Iteration 548, loss = 0.04987366\n",
            "Iteration 549, loss = 0.04972119\n",
            "Iteration 550, loss = 0.04957636\n",
            "Iteration 551, loss = 0.04952804\n",
            "Iteration 552, loss = 0.04938424\n",
            "Iteration 553, loss = 0.04934595\n",
            "Iteration 554, loss = 0.04916300\n",
            "Iteration 555, loss = 0.04898236\n",
            "Iteration 556, loss = 0.04887187\n",
            "Iteration 557, loss = 0.04879413\n",
            "Iteration 558, loss = 0.04862811\n",
            "Iteration 559, loss = 0.04848916\n",
            "Iteration 560, loss = 0.04848380\n",
            "Iteration 561, loss = 0.04835420\n",
            "Iteration 562, loss = 0.04816142\n",
            "Iteration 563, loss = 0.04830174\n",
            "Iteration 564, loss = 0.04798198\n",
            "Iteration 565, loss = 0.04813533\n",
            "Iteration 566, loss = 0.04795289\n",
            "Iteration 567, loss = 0.04773700\n",
            "Iteration 568, loss = 0.04759430\n",
            "Iteration 569, loss = 0.04748339\n",
            "Iteration 570, loss = 0.04738092\n",
            "Iteration 571, loss = 0.04730591\n",
            "Iteration 572, loss = 0.04731564\n",
            "Iteration 573, loss = 0.04702156\n",
            "Iteration 574, loss = 0.04695539\n",
            "Iteration 575, loss = 0.04690281\n",
            "Iteration 576, loss = 0.04671100\n",
            "Iteration 577, loss = 0.04683814\n",
            "Iteration 578, loss = 0.04652040\n",
            "Iteration 579, loss = 0.04638230\n",
            "Iteration 580, loss = 0.04632897\n",
            "Iteration 581, loss = 0.04622621\n",
            "Iteration 582, loss = 0.04606499\n",
            "Iteration 583, loss = 0.04599473\n",
            "Iteration 584, loss = 0.04604054\n",
            "Iteration 585, loss = 0.04587851\n",
            "Iteration 586, loss = 0.04572087\n",
            "Iteration 587, loss = 0.04570309\n",
            "Iteration 588, loss = 0.04554658\n",
            "Iteration 589, loss = 0.04548165\n",
            "Iteration 590, loss = 0.04527599\n",
            "Iteration 591, loss = 0.04527318\n",
            "Iteration 592, loss = 0.04533180\n",
            "Iteration 593, loss = 0.04509075\n",
            "Iteration 594, loss = 0.04493057\n",
            "Iteration 595, loss = 0.04482340\n",
            "Iteration 596, loss = 0.04484413\n",
            "Iteration 597, loss = 0.04487009\n",
            "Iteration 598, loss = 0.04462215\n",
            "Iteration 599, loss = 0.04452273\n",
            "Iteration 600, loss = 0.04445461\n",
            "Iteration 601, loss = 0.04437402\n",
            "Iteration 602, loss = 0.04431477\n",
            "Iteration 603, loss = 0.04423636\n",
            "Iteration 604, loss = 0.04410835\n",
            "Iteration 605, loss = 0.04407372\n",
            "Iteration 606, loss = 0.04381574\n",
            "Iteration 607, loss = 0.04370801\n",
            "Iteration 608, loss = 0.04370538\n",
            "Iteration 609, loss = 0.04366415\n",
            "Iteration 610, loss = 0.04358874\n",
            "Iteration 611, loss = 0.04341109\n",
            "Iteration 612, loss = 0.04328706\n",
            "Iteration 613, loss = 0.04324538\n",
            "Iteration 614, loss = 0.04321566\n",
            "Iteration 615, loss = 0.04320302\n",
            "Iteration 616, loss = 0.04302397\n",
            "Iteration 617, loss = 0.04280997\n",
            "Iteration 618, loss = 0.04297703\n",
            "Iteration 619, loss = 0.04292225\n",
            "Iteration 620, loss = 0.04279790\n",
            "Iteration 621, loss = 0.04260413\n",
            "Iteration 622, loss = 0.04258594\n",
            "Iteration 623, loss = 0.04248140\n",
            "Iteration 624, loss = 0.04242087\n",
            "Iteration 625, loss = 0.04218486\n",
            "Iteration 626, loss = 0.04220428\n",
            "Iteration 627, loss = 0.04213108\n",
            "Iteration 628, loss = 0.04219589\n",
            "Iteration 629, loss = 0.04194259\n",
            "Iteration 630, loss = 0.04197594\n",
            "Iteration 631, loss = 0.04174724\n",
            "Iteration 632, loss = 0.04166943\n",
            "Iteration 633, loss = 0.04167339\n",
            "Iteration 634, loss = 0.04170626\n",
            "Iteration 635, loss = 0.04145138\n",
            "Iteration 636, loss = 0.04144140\n",
            "Iteration 637, loss = 0.04148210\n",
            "Iteration 638, loss = 0.04131212\n",
            "Iteration 639, loss = 0.04104549\n",
            "Iteration 640, loss = 0.04100969\n",
            "Iteration 641, loss = 0.04113248\n",
            "Iteration 642, loss = 0.04115804\n",
            "Iteration 643, loss = 0.04103725\n",
            "Iteration 644, loss = 0.04107091\n",
            "Iteration 645, loss = 0.04072900\n",
            "Iteration 646, loss = 0.04074965\n",
            "Iteration 647, loss = 0.04057249\n",
            "Iteration 648, loss = 0.04045900\n",
            "Iteration 649, loss = 0.04048572\n",
            "Iteration 650, loss = 0.04038281\n",
            "Iteration 651, loss = 0.04024407\n",
            "Iteration 652, loss = 0.04017201\n",
            "Iteration 653, loss = 0.04002602\n",
            "Iteration 654, loss = 0.04000962\n",
            "Iteration 655, loss = 0.04005518\n",
            "Iteration 656, loss = 0.03997970\n",
            "Iteration 657, loss = 0.04006593\n",
            "Iteration 658, loss = 0.03983618\n",
            "Iteration 659, loss = 0.03981426\n",
            "Iteration 660, loss = 0.03968954\n",
            "Iteration 661, loss = 0.03965861\n",
            "Iteration 662, loss = 0.03957147\n",
            "Iteration 663, loss = 0.03950363\n",
            "Iteration 664, loss = 0.03937553\n",
            "Iteration 665, loss = 0.03928461\n",
            "Iteration 666, loss = 0.03928840\n",
            "Iteration 667, loss = 0.03924585\n",
            "Iteration 668, loss = 0.03907958\n",
            "Iteration 669, loss = 0.03902247\n",
            "Iteration 670, loss = 0.03894933\n",
            "Iteration 671, loss = 0.03897199\n",
            "Iteration 672, loss = 0.03879639\n",
            "Iteration 673, loss = 0.03898127\n",
            "Iteration 674, loss = 0.03876060\n",
            "Iteration 675, loss = 0.03871350\n",
            "Iteration 676, loss = 0.03862472\n",
            "Iteration 677, loss = 0.03850447\n",
            "Iteration 678, loss = 0.03845291\n",
            "Iteration 679, loss = 0.03849525\n",
            "Iteration 680, loss = 0.03837471\n",
            "Iteration 681, loss = 0.03825091\n",
            "Iteration 682, loss = 0.03828519\n",
            "Iteration 683, loss = 0.03810303\n",
            "Iteration 684, loss = 0.03807556\n",
            "Iteration 685, loss = 0.03801292\n",
            "Iteration 686, loss = 0.03797885\n",
            "Iteration 687, loss = 0.03803549\n",
            "Iteration 688, loss = 0.03778730\n",
            "Iteration 689, loss = 0.03766464\n",
            "Iteration 690, loss = 0.03764841\n",
            "Iteration 691, loss = 0.03774166\n",
            "Iteration 692, loss = 0.03768328\n",
            "Iteration 693, loss = 0.03753584\n",
            "Iteration 694, loss = 0.03745848\n",
            "Iteration 695, loss = 0.03745621\n",
            "Iteration 696, loss = 0.03735105\n",
            "Iteration 697, loss = 0.03730484\n",
            "Iteration 698, loss = 0.03722607\n",
            "Iteration 699, loss = 0.03723438\n",
            "Iteration 700, loss = 0.03713736\n",
            "Iteration 701, loss = 0.03702284\n",
            "Iteration 702, loss = 0.03701035\n",
            "Iteration 703, loss = 0.03708047\n",
            "Iteration 704, loss = 0.03686971\n",
            "Iteration 705, loss = 0.03676722\n",
            "Iteration 706, loss = 0.03675494\n",
            "Iteration 707, loss = 0.03669546\n",
            "Iteration 708, loss = 0.03668468\n",
            "Iteration 709, loss = 0.03659543\n",
            "Iteration 710, loss = 0.03669190\n",
            "Iteration 711, loss = 0.03648980\n",
            "Iteration 712, loss = 0.03667960\n",
            "Iteration 713, loss = 0.03633583\n",
            "Iteration 714, loss = 0.03651184\n",
            "Iteration 715, loss = 0.03631811\n",
            "Iteration 716, loss = 0.03632584\n",
            "Iteration 717, loss = 0.03620869\n",
            "Iteration 718, loss = 0.03620687\n",
            "Iteration 719, loss = 0.03599380\n",
            "Iteration 720, loss = 0.03598342\n",
            "Iteration 721, loss = 0.03597269\n",
            "Iteration 722, loss = 0.03589710\n",
            "Iteration 723, loss = 0.03591338\n",
            "Iteration 724, loss = 0.03586504\n",
            "Iteration 725, loss = 0.03570522\n",
            "Iteration 726, loss = 0.03575751\n",
            "Iteration 727, loss = 0.03553649\n",
            "Iteration 728, loss = 0.03554569\n",
            "Iteration 729, loss = 0.03554539\n",
            "Iteration 730, loss = 0.03545754\n",
            "Iteration 731, loss = 0.03551387\n",
            "Iteration 732, loss = 0.03542100\n",
            "Iteration 733, loss = 0.03533324\n",
            "Iteration 734, loss = 0.03524413\n",
            "Iteration 735, loss = 0.03530287\n",
            "Iteration 736, loss = 0.03517301\n",
            "Iteration 737, loss = 0.03512388\n",
            "Iteration 738, loss = 0.03506914\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00000000000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = 9)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-S4o3IczVP"
      },
      "source": [
        "\n",
        "\n",
        "> **Vamos testar o modelo?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1q9nsbSjdu23"
      },
      "outputs": [],
      "source": [
        "previsoes = modelo.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PlSJE8fAUL",
        "outputId": "501c2371-84dd-4c64-f216-be2b03707888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events'], dtype='<U20')"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWziqc5fV8m"
      },
      "source": [
        "\n",
        "\n",
        "> **Será se o modelo acertou?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92H3KOtfN5E",
        "outputId": "864bf803-aaec-436b-ee87-2bd4956fc72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "285    no-recurrence-events\n",
              "281    no-recurrence-events\n",
              "33     no-recurrence-events\n",
              "211       recurrence-events\n",
              "93     no-recurrence-events\n",
              "               ...         \n",
              "228    no-recurrence-events\n",
              "371       recurrence-events\n",
              "176    no-recurrence-events\n",
              "272    no-recurrence-events\n",
              "3      no-recurrence-events\n",
              "Name: Class, Length: 81, dtype: object"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9MxYOIfmwv",
        "outputId": "d81573e1-45aa-4ea6-d922-c25e1a623343"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8024691358024691"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_teste,previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3D5bvushr9W",
        "outputId": "45328eb3-d2be-4d4c-c5e1-248e262d7d9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[35, 12],\n",
              "       [ 4, 30]])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "confusion_matrix(y_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wX15YT-7j-c9",
        "outputId": "db8b53f9-e72b-4257-c955-a229ab056d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8024691358024691"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIixOPw1kw-z",
        "outputId": "5b4735c0-0e3f-4981-b422-f4a5d25f5c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.90      0.74      0.81        47\n",
            "   recurrence-events       0.71      0.88      0.79        34\n",
            "\n",
            "            accuracy                           0.80        81\n",
            "           macro avg       0.81      0.81      0.80        81\n",
            "        weighted avg       0.82      0.80      0.80        81\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_teste, previsoes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avaliando topologia: (10,)\n",
            "Iteration 1, loss = 0.85479268\n",
            "Iteration 2, loss = 0.84043792\n",
            "Iteration 3, loss = 0.82646737\n",
            "Iteration 4, loss = 0.81281406\n",
            "Iteration 5, loss = 0.79931584\n",
            "Iteration 6, loss = 0.78776728\n",
            "Iteration 7, loss = 0.77451301\n",
            "Iteration 8, loss = 0.76314419\n",
            "Iteration 9, loss = 0.75275122\n",
            "Iteration 10, loss = 0.74207070\n",
            "Iteration 11, loss = 0.73312098\n",
            "Iteration 12, loss = 0.72379119\n",
            "Iteration 13, loss = 0.71532931\n",
            "Iteration 14, loss = 0.70708671\n",
            "Iteration 15, loss = 0.69956669\n",
            "Iteration 16, loss = 0.69309677\n",
            "Iteration 17, loss = 0.68621720\n",
            "Iteration 18, loss = 0.68079641\n",
            "Iteration 19, loss = 0.67528331\n",
            "Iteration 20, loss = 0.67019230\n",
            "Iteration 21, loss = 0.66594240\n",
            "Iteration 22, loss = 0.66158883\n",
            "Iteration 23, loss = 0.65745465\n",
            "Iteration 24, loss = 0.65422283\n",
            "Iteration 25, loss = 0.65066251\n",
            "Iteration 26, loss = 0.64789573\n",
            "Iteration 27, loss = 0.64513553\n",
            "Iteration 28, loss = 0.64225979\n",
            "Iteration 29, loss = 0.63979957\n",
            "Iteration 30, loss = 0.63765722\n",
            "Iteration 31, loss = 0.63557758\n",
            "Iteration 32, loss = 0.63351128\n",
            "Iteration 33, loss = 0.63135786\n",
            "Iteration 34, loss = 0.62953202\n",
            "Iteration 35, loss = 0.62761785\n",
            "Iteration 36, loss = 0.62584075\n",
            "Iteration 37, loss = 0.62401731\n",
            "Iteration 38, loss = 0.62236935\n",
            "Iteration 39, loss = 0.62060545\n",
            "Iteration 40, loss = 0.61894405\n",
            "Iteration 41, loss = 0.61727976\n",
            "Iteration 42, loss = 0.61564137\n",
            "Iteration 43, loss = 0.61409236\n",
            "Iteration 44, loss = 0.61259589\n",
            "Iteration 45, loss = 0.61105782\n",
            "Iteration 46, loss = 0.60958160\n",
            "Iteration 47, loss = 0.60819580\n",
            "Iteration 48, loss = 0.60669076\n",
            "Iteration 49, loss = 0.60537739\n",
            "Iteration 50, loss = 0.60401280\n",
            "Iteration 51, loss = 0.60261064\n",
            "Iteration 52, loss = 0.60140482\n",
            "Iteration 53, loss = 0.60013893\n",
            "Iteration 54, loss = 0.59882954\n",
            "Iteration 55, loss = 0.59758933\n",
            "Iteration 56, loss = 0.59645741\n",
            "Iteration 57, loss = 0.59533282\n",
            "Iteration 58, loss = 0.59411985\n",
            "Iteration 59, loss = 0.59306144\n",
            "Iteration 60, loss = 0.59191272\n",
            "Iteration 61, loss = 0.59081276\n",
            "Iteration 62, loss = 0.58970737\n",
            "Iteration 63, loss = 0.58859015\n",
            "Iteration 64, loss = 0.58758539\n",
            "Iteration 65, loss = 0.58653036\n",
            "Iteration 66, loss = 0.58550801\n",
            "Iteration 67, loss = 0.58444476\n",
            "Iteration 68, loss = 0.58346952\n",
            "Iteration 69, loss = 0.58241206\n",
            "Iteration 70, loss = 0.58155943\n",
            "Iteration 71, loss = 0.58049925\n",
            "Iteration 72, loss = 0.57955912\n",
            "Iteration 73, loss = 0.57861652\n",
            "Iteration 74, loss = 0.57770098\n",
            "Iteration 75, loss = 0.57681835\n",
            "Iteration 76, loss = 0.57593697\n",
            "Iteration 77, loss = 0.57508622\n",
            "Iteration 78, loss = 0.57420108\n",
            "Iteration 79, loss = 0.57340748\n",
            "Iteration 80, loss = 0.57254065\n",
            "Iteration 81, loss = 0.57170481\n",
            "Iteration 82, loss = 0.57089685\n",
            "Iteration 83, loss = 0.57008177\n",
            "Iteration 84, loss = 0.56924675\n",
            "Iteration 85, loss = 0.56843920\n",
            "Iteration 86, loss = 0.56764249\n",
            "Iteration 87, loss = 0.56690497\n",
            "Iteration 88, loss = 0.56606929\n",
            "Iteration 89, loss = 0.56529615\n",
            "Iteration 90, loss = 0.56451105\n",
            "Iteration 91, loss = 0.56369630\n",
            "Iteration 92, loss = 0.56296270\n",
            "Iteration 93, loss = 0.56222424\n",
            "Iteration 94, loss = 0.56146983\n",
            "Iteration 95, loss = 0.56068225\n",
            "Iteration 96, loss = 0.55993473\n",
            "Iteration 97, loss = 0.55923925\n",
            "Iteration 98, loss = 0.55846636\n",
            "Iteration 99, loss = 0.55771092\n",
            "Iteration 100, loss = 0.55699835\n",
            "Iteration 101, loss = 0.55624636\n",
            "Iteration 102, loss = 0.55553511\n",
            "Iteration 103, loss = 0.55482091\n",
            "Iteration 104, loss = 0.55408152\n",
            "Iteration 105, loss = 0.55341600\n",
            "Iteration 106, loss = 0.55269092\n",
            "Iteration 107, loss = 0.55202054\n",
            "Iteration 108, loss = 0.55130930\n",
            "Iteration 109, loss = 0.55061706\n",
            "Iteration 110, loss = 0.54994919\n",
            "Iteration 111, loss = 0.54925575\n",
            "Iteration 112, loss = 0.54860424\n",
            "Iteration 113, loss = 0.54789827\n",
            "Iteration 114, loss = 0.54725133\n",
            "Iteration 115, loss = 0.54655470\n",
            "Iteration 116, loss = 0.54587022\n",
            "Iteration 117, loss = 0.54527054\n",
            "Iteration 118, loss = 0.54456416\n",
            "Iteration 119, loss = 0.54387758\n",
            "Iteration 120, loss = 0.54326339\n",
            "Iteration 121, loss = 0.54255044\n",
            "Iteration 122, loss = 0.54190164\n",
            "Iteration 123, loss = 0.54124287\n",
            "Iteration 124, loss = 0.54060693\n",
            "Iteration 125, loss = 0.53992870\n",
            "Iteration 126, loss = 0.53927812\n",
            "Iteration 127, loss = 0.53866702\n",
            "Iteration 128, loss = 0.53802477\n",
            "Iteration 129, loss = 0.53737042\n",
            "Iteration 130, loss = 0.53676866\n",
            "Iteration 131, loss = 0.53611615\n",
            "Iteration 132, loss = 0.53549144\n",
            "Iteration 133, loss = 0.53485554\n",
            "Iteration 134, loss = 0.53420975\n",
            "Iteration 135, loss = 0.53366988\n",
            "Iteration 136, loss = 0.53298441\n",
            "Iteration 137, loss = 0.53234757\n",
            "Iteration 138, loss = 0.53174973\n",
            "Iteration 139, loss = 0.53112931\n",
            "Iteration 140, loss = 0.53054102\n",
            "Iteration 141, loss = 0.52990504\n",
            "Iteration 142, loss = 0.52925857\n",
            "Iteration 143, loss = 0.52860256\n",
            "Iteration 144, loss = 0.52800315\n",
            "Iteration 145, loss = 0.52737888\n",
            "Iteration 146, loss = 0.52679949\n",
            "Iteration 147, loss = 0.52613475\n",
            "Iteration 148, loss = 0.52553840\n",
            "Iteration 149, loss = 0.52491456\n",
            "Iteration 150, loss = 0.52429342\n",
            "Iteration 151, loss = 0.52365632\n",
            "Iteration 152, loss = 0.52306894\n",
            "Iteration 153, loss = 0.52246037\n",
            "Iteration 154, loss = 0.52182281\n",
            "Iteration 155, loss = 0.52123873\n",
            "Iteration 156, loss = 0.52062498\n",
            "Iteration 157, loss = 0.51998301\n",
            "Iteration 158, loss = 0.51940261\n",
            "Iteration 159, loss = 0.51878247\n",
            "Iteration 160, loss = 0.51820998\n",
            "Iteration 161, loss = 0.51762539\n",
            "Iteration 162, loss = 0.51699187\n",
            "Iteration 163, loss = 0.51636878\n",
            "Iteration 164, loss = 0.51578351\n",
            "Iteration 165, loss = 0.51517954\n",
            "Iteration 166, loss = 0.51458063\n",
            "Iteration 167, loss = 0.51395668\n",
            "Iteration 168, loss = 0.51333897\n",
            "Iteration 169, loss = 0.51275306\n",
            "Iteration 170, loss = 0.51216966\n",
            "Iteration 171, loss = 0.51153053\n",
            "Iteration 172, loss = 0.51094341\n",
            "Iteration 173, loss = 0.51034227\n",
            "Iteration 174, loss = 0.50981756\n",
            "Iteration 175, loss = 0.50916257\n",
            "Iteration 176, loss = 0.50851955\n",
            "Iteration 177, loss = 0.50789041\n",
            "Iteration 178, loss = 0.50726961\n",
            "Iteration 179, loss = 0.50664890\n",
            "Iteration 180, loss = 0.50598919\n",
            "Iteration 181, loss = 0.50538079\n",
            "Iteration 182, loss = 0.50475888\n",
            "Iteration 183, loss = 0.50413147\n",
            "Iteration 184, loss = 0.50349996\n",
            "Iteration 185, loss = 0.50290459\n",
            "Iteration 186, loss = 0.50229347\n",
            "Iteration 187, loss = 0.50167834\n",
            "Iteration 188, loss = 0.50101420\n",
            "Iteration 189, loss = 0.50042144\n",
            "Iteration 190, loss = 0.49987634\n",
            "Iteration 191, loss = 0.49917753\n",
            "Iteration 192, loss = 0.49854649\n",
            "Iteration 193, loss = 0.49793749\n",
            "Iteration 194, loss = 0.49733034\n",
            "Iteration 195, loss = 0.49666948\n",
            "Iteration 196, loss = 0.49605555\n",
            "Iteration 197, loss = 0.49545650\n",
            "Iteration 198, loss = 0.49481554\n",
            "Iteration 199, loss = 0.49424813\n",
            "Iteration 200, loss = 0.49367721\n",
            "Iteration 201, loss = 0.49299282\n",
            "Iteration 202, loss = 0.49237636\n",
            "Iteration 203, loss = 0.49182183\n",
            "Iteration 204, loss = 0.49119718\n",
            "Iteration 205, loss = 0.49054975\n",
            "Iteration 206, loss = 0.48997926\n",
            "Iteration 207, loss = 0.48943714\n",
            "Iteration 208, loss = 0.48879271\n",
            "Iteration 209, loss = 0.48820374\n",
            "Iteration 210, loss = 0.48758802\n",
            "Iteration 211, loss = 0.48695544\n",
            "Iteration 212, loss = 0.48653876\n",
            "Iteration 213, loss = 0.48580005\n",
            "Iteration 214, loss = 0.48515611\n",
            "Iteration 215, loss = 0.48460979\n",
            "Iteration 216, loss = 0.48404184\n",
            "Iteration 217, loss = 0.48347315\n",
            "Iteration 218, loss = 0.48280345\n",
            "Iteration 219, loss = 0.48216176\n",
            "Iteration 220, loss = 0.48157311\n",
            "Iteration 221, loss = 0.48098632\n",
            "Iteration 222, loss = 0.48034758\n",
            "Iteration 223, loss = 0.47978288\n",
            "Iteration 224, loss = 0.47914395\n",
            "Iteration 225, loss = 0.47858118\n",
            "Iteration 226, loss = 0.47796941\n",
            "Iteration 227, loss = 0.47734135\n",
            "Iteration 228, loss = 0.47674095\n",
            "Iteration 229, loss = 0.47611983\n",
            "Iteration 230, loss = 0.47550965\n",
            "Iteration 231, loss = 0.47490509\n",
            "Iteration 232, loss = 0.47425419\n",
            "Iteration 233, loss = 0.47375726\n",
            "Iteration 234, loss = 0.47303219\n",
            "Iteration 235, loss = 0.47247569\n",
            "Iteration 236, loss = 0.47182686\n",
            "Iteration 237, loss = 0.47121245\n",
            "Iteration 238, loss = 0.47057557\n",
            "Iteration 239, loss = 0.46998998\n",
            "Iteration 240, loss = 0.46935656\n",
            "Iteration 241, loss = 0.46876187\n",
            "Iteration 242, loss = 0.46812270\n",
            "Iteration 243, loss = 0.46752557\n",
            "Iteration 244, loss = 0.46691918\n",
            "Iteration 245, loss = 0.46631438\n",
            "Iteration 246, loss = 0.46569265\n",
            "Iteration 247, loss = 0.46512339\n",
            "Iteration 248, loss = 0.46451619\n",
            "Iteration 249, loss = 0.46385326\n",
            "Iteration 250, loss = 0.46320724\n",
            "Iteration 251, loss = 0.46268953\n",
            "Iteration 252, loss = 0.46205859\n",
            "Iteration 253, loss = 0.46150464\n",
            "Iteration 254, loss = 0.46089172\n",
            "Iteration 255, loss = 0.46028219\n",
            "Iteration 256, loss = 0.45967451\n",
            "Iteration 257, loss = 0.45909398\n",
            "Iteration 258, loss = 0.45845476\n",
            "Iteration 259, loss = 0.45783245\n",
            "Iteration 260, loss = 0.45723601\n",
            "Iteration 261, loss = 0.45662323\n",
            "Iteration 262, loss = 0.45598589\n",
            "Iteration 263, loss = 0.45540050\n",
            "Iteration 264, loss = 0.45480198\n",
            "Iteration 265, loss = 0.45421995\n",
            "Iteration 266, loss = 0.45360157\n",
            "Iteration 267, loss = 0.45299356\n",
            "Iteration 268, loss = 0.45239559\n",
            "Iteration 269, loss = 0.45180976\n",
            "Iteration 270, loss = 0.45119509\n",
            "Iteration 271, loss = 0.45060801\n",
            "Iteration 272, loss = 0.45002109\n",
            "Iteration 273, loss = 0.44942421\n",
            "Iteration 274, loss = 0.44881984\n",
            "Iteration 275, loss = 0.44825667\n",
            "Iteration 276, loss = 0.44785018\n",
            "Iteration 277, loss = 0.44703604\n",
            "Iteration 278, loss = 0.44642464\n",
            "Iteration 279, loss = 0.44585244\n",
            "Iteration 280, loss = 0.44522578\n",
            "Iteration 281, loss = 0.44462101\n",
            "Iteration 282, loss = 0.44406279\n",
            "Iteration 283, loss = 0.44342856\n",
            "Iteration 284, loss = 0.44289260\n",
            "Iteration 285, loss = 0.44224883\n",
            "Iteration 286, loss = 0.44165845\n",
            "Iteration 287, loss = 0.44103241\n",
            "Iteration 288, loss = 0.44042199\n",
            "Iteration 289, loss = 0.43982829\n",
            "Iteration 290, loss = 0.43928701\n",
            "Iteration 291, loss = 0.43864818\n",
            "Iteration 292, loss = 0.43804766\n",
            "Iteration 293, loss = 0.43741816\n",
            "Iteration 294, loss = 0.43684338\n",
            "Iteration 295, loss = 0.43623822\n",
            "Iteration 296, loss = 0.43568887\n",
            "Iteration 297, loss = 0.43504578\n",
            "Iteration 298, loss = 0.43443151\n",
            "Iteration 299, loss = 0.43382978\n",
            "Iteration 300, loss = 0.43319565\n",
            "Iteration 301, loss = 0.43270646\n",
            "Iteration 302, loss = 0.43201017\n",
            "Iteration 303, loss = 0.43144122\n",
            "Iteration 304, loss = 0.43076710\n",
            "Iteration 305, loss = 0.43021022\n",
            "Iteration 306, loss = 0.42958537\n",
            "Iteration 307, loss = 0.42898225\n",
            "Iteration 308, loss = 0.42835472\n",
            "Iteration 309, loss = 0.42773537\n",
            "Iteration 310, loss = 0.42715948\n",
            "Iteration 311, loss = 0.42654326\n",
            "Iteration 312, loss = 0.42600890\n",
            "Iteration 313, loss = 0.42536237\n",
            "Iteration 314, loss = 0.42473243\n",
            "Iteration 315, loss = 0.42412583\n",
            "Iteration 316, loss = 0.42357156\n",
            "Iteration 317, loss = 0.42295356\n",
            "Iteration 318, loss = 0.42233091\n",
            "Iteration 319, loss = 0.42175530\n",
            "Iteration 320, loss = 0.42117070\n",
            "Iteration 321, loss = 0.42054233\n",
            "Iteration 322, loss = 0.41993793\n",
            "Iteration 323, loss = 0.41933352\n",
            "Iteration 324, loss = 0.41881354\n",
            "Iteration 325, loss = 0.41816036\n",
            "Iteration 326, loss = 0.41760173\n",
            "Iteration 327, loss = 0.41695530\n",
            "Iteration 328, loss = 0.41638977\n",
            "Iteration 329, loss = 0.41573191\n",
            "Iteration 330, loss = 0.41516906\n",
            "Iteration 331, loss = 0.41454075\n",
            "Iteration 332, loss = 0.41397481\n",
            "Iteration 333, loss = 0.41337036\n",
            "Iteration 334, loss = 0.41278290\n",
            "Iteration 335, loss = 0.41216036\n",
            "Iteration 336, loss = 0.41158085\n",
            "Iteration 337, loss = 0.41095824\n",
            "Iteration 338, loss = 0.41038605\n",
            "Iteration 339, loss = 0.40982071\n",
            "Iteration 340, loss = 0.40920241\n",
            "Iteration 341, loss = 0.40862951\n",
            "Iteration 342, loss = 0.40802867\n",
            "Iteration 343, loss = 0.40743671\n",
            "Iteration 344, loss = 0.40691847\n",
            "Iteration 345, loss = 0.40629421\n",
            "Iteration 346, loss = 0.40566645\n",
            "Iteration 347, loss = 0.40509539\n",
            "Iteration 348, loss = 0.40451477\n",
            "Iteration 349, loss = 0.40393292\n",
            "Iteration 350, loss = 0.40337215\n",
            "Iteration 351, loss = 0.40272389\n",
            "Iteration 352, loss = 0.40215731\n",
            "Iteration 353, loss = 0.40159006\n",
            "Iteration 354, loss = 0.40105591\n",
            "Iteration 355, loss = 0.40044365\n",
            "Iteration 356, loss = 0.39986858\n",
            "Iteration 357, loss = 0.39928295\n",
            "Iteration 358, loss = 0.39872291\n",
            "Iteration 359, loss = 0.39807219\n",
            "Iteration 360, loss = 0.39745555\n",
            "Iteration 361, loss = 0.39686788\n",
            "Iteration 362, loss = 0.39628560\n",
            "Iteration 363, loss = 0.39575986\n",
            "Iteration 364, loss = 0.39510355\n",
            "Iteration 365, loss = 0.39455013\n",
            "Iteration 366, loss = 0.39395920\n",
            "Iteration 367, loss = 0.39333686\n",
            "Iteration 368, loss = 0.39277539\n",
            "Iteration 369, loss = 0.39225942\n",
            "Iteration 370, loss = 0.39157125\n",
            "Iteration 371, loss = 0.39104956\n",
            "Iteration 372, loss = 0.39038258\n",
            "Iteration 373, loss = 0.38978446\n",
            "Iteration 374, loss = 0.38924556\n",
            "Iteration 375, loss = 0.38866163\n",
            "Iteration 376, loss = 0.38811517\n",
            "Iteration 377, loss = 0.38756187\n",
            "Iteration 378, loss = 0.38694526\n",
            "Iteration 379, loss = 0.38634303\n",
            "Iteration 380, loss = 0.38574789\n",
            "Iteration 381, loss = 0.38516261\n",
            "Iteration 382, loss = 0.38459761\n",
            "Iteration 383, loss = 0.38414973\n",
            "Iteration 384, loss = 0.38348246\n",
            "Iteration 385, loss = 0.38295550\n",
            "Iteration 386, loss = 0.38229096\n",
            "Iteration 387, loss = 0.38173668\n",
            "Iteration 388, loss = 0.38128447\n",
            "Iteration 389, loss = 0.38064985\n",
            "Iteration 390, loss = 0.38010052\n",
            "Iteration 391, loss = 0.37953267\n",
            "Iteration 392, loss = 0.37898681\n",
            "Iteration 393, loss = 0.37847193\n",
            "Iteration 394, loss = 0.37786329\n",
            "Iteration 395, loss = 0.37736197\n",
            "Iteration 396, loss = 0.37675572\n",
            "Iteration 397, loss = 0.37624518\n",
            "Iteration 398, loss = 0.37574842\n",
            "Iteration 399, loss = 0.37509601\n",
            "Iteration 400, loss = 0.37452969\n",
            "Iteration 401, loss = 0.37400115\n",
            "Iteration 402, loss = 0.37347202\n",
            "Iteration 403, loss = 0.37287517\n",
            "Iteration 404, loss = 0.37233144\n",
            "Iteration 405, loss = 0.37181910\n",
            "Iteration 406, loss = 0.37120456\n",
            "Iteration 407, loss = 0.37071548\n",
            "Iteration 408, loss = 0.37011372\n",
            "Iteration 409, loss = 0.36956799\n",
            "Iteration 410, loss = 0.36901762\n",
            "Iteration 411, loss = 0.36845829\n",
            "Iteration 412, loss = 0.36802107\n",
            "Iteration 413, loss = 0.36737702\n",
            "Iteration 414, loss = 0.36693104\n",
            "Iteration 415, loss = 0.36627593\n",
            "Iteration 416, loss = 0.36573680\n",
            "Iteration 417, loss = 0.36523678\n",
            "Iteration 418, loss = 0.36466924\n",
            "Iteration 419, loss = 0.36410550\n",
            "Iteration 420, loss = 0.36355974\n",
            "Iteration 421, loss = 0.36301712\n",
            "Iteration 422, loss = 0.36251981\n",
            "Iteration 423, loss = 0.36200901\n",
            "Iteration 424, loss = 0.36141311\n",
            "Iteration 425, loss = 0.36095090\n",
            "Iteration 426, loss = 0.36033980\n",
            "Iteration 427, loss = 0.35980589\n",
            "Iteration 428, loss = 0.35926652\n",
            "Iteration 429, loss = 0.35878704\n",
            "Iteration 430, loss = 0.35823307\n",
            "Iteration 431, loss = 0.35770398\n",
            "Iteration 432, loss = 0.35715545\n",
            "Iteration 433, loss = 0.35670903\n",
            "Iteration 434, loss = 0.35610508\n",
            "Iteration 435, loss = 0.35558689\n",
            "Iteration 436, loss = 0.35510794\n",
            "Iteration 437, loss = 0.35453773\n",
            "Iteration 438, loss = 0.35411474\n",
            "Iteration 439, loss = 0.35352626\n",
            "Iteration 440, loss = 0.35298303\n",
            "Iteration 441, loss = 0.35247940\n",
            "Iteration 442, loss = 0.35195512\n",
            "Iteration 443, loss = 0.35143839\n",
            "Iteration 444, loss = 0.35091041\n",
            "Iteration 445, loss = 0.35045981\n",
            "Iteration 446, loss = 0.34983908\n",
            "Iteration 447, loss = 0.34931753\n",
            "Iteration 448, loss = 0.34880703\n",
            "Iteration 449, loss = 0.34838673\n",
            "Iteration 450, loss = 0.34784441\n",
            "Iteration 451, loss = 0.34733514\n",
            "Iteration 452, loss = 0.34682634\n",
            "Iteration 453, loss = 0.34627920\n",
            "Iteration 454, loss = 0.34577463\n",
            "Iteration 455, loss = 0.34522373\n",
            "Iteration 456, loss = 0.34473206\n",
            "Iteration 457, loss = 0.34419322\n",
            "Iteration 458, loss = 0.34371847\n",
            "Iteration 459, loss = 0.34328984\n",
            "Iteration 460, loss = 0.34270523\n",
            "Iteration 461, loss = 0.34218697\n",
            "Iteration 462, loss = 0.34169802\n",
            "Iteration 463, loss = 0.34121007\n",
            "Iteration 464, loss = 0.34070532\n",
            "Iteration 465, loss = 0.34020158\n",
            "Iteration 466, loss = 0.33972010\n",
            "Iteration 467, loss = 0.33930831\n",
            "Iteration 468, loss = 0.33872999\n",
            "Iteration 469, loss = 0.33834518\n",
            "Iteration 470, loss = 0.33776207\n",
            "Iteration 471, loss = 0.33730061\n",
            "Iteration 472, loss = 0.33687851\n",
            "Iteration 473, loss = 0.33633361\n",
            "Iteration 474, loss = 0.33583932\n",
            "Iteration 475, loss = 0.33538540\n",
            "Iteration 476, loss = 0.33492706\n",
            "Iteration 477, loss = 0.33441050\n",
            "Iteration 478, loss = 0.33396402\n",
            "Iteration 479, loss = 0.33349355\n",
            "Iteration 480, loss = 0.33298401\n",
            "Iteration 481, loss = 0.33254043\n",
            "Iteration 482, loss = 0.33206835\n",
            "Iteration 483, loss = 0.33158297\n",
            "Iteration 484, loss = 0.33121557\n",
            "Iteration 485, loss = 0.33075609\n",
            "Iteration 486, loss = 0.33026220\n",
            "Iteration 487, loss = 0.32976588\n",
            "Iteration 488, loss = 0.32925911\n",
            "Iteration 489, loss = 0.32885979\n",
            "Iteration 490, loss = 0.32843593\n",
            "Iteration 491, loss = 0.32794505\n",
            "Iteration 492, loss = 0.32746313\n",
            "Iteration 493, loss = 0.32702890\n",
            "Iteration 494, loss = 0.32653173\n",
            "Iteration 495, loss = 0.32610779\n",
            "Iteration 496, loss = 0.32562229\n",
            "Iteration 497, loss = 0.32514741\n",
            "Iteration 498, loss = 0.32478095\n",
            "Iteration 499, loss = 0.32432578\n",
            "Iteration 500, loss = 0.32373697\n",
            "Iteration 501, loss = 0.32337856\n",
            "Iteration 502, loss = 0.32285584\n",
            "Iteration 503, loss = 0.32242274\n",
            "Iteration 504, loss = 0.32194555\n",
            "Iteration 505, loss = 0.32149512\n",
            "Iteration 506, loss = 0.32107075\n",
            "Iteration 507, loss = 0.32056700\n",
            "Iteration 508, loss = 0.32011101\n",
            "Iteration 509, loss = 0.31963405\n",
            "Iteration 510, loss = 0.31916507\n",
            "Iteration 511, loss = 0.31874327\n",
            "Iteration 512, loss = 0.31829058\n",
            "Iteration 513, loss = 0.31784732\n",
            "Iteration 514, loss = 0.31747199\n",
            "Iteration 515, loss = 0.31698994\n",
            "Iteration 516, loss = 0.31648990\n",
            "Iteration 517, loss = 0.31607507\n",
            "Iteration 518, loss = 0.31560414\n",
            "Iteration 519, loss = 0.31521105\n",
            "Iteration 520, loss = 0.31474056\n",
            "Iteration 521, loss = 0.31429799\n",
            "Iteration 522, loss = 0.31384889\n",
            "Iteration 523, loss = 0.31340451\n",
            "Iteration 524, loss = 0.31295883\n",
            "Iteration 525, loss = 0.31249081\n",
            "Iteration 526, loss = 0.31207663\n",
            "Iteration 527, loss = 0.31168750\n",
            "Iteration 528, loss = 0.31120467\n",
            "Iteration 529, loss = 0.31077005\n",
            "Iteration 530, loss = 0.31039449\n",
            "Iteration 531, loss = 0.30998459\n",
            "Iteration 532, loss = 0.30949721\n",
            "Iteration 533, loss = 0.30902272\n",
            "Iteration 534, loss = 0.30857193\n",
            "Iteration 535, loss = 0.30823950\n",
            "Iteration 536, loss = 0.30775886\n",
            "Iteration 537, loss = 0.30732871\n",
            "Iteration 538, loss = 0.30694443\n",
            "Iteration 539, loss = 0.30643729\n",
            "Iteration 540, loss = 0.30605058\n",
            "Iteration 541, loss = 0.30559891\n",
            "Iteration 542, loss = 0.30517280\n",
            "Iteration 543, loss = 0.30486360\n",
            "Iteration 544, loss = 0.30432792\n",
            "Iteration 545, loss = 0.30386383\n",
            "Iteration 546, loss = 0.30355584\n",
            "Iteration 547, loss = 0.30310846\n",
            "Iteration 548, loss = 0.30265875\n",
            "Iteration 549, loss = 0.30224929\n",
            "Iteration 550, loss = 0.30179320\n",
            "Iteration 551, loss = 0.30142610\n",
            "Iteration 552, loss = 0.30099638\n",
            "Iteration 553, loss = 0.30056627\n",
            "Iteration 554, loss = 0.30014561\n",
            "Iteration 555, loss = 0.29974880\n",
            "Iteration 556, loss = 0.29935751\n",
            "Iteration 557, loss = 0.29901397\n",
            "Iteration 558, loss = 0.29857797\n",
            "Iteration 559, loss = 0.29820389\n",
            "Iteration 560, loss = 0.29774431\n",
            "Iteration 561, loss = 0.29748096\n",
            "Iteration 562, loss = 0.29705811\n",
            "Iteration 563, loss = 0.29652359\n",
            "Iteration 564, loss = 0.29616865\n",
            "Iteration 565, loss = 0.29573620\n",
            "Iteration 566, loss = 0.29533561\n",
            "Iteration 567, loss = 0.29495754\n",
            "Iteration 568, loss = 0.29456177\n",
            "Iteration 569, loss = 0.29414875\n",
            "Iteration 570, loss = 0.29382214\n",
            "Iteration 571, loss = 0.29335451\n",
            "Iteration 572, loss = 0.29299320\n",
            "Iteration 573, loss = 0.29261136\n",
            "Iteration 574, loss = 0.29223513\n",
            "Iteration 575, loss = 0.29183280\n",
            "Iteration 576, loss = 0.29145922\n",
            "Iteration 577, loss = 0.29106993\n",
            "Iteration 578, loss = 0.29072226\n",
            "Iteration 579, loss = 0.29028356\n",
            "Iteration 580, loss = 0.29002090\n",
            "Iteration 581, loss = 0.28951792\n",
            "Iteration 582, loss = 0.28916986\n",
            "Iteration 583, loss = 0.28883140\n",
            "Iteration 584, loss = 0.28840281\n",
            "Iteration 585, loss = 0.28808255\n",
            "Iteration 586, loss = 0.28774289\n",
            "Iteration 587, loss = 0.28732949\n",
            "Iteration 588, loss = 0.28696147\n",
            "Iteration 589, loss = 0.28652749\n",
            "Iteration 590, loss = 0.28618297\n",
            "Iteration 591, loss = 0.28581567\n",
            "Iteration 592, loss = 0.28559809\n",
            "Iteration 593, loss = 0.28519163\n",
            "Iteration 594, loss = 0.28480630\n",
            "Iteration 595, loss = 0.28443212\n",
            "Iteration 596, loss = 0.28406749\n",
            "Iteration 597, loss = 0.28373200\n",
            "Iteration 598, loss = 0.28334777\n",
            "Iteration 599, loss = 0.28301466\n",
            "Iteration 600, loss = 0.28269196\n",
            "Iteration 601, loss = 0.28229190\n",
            "Iteration 602, loss = 0.28194468\n",
            "Iteration 603, loss = 0.28160776\n",
            "Iteration 604, loss = 0.28122636\n",
            "Iteration 605, loss = 0.28088500\n",
            "Iteration 606, loss = 0.28052077\n",
            "Iteration 607, loss = 0.28014625\n",
            "Iteration 608, loss = 0.27982454\n",
            "Iteration 609, loss = 0.27945888\n",
            "Iteration 610, loss = 0.27917307\n",
            "Iteration 611, loss = 0.27875812\n",
            "Iteration 612, loss = 0.27841978\n",
            "Iteration 613, loss = 0.27809049\n",
            "Iteration 614, loss = 0.27770686\n",
            "Iteration 615, loss = 0.27741185\n",
            "Iteration 616, loss = 0.27707093\n",
            "Iteration 617, loss = 0.27668520\n",
            "Iteration 618, loss = 0.27635724\n",
            "Iteration 619, loss = 0.27599683\n",
            "Iteration 620, loss = 0.27563192\n",
            "Iteration 621, loss = 0.27527879\n",
            "Iteration 622, loss = 0.27494552\n",
            "Iteration 623, loss = 0.27463812\n",
            "Iteration 624, loss = 0.27430999\n",
            "Iteration 625, loss = 0.27394516\n",
            "Iteration 626, loss = 0.27360445\n",
            "Iteration 627, loss = 0.27324032\n",
            "Iteration 628, loss = 0.27292639\n",
            "Iteration 629, loss = 0.27257540\n",
            "Iteration 630, loss = 0.27224031\n",
            "Iteration 631, loss = 0.27194335\n",
            "Iteration 632, loss = 0.27155285\n",
            "Iteration 633, loss = 0.27123048\n",
            "Iteration 634, loss = 0.27088622\n",
            "Iteration 635, loss = 0.27056532\n",
            "Iteration 636, loss = 0.27022734\n",
            "Iteration 637, loss = 0.26985234\n",
            "Iteration 638, loss = 0.26958142\n",
            "Iteration 639, loss = 0.26922808\n",
            "Iteration 640, loss = 0.26888220\n",
            "Iteration 641, loss = 0.26861764\n",
            "Iteration 642, loss = 0.26821668\n",
            "Iteration 643, loss = 0.26790669\n",
            "Iteration 644, loss = 0.26757636\n",
            "Iteration 645, loss = 0.26722236\n",
            "Iteration 646, loss = 0.26691506\n",
            "Iteration 647, loss = 0.26660474\n",
            "Iteration 648, loss = 0.26628727\n",
            "Iteration 649, loss = 0.26596369\n",
            "Iteration 650, loss = 0.26561314\n",
            "Iteration 651, loss = 0.26539317\n",
            "Iteration 652, loss = 0.26503844\n",
            "Iteration 653, loss = 0.26465706\n",
            "Iteration 654, loss = 0.26439384\n",
            "Iteration 655, loss = 0.26404570\n",
            "Iteration 656, loss = 0.26368370\n",
            "Iteration 657, loss = 0.26340539\n",
            "Iteration 658, loss = 0.26312354\n",
            "Iteration 659, loss = 0.26276657\n",
            "Iteration 660, loss = 0.26245161\n",
            "Iteration 661, loss = 0.26207073\n",
            "Iteration 662, loss = 0.26182471\n",
            "Iteration 663, loss = 0.26145810\n",
            "Iteration 664, loss = 0.26112911\n",
            "Iteration 665, loss = 0.26077229\n",
            "Iteration 666, loss = 0.26044678\n",
            "Iteration 667, loss = 0.26013432\n",
            "Iteration 668, loss = 0.25979988\n",
            "Iteration 669, loss = 0.25946810\n",
            "Iteration 670, loss = 0.25914287\n",
            "Iteration 671, loss = 0.25883019\n",
            "Iteration 672, loss = 0.25854821\n",
            "Iteration 673, loss = 0.25820686\n",
            "Iteration 674, loss = 0.25789004\n",
            "Iteration 675, loss = 0.25758398\n",
            "Iteration 676, loss = 0.25726443\n",
            "Iteration 677, loss = 0.25690246\n",
            "Iteration 678, loss = 0.25665423\n",
            "Iteration 679, loss = 0.25629198\n",
            "Iteration 680, loss = 0.25598227\n",
            "Iteration 681, loss = 0.25576077\n",
            "Iteration 682, loss = 0.25539321\n",
            "Iteration 683, loss = 0.25503707\n",
            "Iteration 684, loss = 0.25475878\n",
            "Iteration 685, loss = 0.25446369\n",
            "Iteration 686, loss = 0.25415693\n",
            "Iteration 687, loss = 0.25384429\n",
            "Iteration 688, loss = 0.25355298\n",
            "Iteration 689, loss = 0.25322845\n",
            "Iteration 690, loss = 0.25292109\n",
            "Iteration 691, loss = 0.25268100\n",
            "Iteration 692, loss = 0.25229906\n",
            "Iteration 693, loss = 0.25199298\n",
            "Iteration 694, loss = 0.25169898\n",
            "Iteration 695, loss = 0.25143121\n",
            "Iteration 696, loss = 0.25111251\n",
            "Iteration 697, loss = 0.25081501\n",
            "Iteration 698, loss = 0.25052424\n",
            "Iteration 699, loss = 0.25026438\n",
            "Iteration 700, loss = 0.24997252\n",
            "Iteration 701, loss = 0.24965425\n",
            "Iteration 702, loss = 0.24942756\n",
            "Iteration 703, loss = 0.24914459\n",
            "Iteration 704, loss = 0.24885328\n",
            "Iteration 705, loss = 0.24873011\n",
            "Iteration 706, loss = 0.24838297\n",
            "Iteration 707, loss = 0.24798213\n",
            "Iteration 708, loss = 0.24764144\n",
            "Iteration 709, loss = 0.24747709\n",
            "Iteration 710, loss = 0.24709014\n",
            "Iteration 711, loss = 0.24683554\n",
            "Iteration 712, loss = 0.24653255\n",
            "Iteration 713, loss = 0.24619114\n",
            "Iteration 714, loss = 0.24590334\n",
            "Iteration 715, loss = 0.24563480\n",
            "Iteration 716, loss = 0.24535894\n",
            "Iteration 717, loss = 0.24507855\n",
            "Iteration 718, loss = 0.24479141\n",
            "Iteration 719, loss = 0.24452399\n",
            "Iteration 720, loss = 0.24429681\n",
            "Iteration 721, loss = 0.24398899\n",
            "Iteration 722, loss = 0.24372791\n",
            "Iteration 723, loss = 0.24343152\n",
            "Iteration 724, loss = 0.24316167\n",
            "Iteration 725, loss = 0.24290486\n",
            "Iteration 726, loss = 0.24260134\n",
            "Iteration 727, loss = 0.24238685\n",
            "Iteration 728, loss = 0.24207337\n",
            "Iteration 729, loss = 0.24180842\n",
            "Iteration 730, loss = 0.24159141\n",
            "Iteration 731, loss = 0.24128825\n",
            "Iteration 732, loss = 0.24097622\n",
            "Iteration 733, loss = 0.24069340\n",
            "Iteration 734, loss = 0.24043751\n",
            "Iteration 735, loss = 0.24018215\n",
            "Iteration 736, loss = 0.23990661\n",
            "Iteration 737, loss = 0.23964208\n",
            "Iteration 738, loss = 0.23936420\n",
            "Iteration 739, loss = 0.23905998\n",
            "Iteration 740, loss = 0.23889220\n",
            "Iteration 741, loss = 0.23867516\n",
            "Iteration 742, loss = 0.23842914\n",
            "Iteration 743, loss = 0.23803757\n",
            "Iteration 744, loss = 0.23779416\n",
            "Iteration 745, loss = 0.23761324\n",
            "Iteration 746, loss = 0.23725675\n",
            "Iteration 747, loss = 0.23700409\n",
            "Iteration 748, loss = 0.23673076\n",
            "Iteration 749, loss = 0.23651211\n",
            "Iteration 750, loss = 0.23629186\n",
            "Iteration 751, loss = 0.23596501\n",
            "Iteration 752, loss = 0.23576052\n",
            "Iteration 753, loss = 0.23547807\n",
            "Iteration 754, loss = 0.23524363\n",
            "Iteration 755, loss = 0.23495668\n",
            "Iteration 756, loss = 0.23477047\n",
            "Iteration 757, loss = 0.23445919\n",
            "Iteration 758, loss = 0.23424543\n",
            "Iteration 759, loss = 0.23395283\n",
            "Iteration 760, loss = 0.23373322\n",
            "Iteration 761, loss = 0.23351274\n",
            "Iteration 762, loss = 0.23318592\n",
            "Iteration 763, loss = 0.23297720\n",
            "Iteration 764, loss = 0.23269452\n",
            "Iteration 765, loss = 0.23243088\n",
            "Iteration 766, loss = 0.23216533\n",
            "Iteration 767, loss = 0.23191166\n",
            "Iteration 768, loss = 0.23167648\n",
            "Iteration 769, loss = 0.23147985\n",
            "Iteration 770, loss = 0.23118360\n",
            "Iteration 771, loss = 0.23100100\n",
            "Iteration 772, loss = 0.23070003\n",
            "Iteration 773, loss = 0.23040367\n",
            "Iteration 774, loss = 0.23016919\n",
            "Iteration 775, loss = 0.22991531\n",
            "Iteration 776, loss = 0.22969659\n",
            "Iteration 777, loss = 0.22945953\n",
            "Iteration 778, loss = 0.22924865\n",
            "Iteration 779, loss = 0.22898745\n",
            "Iteration 780, loss = 0.22869417\n",
            "Iteration 781, loss = 0.22856578\n",
            "Iteration 782, loss = 0.22834763\n",
            "Iteration 783, loss = 0.22800727\n",
            "Iteration 784, loss = 0.22780762\n",
            "Iteration 785, loss = 0.22753967\n",
            "Iteration 786, loss = 0.22727003\n",
            "Iteration 787, loss = 0.22707964\n",
            "Iteration 788, loss = 0.22678836\n",
            "Iteration 789, loss = 0.22657599\n",
            "Iteration 790, loss = 0.22632721\n",
            "Iteration 791, loss = 0.22617221\n",
            "Iteration 792, loss = 0.22587922\n",
            "Iteration 793, loss = 0.22560708\n",
            "Iteration 794, loss = 0.22553700\n",
            "Iteration 795, loss = 0.22517186\n",
            "Iteration 796, loss = 0.22491154\n",
            "Iteration 797, loss = 0.22466299\n",
            "Iteration 798, loss = 0.22442741\n",
            "Iteration 799, loss = 0.22421781\n",
            "Iteration 800, loss = 0.22399652\n",
            "Iteration 801, loss = 0.22375949\n",
            "Iteration 802, loss = 0.22349817\n",
            "Iteration 803, loss = 0.22327039\n",
            "Iteration 804, loss = 0.22302297\n",
            "Iteration 805, loss = 0.22280454\n",
            "Iteration 806, loss = 0.22256691\n",
            "Iteration 807, loss = 0.22233566\n",
            "Iteration 808, loss = 0.22207110\n",
            "Iteration 809, loss = 0.22191856\n",
            "Iteration 810, loss = 0.22171679\n",
            "Iteration 811, loss = 0.22140646\n",
            "Iteration 812, loss = 0.22120898\n",
            "Iteration 813, loss = 0.22097891\n",
            "Iteration 814, loss = 0.22073373\n",
            "Iteration 815, loss = 0.22048823\n",
            "Iteration 816, loss = 0.22022654\n",
            "Iteration 817, loss = 0.22002667\n",
            "Iteration 818, loss = 0.21979773\n",
            "Iteration 819, loss = 0.21962285\n",
            "Iteration 820, loss = 0.21937459\n",
            "Iteration 821, loss = 0.21909753\n",
            "Iteration 822, loss = 0.21888887\n",
            "Iteration 823, loss = 0.21859301\n",
            "Iteration 824, loss = 0.21837319\n",
            "Iteration 825, loss = 0.21824587\n",
            "Iteration 826, loss = 0.21797587\n",
            "Iteration 827, loss = 0.21777397\n",
            "Iteration 828, loss = 0.21748108\n",
            "Iteration 829, loss = 0.21725735\n",
            "Iteration 830, loss = 0.21702188\n",
            "Iteration 831, loss = 0.21680687\n",
            "Iteration 832, loss = 0.21657051\n",
            "Iteration 833, loss = 0.21638150\n",
            "Iteration 834, loss = 0.21619384\n",
            "Iteration 835, loss = 0.21595309\n",
            "Iteration 836, loss = 0.21568624\n",
            "Iteration 837, loss = 0.21547854\n",
            "Iteration 838, loss = 0.21528235\n",
            "Iteration 839, loss = 0.21499029\n",
            "Iteration 840, loss = 0.21477838\n",
            "Iteration 841, loss = 0.21458376\n",
            "Iteration 842, loss = 0.21437334\n",
            "Iteration 843, loss = 0.21422091\n",
            "Iteration 844, loss = 0.21400495\n",
            "Iteration 845, loss = 0.21379786\n",
            "Iteration 846, loss = 0.21355175\n",
            "Iteration 847, loss = 0.21329322\n",
            "Iteration 848, loss = 0.21308778\n",
            "Iteration 849, loss = 0.21282892\n",
            "Iteration 850, loss = 0.21259912\n",
            "Iteration 851, loss = 0.21243384\n",
            "Iteration 852, loss = 0.21220230\n",
            "Iteration 853, loss = 0.21198103\n",
            "Iteration 854, loss = 0.21179390\n",
            "Iteration 855, loss = 0.21155168\n",
            "Iteration 856, loss = 0.21131626\n",
            "Iteration 857, loss = 0.21106934\n",
            "Iteration 858, loss = 0.21090032\n",
            "Iteration 859, loss = 0.21066440\n",
            "Iteration 860, loss = 0.21043587\n",
            "Iteration 861, loss = 0.21024890\n",
            "Iteration 862, loss = 0.21005121\n",
            "Iteration 863, loss = 0.20977740\n",
            "Iteration 864, loss = 0.20957647\n",
            "Iteration 865, loss = 0.20932488\n",
            "Iteration 866, loss = 0.20910246\n",
            "Iteration 867, loss = 0.20897829\n",
            "Iteration 868, loss = 0.20872361\n",
            "Iteration 869, loss = 0.20850100\n",
            "Iteration 870, loss = 0.20829893\n",
            "Iteration 871, loss = 0.20804211\n",
            "Iteration 872, loss = 0.20784961\n",
            "Iteration 873, loss = 0.20764791\n",
            "Iteration 874, loss = 0.20738113\n",
            "Iteration 875, loss = 0.20722723\n",
            "Iteration 876, loss = 0.20704507\n",
            "Iteration 877, loss = 0.20676735\n",
            "Iteration 878, loss = 0.20655769\n",
            "Iteration 879, loss = 0.20630866\n",
            "Iteration 880, loss = 0.20617008\n",
            "Iteration 881, loss = 0.20589654\n",
            "Iteration 882, loss = 0.20571422\n",
            "Iteration 883, loss = 0.20553643\n",
            "Iteration 884, loss = 0.20534366\n",
            "Iteration 885, loss = 0.20507926\n",
            "Iteration 886, loss = 0.20486762\n",
            "Iteration 887, loss = 0.20468045\n",
            "Iteration 888, loss = 0.20446246\n",
            "Iteration 889, loss = 0.20428453\n",
            "Iteration 890, loss = 0.20406151\n",
            "Iteration 891, loss = 0.20385702\n",
            "Iteration 892, loss = 0.20373830\n",
            "Iteration 893, loss = 0.20341558\n",
            "Iteration 894, loss = 0.20331155\n",
            "Iteration 895, loss = 0.20301020\n",
            "Iteration 896, loss = 0.20283740\n",
            "Iteration 897, loss = 0.20271698\n",
            "Iteration 898, loss = 0.20252046\n",
            "Iteration 899, loss = 0.20244574\n",
            "Iteration 900, loss = 0.20205307\n",
            "Iteration 901, loss = 0.20185754\n",
            "Iteration 902, loss = 0.20164051\n",
            "Iteration 903, loss = 0.20143354\n",
            "Iteration 904, loss = 0.20125655\n",
            "Iteration 905, loss = 0.20102563\n",
            "Iteration 906, loss = 0.20086392\n",
            "Iteration 907, loss = 0.20062375\n",
            "Iteration 908, loss = 0.20041194\n",
            "Iteration 909, loss = 0.20023081\n",
            "Iteration 910, loss = 0.20002965\n",
            "Iteration 911, loss = 0.19983137\n",
            "Iteration 912, loss = 0.19966001\n",
            "Iteration 913, loss = 0.19944496\n",
            "Iteration 914, loss = 0.19923932\n",
            "Iteration 915, loss = 0.19906378\n",
            "Iteration 916, loss = 0.19883346\n",
            "Iteration 917, loss = 0.19871941\n",
            "Iteration 918, loss = 0.19845781\n",
            "Iteration 919, loss = 0.19828483\n",
            "Iteration 920, loss = 0.19805433\n",
            "Iteration 921, loss = 0.19785282\n",
            "Iteration 922, loss = 0.19769203\n",
            "Iteration 923, loss = 0.19747579\n",
            "Iteration 924, loss = 0.19726826\n",
            "Iteration 925, loss = 0.19708343\n",
            "Iteration 926, loss = 0.19689295\n",
            "Iteration 927, loss = 0.19667344\n",
            "Iteration 928, loss = 0.19649175\n",
            "Iteration 929, loss = 0.19634406\n",
            "Iteration 930, loss = 0.19613816\n",
            "Iteration 931, loss = 0.19598496\n",
            "Iteration 932, loss = 0.19580017\n",
            "Iteration 933, loss = 0.19553662\n",
            "Iteration 934, loss = 0.19532716\n",
            "Iteration 935, loss = 0.19518501\n",
            "Iteration 936, loss = 0.19500618\n",
            "Iteration 937, loss = 0.19487183\n",
            "Iteration 938, loss = 0.19465347\n",
            "Iteration 939, loss = 0.19440223\n",
            "Iteration 940, loss = 0.19421273\n",
            "Iteration 941, loss = 0.19403579\n",
            "Iteration 942, loss = 0.19387503\n",
            "Iteration 943, loss = 0.19369117\n",
            "Iteration 944, loss = 0.19348081\n",
            "Iteration 945, loss = 0.19333132\n",
            "Iteration 946, loss = 0.19308502\n",
            "Iteration 947, loss = 0.19293483\n",
            "Iteration 948, loss = 0.19273957\n",
            "Iteration 949, loss = 0.19263412\n",
            "Iteration 950, loss = 0.19240117\n",
            "Iteration 951, loss = 0.19232778\n",
            "Iteration 952, loss = 0.19215969\n",
            "Iteration 953, loss = 0.19191845\n",
            "Iteration 954, loss = 0.19173401\n",
            "Iteration 955, loss = 0.19157322\n",
            "Iteration 956, loss = 0.19131209\n",
            "Iteration 957, loss = 0.19114876\n",
            "Iteration 958, loss = 0.19095500\n",
            "Iteration 959, loss = 0.19078922\n",
            "Iteration 960, loss = 0.19063057\n",
            "Iteration 961, loss = 0.19040864\n",
            "Iteration 962, loss = 0.19027619\n",
            "Iteration 963, loss = 0.19008906\n",
            "Iteration 964, loss = 0.18988744\n",
            "Iteration 965, loss = 0.18983927\n",
            "Iteration 966, loss = 0.18953613\n",
            "Iteration 967, loss = 0.18939217\n",
            "Iteration 968, loss = 0.18920806\n",
            "Iteration 969, loss = 0.18902570\n",
            "Iteration 970, loss = 0.18881167\n",
            "Iteration 971, loss = 0.18869601\n",
            "Iteration 972, loss = 0.18848610\n",
            "Iteration 973, loss = 0.18832598\n",
            "Iteration 974, loss = 0.18811650\n",
            "Iteration 975, loss = 0.18795376\n",
            "Iteration 976, loss = 0.18777749\n",
            "Iteration 977, loss = 0.18759057\n",
            "Iteration 978, loss = 0.18742916\n",
            "Iteration 979, loss = 0.18724319\n",
            "Iteration 980, loss = 0.18711400\n",
            "Iteration 981, loss = 0.18685767\n",
            "Iteration 982, loss = 0.18690240\n",
            "Iteration 983, loss = 0.18655316\n",
            "Iteration 984, loss = 0.18637887\n",
            "Iteration 985, loss = 0.18616910\n",
            "Iteration 986, loss = 0.18598960\n",
            "Iteration 987, loss = 0.18588760\n",
            "Iteration 988, loss = 0.18568106\n",
            "Iteration 989, loss = 0.18554356\n",
            "Iteration 990, loss = 0.18539685\n",
            "Iteration 991, loss = 0.18508282\n",
            "Iteration 992, loss = 0.18489459\n",
            "Iteration 993, loss = 0.18480436\n",
            "Iteration 994, loss = 0.18464249\n",
            "Iteration 995, loss = 0.18442987\n",
            "Iteration 996, loss = 0.18426041\n",
            "Iteration 997, loss = 0.18411805\n",
            "Iteration 998, loss = 0.18391626\n",
            "Iteration 999, loss = 0.18374698\n",
            "Iteration 1000, loss = 0.18356762\n",
            "Acurácia: 0.7037037037037037\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.87      0.57      0.69        47\n",
            "   recurrence-events       0.60      0.88      0.71        34\n",
            "\n",
            "            accuracy                           0.70        81\n",
            "           macro avg       0.74      0.73      0.70        81\n",
            "        weighted avg       0.76      0.70      0.70        81\n",
            "\n",
            "[[27 20]\n",
            " [ 4 30]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Avaliando topologia: (20,)\n",
            "Iteration 1, loss = 0.77077685\n",
            "Iteration 2, loss = 0.76182073\n",
            "Iteration 3, loss = 0.75280710\n",
            "Iteration 4, loss = 0.74487419\n",
            "Iteration 5, loss = 0.73694058\n",
            "Iteration 6, loss = 0.72909355\n",
            "Iteration 7, loss = 0.72204148\n",
            "Iteration 8, loss = 0.71520911\n",
            "Iteration 9, loss = 0.70831220\n",
            "Iteration 10, loss = 0.70170483\n",
            "Iteration 11, loss = 0.69525171\n",
            "Iteration 12, loss = 0.68912605\n",
            "Iteration 13, loss = 0.68403528\n",
            "Iteration 14, loss = 0.67851239\n",
            "Iteration 15, loss = 0.67329892\n",
            "Iteration 16, loss = 0.66827105\n",
            "Iteration 17, loss = 0.66356488\n",
            "Iteration 18, loss = 0.65874981\n",
            "Iteration 19, loss = 0.65448275\n",
            "Iteration 20, loss = 0.64990818\n",
            "Iteration 21, loss = 0.64635882\n",
            "Iteration 22, loss = 0.64217179\n",
            "Iteration 23, loss = 0.63828081\n",
            "Iteration 24, loss = 0.63465659\n",
            "Iteration 25, loss = 0.63108406\n",
            "Iteration 26, loss = 0.62757965\n",
            "Iteration 27, loss = 0.62456085\n",
            "Iteration 28, loss = 0.62119881\n",
            "Iteration 29, loss = 0.61808926\n",
            "Iteration 30, loss = 0.61505793\n",
            "Iteration 31, loss = 0.61235615\n",
            "Iteration 32, loss = 0.60930940\n",
            "Iteration 33, loss = 0.60660094\n",
            "Iteration 34, loss = 0.60421776\n",
            "Iteration 35, loss = 0.60155403\n",
            "Iteration 36, loss = 0.59895275\n",
            "Iteration 37, loss = 0.59644215\n",
            "Iteration 38, loss = 0.59423753\n",
            "Iteration 39, loss = 0.59196368\n",
            "Iteration 40, loss = 0.58985644\n",
            "Iteration 41, loss = 0.58785423\n",
            "Iteration 42, loss = 0.58561672\n",
            "Iteration 43, loss = 0.58356011\n",
            "Iteration 44, loss = 0.58177284\n",
            "Iteration 45, loss = 0.57977550\n",
            "Iteration 46, loss = 0.57780204\n",
            "Iteration 47, loss = 0.57595298\n",
            "Iteration 48, loss = 0.57424549\n",
            "Iteration 49, loss = 0.57244160\n",
            "Iteration 50, loss = 0.57069990\n",
            "Iteration 51, loss = 0.56897780\n",
            "Iteration 52, loss = 0.56734060\n",
            "Iteration 53, loss = 0.56564804\n",
            "Iteration 54, loss = 0.56410445\n",
            "Iteration 55, loss = 0.56251474\n",
            "Iteration 56, loss = 0.56106931\n",
            "Iteration 57, loss = 0.55961829\n",
            "Iteration 58, loss = 0.55808199\n",
            "Iteration 59, loss = 0.55668486\n",
            "Iteration 60, loss = 0.55534827\n",
            "Iteration 61, loss = 0.55392586\n",
            "Iteration 62, loss = 0.55260802\n",
            "Iteration 63, loss = 0.55136280\n",
            "Iteration 64, loss = 0.55006871\n",
            "Iteration 65, loss = 0.54887304\n",
            "Iteration 66, loss = 0.54753325\n",
            "Iteration 67, loss = 0.54635260\n",
            "Iteration 68, loss = 0.54533829\n",
            "Iteration 69, loss = 0.54402449\n",
            "Iteration 70, loss = 0.54286076\n",
            "Iteration 71, loss = 0.54170755\n",
            "Iteration 72, loss = 0.54063231\n",
            "Iteration 73, loss = 0.53942677\n",
            "Iteration 74, loss = 0.53841951\n",
            "Iteration 75, loss = 0.53724490\n",
            "Iteration 76, loss = 0.53623148\n",
            "Iteration 77, loss = 0.53519204\n",
            "Iteration 78, loss = 0.53412784\n",
            "Iteration 79, loss = 0.53309465\n",
            "Iteration 80, loss = 0.53219866\n",
            "Iteration 81, loss = 0.53113700\n",
            "Iteration 82, loss = 0.53016540\n",
            "Iteration 83, loss = 0.52917468\n",
            "Iteration 84, loss = 0.52817393\n",
            "Iteration 85, loss = 0.52729328\n",
            "Iteration 86, loss = 0.52635450\n",
            "Iteration 87, loss = 0.52547668\n",
            "Iteration 88, loss = 0.52444501\n",
            "Iteration 89, loss = 0.52352532\n",
            "Iteration 90, loss = 0.52264766\n",
            "Iteration 91, loss = 0.52180025\n",
            "Iteration 92, loss = 0.52084655\n",
            "Iteration 93, loss = 0.52006431\n",
            "Iteration 94, loss = 0.51901057\n",
            "Iteration 95, loss = 0.51820564\n",
            "Iteration 96, loss = 0.51731700\n",
            "Iteration 97, loss = 0.51657656\n",
            "Iteration 98, loss = 0.51549666\n",
            "Iteration 99, loss = 0.51468946\n",
            "Iteration 100, loss = 0.51390926\n",
            "Iteration 101, loss = 0.51305187\n",
            "Iteration 102, loss = 0.51213038\n",
            "Iteration 103, loss = 0.51135589\n",
            "Iteration 104, loss = 0.51048203\n",
            "Iteration 105, loss = 0.50960618\n",
            "Iteration 106, loss = 0.50888610\n",
            "Iteration 107, loss = 0.50790497\n",
            "Iteration 108, loss = 0.50703930\n",
            "Iteration 109, loss = 0.50620072\n",
            "Iteration 110, loss = 0.50533166\n",
            "Iteration 111, loss = 0.50441924\n",
            "Iteration 112, loss = 0.50352834\n",
            "Iteration 113, loss = 0.50261266\n",
            "Iteration 114, loss = 0.50174162\n",
            "Iteration 115, loss = 0.50076830\n",
            "Iteration 116, loss = 0.49982505\n",
            "Iteration 117, loss = 0.49883584\n",
            "Iteration 118, loss = 0.49795977\n",
            "Iteration 119, loss = 0.49699117\n",
            "Iteration 120, loss = 0.49623379\n",
            "Iteration 121, loss = 0.49513986\n",
            "Iteration 122, loss = 0.49417746\n",
            "Iteration 123, loss = 0.49323234\n",
            "Iteration 124, loss = 0.49227344\n",
            "Iteration 125, loss = 0.49134551\n",
            "Iteration 126, loss = 0.49048132\n",
            "Iteration 127, loss = 0.48953190\n",
            "Iteration 128, loss = 0.48862298\n",
            "Iteration 129, loss = 0.48770412\n",
            "Iteration 130, loss = 0.48677488\n",
            "Iteration 131, loss = 0.48586501\n",
            "Iteration 132, loss = 0.48503375\n",
            "Iteration 133, loss = 0.48415568\n",
            "Iteration 134, loss = 0.48317268\n",
            "Iteration 135, loss = 0.48222394\n",
            "Iteration 136, loss = 0.48130958\n",
            "Iteration 137, loss = 0.48039976\n",
            "Iteration 138, loss = 0.47946966\n",
            "Iteration 139, loss = 0.47861996\n",
            "Iteration 140, loss = 0.47767555\n",
            "Iteration 141, loss = 0.47666416\n",
            "Iteration 142, loss = 0.47577112\n",
            "Iteration 143, loss = 0.47488333\n",
            "Iteration 144, loss = 0.47396058\n",
            "Iteration 145, loss = 0.47306351\n",
            "Iteration 146, loss = 0.47214409\n",
            "Iteration 147, loss = 0.47121843\n",
            "Iteration 148, loss = 0.47031038\n",
            "Iteration 149, loss = 0.46946093\n",
            "Iteration 150, loss = 0.46850453\n",
            "Iteration 151, loss = 0.46760745\n",
            "Iteration 152, loss = 0.46672546\n",
            "Iteration 153, loss = 0.46582917\n",
            "Iteration 154, loss = 0.46495496\n",
            "Iteration 155, loss = 0.46401448\n",
            "Iteration 156, loss = 0.46308422\n",
            "Iteration 157, loss = 0.46221302\n",
            "Iteration 158, loss = 0.46131581\n",
            "Iteration 159, loss = 0.46041114\n",
            "Iteration 160, loss = 0.45949457\n",
            "Iteration 161, loss = 0.45863462\n",
            "Iteration 162, loss = 0.45776296\n",
            "Iteration 163, loss = 0.45687512\n",
            "Iteration 164, loss = 0.45599484\n",
            "Iteration 165, loss = 0.45500486\n",
            "Iteration 166, loss = 0.45425161\n",
            "Iteration 167, loss = 0.45321741\n",
            "Iteration 168, loss = 0.45235115\n",
            "Iteration 169, loss = 0.45156268\n",
            "Iteration 170, loss = 0.45059548\n",
            "Iteration 171, loss = 0.44967170\n",
            "Iteration 172, loss = 0.44877682\n",
            "Iteration 173, loss = 0.44793988\n",
            "Iteration 174, loss = 0.44700439\n",
            "Iteration 175, loss = 0.44611892\n",
            "Iteration 176, loss = 0.44524225\n",
            "Iteration 177, loss = 0.44426366\n",
            "Iteration 178, loss = 0.44349110\n",
            "Iteration 179, loss = 0.44252389\n",
            "Iteration 180, loss = 0.44178711\n",
            "Iteration 181, loss = 0.44085266\n",
            "Iteration 182, loss = 0.43987297\n",
            "Iteration 183, loss = 0.43894621\n",
            "Iteration 184, loss = 0.43808820\n",
            "Iteration 185, loss = 0.43719726\n",
            "Iteration 186, loss = 0.43629350\n",
            "Iteration 187, loss = 0.43545047\n",
            "Iteration 188, loss = 0.43455320\n",
            "Iteration 189, loss = 0.43377960\n",
            "Iteration 190, loss = 0.43285307\n",
            "Iteration 191, loss = 0.43184231\n",
            "Iteration 192, loss = 0.43098141\n",
            "Iteration 193, loss = 0.43011939\n",
            "Iteration 194, loss = 0.42934811\n",
            "Iteration 195, loss = 0.42852885\n",
            "Iteration 196, loss = 0.42760985\n",
            "Iteration 197, loss = 0.42664775\n",
            "Iteration 198, loss = 0.42571875\n",
            "Iteration 199, loss = 0.42487851\n",
            "Iteration 200, loss = 0.42402649\n",
            "Iteration 201, loss = 0.42313412\n",
            "Iteration 202, loss = 0.42231427\n",
            "Iteration 203, loss = 0.42144461\n",
            "Iteration 204, loss = 0.42059562\n",
            "Iteration 205, loss = 0.41974412\n",
            "Iteration 206, loss = 0.41884177\n",
            "Iteration 207, loss = 0.41802538\n",
            "Iteration 208, loss = 0.41706501\n",
            "Iteration 209, loss = 0.41620597\n",
            "Iteration 210, loss = 0.41533787\n",
            "Iteration 211, loss = 0.41437244\n",
            "Iteration 212, loss = 0.41352236\n",
            "Iteration 213, loss = 0.41268023\n",
            "Iteration 214, loss = 0.41186140\n",
            "Iteration 215, loss = 0.41101357\n",
            "Iteration 216, loss = 0.41004553\n",
            "Iteration 217, loss = 0.40931159\n",
            "Iteration 218, loss = 0.40834170\n",
            "Iteration 219, loss = 0.40742933\n",
            "Iteration 220, loss = 0.40653302\n",
            "Iteration 221, loss = 0.40566750\n",
            "Iteration 222, loss = 0.40488715\n",
            "Iteration 223, loss = 0.40398879\n",
            "Iteration 224, loss = 0.40314706\n",
            "Iteration 225, loss = 0.40227753\n",
            "Iteration 226, loss = 0.40147099\n",
            "Iteration 227, loss = 0.40061248\n",
            "Iteration 228, loss = 0.39976863\n",
            "Iteration 229, loss = 0.39892909\n",
            "Iteration 230, loss = 0.39813580\n",
            "Iteration 231, loss = 0.39730235\n",
            "Iteration 232, loss = 0.39644690\n",
            "Iteration 233, loss = 0.39563856\n",
            "Iteration 234, loss = 0.39481687\n",
            "Iteration 235, loss = 0.39393303\n",
            "Iteration 236, loss = 0.39311982\n",
            "Iteration 237, loss = 0.39223525\n",
            "Iteration 238, loss = 0.39151226\n",
            "Iteration 239, loss = 0.39054352\n",
            "Iteration 240, loss = 0.38979484\n",
            "Iteration 241, loss = 0.38891794\n",
            "Iteration 242, loss = 0.38814201\n",
            "Iteration 243, loss = 0.38723928\n",
            "Iteration 244, loss = 0.38641021\n",
            "Iteration 245, loss = 0.38564119\n",
            "Iteration 246, loss = 0.38481191\n",
            "Iteration 247, loss = 0.38402777\n",
            "Iteration 248, loss = 0.38317858\n",
            "Iteration 249, loss = 0.38234341\n",
            "Iteration 250, loss = 0.38156373\n",
            "Iteration 251, loss = 0.38077141\n",
            "Iteration 252, loss = 0.37999843\n",
            "Iteration 253, loss = 0.37912270\n",
            "Iteration 254, loss = 0.37835492\n",
            "Iteration 255, loss = 0.37749312\n",
            "Iteration 256, loss = 0.37679591\n",
            "Iteration 257, loss = 0.37591699\n",
            "Iteration 258, loss = 0.37516659\n",
            "Iteration 259, loss = 0.37433856\n",
            "Iteration 260, loss = 0.37362761\n",
            "Iteration 261, loss = 0.37270433\n",
            "Iteration 262, loss = 0.37190886\n",
            "Iteration 263, loss = 0.37125926\n",
            "Iteration 264, loss = 0.37036091\n",
            "Iteration 265, loss = 0.36963964\n",
            "Iteration 266, loss = 0.36883207\n",
            "Iteration 267, loss = 0.36797435\n",
            "Iteration 268, loss = 0.36718153\n",
            "Iteration 269, loss = 0.36639522\n",
            "Iteration 270, loss = 0.36558414\n",
            "Iteration 271, loss = 0.36478901\n",
            "Iteration 272, loss = 0.36403930\n",
            "Iteration 273, loss = 0.36321441\n",
            "Iteration 274, loss = 0.36242871\n",
            "Iteration 275, loss = 0.36161481\n",
            "Iteration 276, loss = 0.36086789\n",
            "Iteration 277, loss = 0.36013680\n",
            "Iteration 278, loss = 0.35935962\n",
            "Iteration 279, loss = 0.35864208\n",
            "Iteration 280, loss = 0.35787435\n",
            "Iteration 281, loss = 0.35706814\n",
            "Iteration 282, loss = 0.35625957\n",
            "Iteration 283, loss = 0.35555931\n",
            "Iteration 284, loss = 0.35475303\n",
            "Iteration 285, loss = 0.35400056\n",
            "Iteration 286, loss = 0.35322902\n",
            "Iteration 287, loss = 0.35257642\n",
            "Iteration 288, loss = 0.35174170\n",
            "Iteration 289, loss = 0.35101105\n",
            "Iteration 290, loss = 0.35031709\n",
            "Iteration 291, loss = 0.34952702\n",
            "Iteration 292, loss = 0.34888055\n",
            "Iteration 293, loss = 0.34811528\n",
            "Iteration 294, loss = 0.34735604\n",
            "Iteration 295, loss = 0.34661793\n",
            "Iteration 296, loss = 0.34595030\n",
            "Iteration 297, loss = 0.34525933\n",
            "Iteration 298, loss = 0.34456350\n",
            "Iteration 299, loss = 0.34382436\n",
            "Iteration 300, loss = 0.34318422\n",
            "Iteration 301, loss = 0.34239370\n",
            "Iteration 302, loss = 0.34171318\n",
            "Iteration 303, loss = 0.34090910\n",
            "Iteration 304, loss = 0.34022735\n",
            "Iteration 305, loss = 0.33954803\n",
            "Iteration 306, loss = 0.33883228\n",
            "Iteration 307, loss = 0.33807639\n",
            "Iteration 308, loss = 0.33747366\n",
            "Iteration 309, loss = 0.33670509\n",
            "Iteration 310, loss = 0.33597550\n",
            "Iteration 311, loss = 0.33535099\n",
            "Iteration 312, loss = 0.33465645\n",
            "Iteration 313, loss = 0.33396429\n",
            "Iteration 314, loss = 0.33327912\n",
            "Iteration 315, loss = 0.33247769\n",
            "Iteration 316, loss = 0.33186178\n",
            "Iteration 317, loss = 0.33117904\n",
            "Iteration 318, loss = 0.33045318\n",
            "Iteration 319, loss = 0.32969941\n",
            "Iteration 320, loss = 0.32909984\n",
            "Iteration 321, loss = 0.32838987\n",
            "Iteration 322, loss = 0.32763907\n",
            "Iteration 323, loss = 0.32695655\n",
            "Iteration 324, loss = 0.32630418\n",
            "Iteration 325, loss = 0.32564633\n",
            "Iteration 326, loss = 0.32499483\n",
            "Iteration 327, loss = 0.32431340\n",
            "Iteration 328, loss = 0.32377679\n",
            "Iteration 329, loss = 0.32302791\n",
            "Iteration 330, loss = 0.32239488\n",
            "Iteration 331, loss = 0.32173556\n",
            "Iteration 332, loss = 0.32122548\n",
            "Iteration 333, loss = 0.32045502\n",
            "Iteration 334, loss = 0.31984769\n",
            "Iteration 335, loss = 0.31915344\n",
            "Iteration 336, loss = 0.31847730\n",
            "Iteration 337, loss = 0.31777602\n",
            "Iteration 338, loss = 0.31722670\n",
            "Iteration 339, loss = 0.31661959\n",
            "Iteration 340, loss = 0.31601917\n",
            "Iteration 341, loss = 0.31537920\n",
            "Iteration 342, loss = 0.31461050\n",
            "Iteration 343, loss = 0.31400068\n",
            "Iteration 344, loss = 0.31337085\n",
            "Iteration 345, loss = 0.31273848\n",
            "Iteration 346, loss = 0.31210470\n",
            "Iteration 347, loss = 0.31147841\n",
            "Iteration 348, loss = 0.31089977\n",
            "Iteration 349, loss = 0.31034794\n",
            "Iteration 350, loss = 0.30959125\n",
            "Iteration 351, loss = 0.30898774\n",
            "Iteration 352, loss = 0.30838697\n",
            "Iteration 353, loss = 0.30777174\n",
            "Iteration 354, loss = 0.30712330\n",
            "Iteration 355, loss = 0.30667831\n",
            "Iteration 356, loss = 0.30587358\n",
            "Iteration 357, loss = 0.30552157\n",
            "Iteration 358, loss = 0.30466937\n",
            "Iteration 359, loss = 0.30401530\n",
            "Iteration 360, loss = 0.30340941\n",
            "Iteration 361, loss = 0.30284816\n",
            "Iteration 362, loss = 0.30231299\n",
            "Iteration 363, loss = 0.30168987\n",
            "Iteration 364, loss = 0.30109538\n",
            "Iteration 365, loss = 0.30049361\n",
            "Iteration 366, loss = 0.29989974\n",
            "Iteration 367, loss = 0.29932569\n",
            "Iteration 368, loss = 0.29870403\n",
            "Iteration 369, loss = 0.29804367\n",
            "Iteration 370, loss = 0.29764798\n",
            "Iteration 371, loss = 0.29696126\n",
            "Iteration 372, loss = 0.29636178\n",
            "Iteration 373, loss = 0.29580707\n",
            "Iteration 374, loss = 0.29525292\n",
            "Iteration 375, loss = 0.29473492\n",
            "Iteration 376, loss = 0.29413227\n",
            "Iteration 377, loss = 0.29354339\n",
            "Iteration 378, loss = 0.29301656\n",
            "Iteration 379, loss = 0.29241451\n",
            "Iteration 380, loss = 0.29185960\n",
            "Iteration 381, loss = 0.29133797\n",
            "Iteration 382, loss = 0.29074482\n",
            "Iteration 383, loss = 0.29017235\n",
            "Iteration 384, loss = 0.28956770\n",
            "Iteration 385, loss = 0.28914908\n",
            "Iteration 386, loss = 0.28884145\n",
            "Iteration 387, loss = 0.28791971\n",
            "Iteration 388, loss = 0.28761992\n",
            "Iteration 389, loss = 0.28702197\n",
            "Iteration 390, loss = 0.28633850\n",
            "Iteration 391, loss = 0.28575828\n",
            "Iteration 392, loss = 0.28517758\n",
            "Iteration 393, loss = 0.28467261\n",
            "Iteration 394, loss = 0.28408792\n",
            "Iteration 395, loss = 0.28351896\n",
            "Iteration 396, loss = 0.28297615\n",
            "Iteration 397, loss = 0.28247483\n",
            "Iteration 398, loss = 0.28201015\n",
            "Iteration 399, loss = 0.28143475\n",
            "Iteration 400, loss = 0.28083857\n",
            "Iteration 401, loss = 0.28042797\n",
            "Iteration 402, loss = 0.27975162\n",
            "Iteration 403, loss = 0.27922824\n",
            "Iteration 404, loss = 0.27875700\n",
            "Iteration 405, loss = 0.27822091\n",
            "Iteration 406, loss = 0.27774534\n",
            "Iteration 407, loss = 0.27723006\n",
            "Iteration 408, loss = 0.27664228\n",
            "Iteration 409, loss = 0.27608717\n",
            "Iteration 410, loss = 0.27563417\n",
            "Iteration 411, loss = 0.27505695\n",
            "Iteration 412, loss = 0.27461270\n",
            "Iteration 413, loss = 0.27412675\n",
            "Iteration 414, loss = 0.27355404\n",
            "Iteration 415, loss = 0.27307352\n",
            "Iteration 416, loss = 0.27284447\n",
            "Iteration 417, loss = 0.27206213\n",
            "Iteration 418, loss = 0.27155361\n",
            "Iteration 419, loss = 0.27126420\n",
            "Iteration 420, loss = 0.27059017\n",
            "Iteration 421, loss = 0.27009393\n",
            "Iteration 422, loss = 0.26957427\n",
            "Iteration 423, loss = 0.26919753\n",
            "Iteration 424, loss = 0.26856250\n",
            "Iteration 425, loss = 0.26814252\n",
            "Iteration 426, loss = 0.26757735\n",
            "Iteration 427, loss = 0.26711126\n",
            "Iteration 428, loss = 0.26664098\n",
            "Iteration 429, loss = 0.26620242\n",
            "Iteration 430, loss = 0.26580689\n",
            "Iteration 431, loss = 0.26533830\n",
            "Iteration 432, loss = 0.26485417\n",
            "Iteration 433, loss = 0.26433140\n",
            "Iteration 434, loss = 0.26393195\n",
            "Iteration 435, loss = 0.26335161\n",
            "Iteration 436, loss = 0.26286418\n",
            "Iteration 437, loss = 0.26239648\n",
            "Iteration 438, loss = 0.26195131\n",
            "Iteration 439, loss = 0.26160825\n",
            "Iteration 440, loss = 0.26105093\n",
            "Iteration 441, loss = 0.26054765\n",
            "Iteration 442, loss = 0.26001522\n",
            "Iteration 443, loss = 0.25956454\n",
            "Iteration 444, loss = 0.25907823\n",
            "Iteration 445, loss = 0.25868486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 446, loss = 0.25815869\n",
            "Iteration 447, loss = 0.25782542\n",
            "Iteration 448, loss = 0.25730677\n",
            "Iteration 449, loss = 0.25681139\n",
            "Iteration 450, loss = 0.25640101\n",
            "Iteration 451, loss = 0.25610427\n",
            "Iteration 452, loss = 0.25548612\n",
            "Iteration 453, loss = 0.25503578\n",
            "Iteration 454, loss = 0.25459066\n",
            "Iteration 455, loss = 0.25416411\n",
            "Iteration 456, loss = 0.25373715\n",
            "Iteration 457, loss = 0.25336008\n",
            "Iteration 458, loss = 0.25279344\n",
            "Iteration 459, loss = 0.25232814\n",
            "Iteration 460, loss = 0.25191130\n",
            "Iteration 461, loss = 0.25148090\n",
            "Iteration 462, loss = 0.25097238\n",
            "Iteration 463, loss = 0.25060942\n",
            "Iteration 464, loss = 0.25016519\n",
            "Iteration 465, loss = 0.24986474\n",
            "Iteration 466, loss = 0.24933656\n",
            "Iteration 467, loss = 0.24889575\n",
            "Iteration 468, loss = 0.24846422\n",
            "Iteration 469, loss = 0.24796802\n",
            "Iteration 470, loss = 0.24761033\n",
            "Iteration 471, loss = 0.24714664\n",
            "Iteration 472, loss = 0.24675087\n",
            "Iteration 473, loss = 0.24625833\n",
            "Iteration 474, loss = 0.24588124\n",
            "Iteration 475, loss = 0.24549899\n",
            "Iteration 476, loss = 0.24514058\n",
            "Iteration 477, loss = 0.24465316\n",
            "Iteration 478, loss = 0.24431266\n",
            "Iteration 479, loss = 0.24377604\n",
            "Iteration 480, loss = 0.24343126\n",
            "Iteration 481, loss = 0.24299193\n",
            "Iteration 482, loss = 0.24256914\n",
            "Iteration 483, loss = 0.24205763\n",
            "Iteration 484, loss = 0.24172831\n",
            "Iteration 485, loss = 0.24124914\n",
            "Iteration 486, loss = 0.24088226\n",
            "Iteration 487, loss = 0.24047324\n",
            "Iteration 488, loss = 0.24003958\n",
            "Iteration 489, loss = 0.23971793\n",
            "Iteration 490, loss = 0.23938018\n",
            "Iteration 491, loss = 0.23884137\n",
            "Iteration 492, loss = 0.23831445\n",
            "Iteration 493, loss = 0.23829193\n",
            "Iteration 494, loss = 0.23766958\n",
            "Iteration 495, loss = 0.23720932\n",
            "Iteration 496, loss = 0.23670659\n",
            "Iteration 497, loss = 0.23632576\n",
            "Iteration 498, loss = 0.23592219\n",
            "Iteration 499, loss = 0.23557100\n",
            "Iteration 500, loss = 0.23531086\n",
            "Iteration 501, loss = 0.23475593\n",
            "Iteration 502, loss = 0.23429324\n",
            "Iteration 503, loss = 0.23390225\n",
            "Iteration 504, loss = 0.23354381\n",
            "Iteration 505, loss = 0.23319477\n",
            "Iteration 506, loss = 0.23274322\n",
            "Iteration 507, loss = 0.23228443\n",
            "Iteration 508, loss = 0.23188563\n",
            "Iteration 509, loss = 0.23143574\n",
            "Iteration 510, loss = 0.23100733\n",
            "Iteration 511, loss = 0.23060522\n",
            "Iteration 512, loss = 0.23020104\n",
            "Iteration 513, loss = 0.22987890\n",
            "Iteration 514, loss = 0.22944682\n",
            "Iteration 515, loss = 0.22911907\n",
            "Iteration 516, loss = 0.22861369\n",
            "Iteration 517, loss = 0.22823913\n",
            "Iteration 518, loss = 0.22787596\n",
            "Iteration 519, loss = 0.22744566\n",
            "Iteration 520, loss = 0.22714360\n",
            "Iteration 521, loss = 0.22679156\n",
            "Iteration 522, loss = 0.22628481\n",
            "Iteration 523, loss = 0.22593456\n",
            "Iteration 524, loss = 0.22549998\n",
            "Iteration 525, loss = 0.22516504\n",
            "Iteration 526, loss = 0.22475115\n",
            "Iteration 527, loss = 0.22440572\n",
            "Iteration 528, loss = 0.22418310\n",
            "Iteration 529, loss = 0.22378048\n",
            "Iteration 530, loss = 0.22325577\n",
            "Iteration 531, loss = 0.22278530\n",
            "Iteration 532, loss = 0.22243302\n",
            "Iteration 533, loss = 0.22208775\n",
            "Iteration 534, loss = 0.22168582\n",
            "Iteration 535, loss = 0.22130742\n",
            "Iteration 536, loss = 0.22098350\n",
            "Iteration 537, loss = 0.22061426\n",
            "Iteration 538, loss = 0.22018808\n",
            "Iteration 539, loss = 0.21981875\n",
            "Iteration 540, loss = 0.21946334\n",
            "Iteration 541, loss = 0.21906214\n",
            "Iteration 542, loss = 0.21869391\n",
            "Iteration 543, loss = 0.21837075\n",
            "Iteration 544, loss = 0.21790822\n",
            "Iteration 545, loss = 0.21751281\n",
            "Iteration 546, loss = 0.21718567\n",
            "Iteration 547, loss = 0.21684542\n",
            "Iteration 548, loss = 0.21644839\n",
            "Iteration 549, loss = 0.21607214\n",
            "Iteration 550, loss = 0.21569400\n",
            "Iteration 551, loss = 0.21531912\n",
            "Iteration 552, loss = 0.21498176\n",
            "Iteration 553, loss = 0.21465113\n",
            "Iteration 554, loss = 0.21435571\n",
            "Iteration 555, loss = 0.21413168\n",
            "Iteration 556, loss = 0.21357562\n",
            "Iteration 557, loss = 0.21319649\n",
            "Iteration 558, loss = 0.21298161\n",
            "Iteration 559, loss = 0.21251021\n",
            "Iteration 560, loss = 0.21220695\n",
            "Iteration 561, loss = 0.21179008\n",
            "Iteration 562, loss = 0.21145107\n",
            "Iteration 563, loss = 0.21105673\n",
            "Iteration 564, loss = 0.21068522\n",
            "Iteration 565, loss = 0.21030702\n",
            "Iteration 566, loss = 0.20999207\n",
            "Iteration 567, loss = 0.20963520\n",
            "Iteration 568, loss = 0.20924406\n",
            "Iteration 569, loss = 0.20888013\n",
            "Iteration 570, loss = 0.20851288\n",
            "Iteration 571, loss = 0.20822192\n",
            "Iteration 572, loss = 0.20777493\n",
            "Iteration 573, loss = 0.20745170\n",
            "Iteration 574, loss = 0.20714856\n",
            "Iteration 575, loss = 0.20689496\n",
            "Iteration 576, loss = 0.20635935\n",
            "Iteration 577, loss = 0.20603499\n",
            "Iteration 578, loss = 0.20560482\n",
            "Iteration 579, loss = 0.20529497\n",
            "Iteration 580, loss = 0.20504674\n",
            "Iteration 581, loss = 0.20464089\n",
            "Iteration 582, loss = 0.20431295\n",
            "Iteration 583, loss = 0.20392539\n",
            "Iteration 584, loss = 0.20347811\n",
            "Iteration 585, loss = 0.20317157\n",
            "Iteration 586, loss = 0.20291029\n",
            "Iteration 587, loss = 0.20259095\n",
            "Iteration 588, loss = 0.20219459\n",
            "Iteration 589, loss = 0.20184421\n",
            "Iteration 590, loss = 0.20138468\n",
            "Iteration 591, loss = 0.20119541\n",
            "Iteration 592, loss = 0.20076249\n",
            "Iteration 593, loss = 0.20041604\n",
            "Iteration 594, loss = 0.20000754\n",
            "Iteration 595, loss = 0.19969453\n",
            "Iteration 596, loss = 0.19942118\n",
            "Iteration 597, loss = 0.19916551\n",
            "Iteration 598, loss = 0.19862015\n",
            "Iteration 599, loss = 0.19834766\n",
            "Iteration 600, loss = 0.19801544\n",
            "Iteration 601, loss = 0.19761849\n",
            "Iteration 602, loss = 0.19726823\n",
            "Iteration 603, loss = 0.19693768\n",
            "Iteration 604, loss = 0.19666281\n",
            "Iteration 605, loss = 0.19632468\n",
            "Iteration 606, loss = 0.19588324\n",
            "Iteration 607, loss = 0.19569695\n",
            "Iteration 608, loss = 0.19521425\n",
            "Iteration 609, loss = 0.19492629\n",
            "Iteration 610, loss = 0.19459222\n",
            "Iteration 611, loss = 0.19429417\n",
            "Iteration 612, loss = 0.19394354\n",
            "Iteration 613, loss = 0.19368433\n",
            "Iteration 614, loss = 0.19325944\n",
            "Iteration 615, loss = 0.19290924\n",
            "Iteration 616, loss = 0.19259570\n",
            "Iteration 617, loss = 0.19228605\n",
            "Iteration 618, loss = 0.19189108\n",
            "Iteration 619, loss = 0.19158953\n",
            "Iteration 620, loss = 0.19147000\n",
            "Iteration 621, loss = 0.19093052\n",
            "Iteration 622, loss = 0.19072597\n",
            "Iteration 623, loss = 0.19038816\n",
            "Iteration 624, loss = 0.19008260\n",
            "Iteration 625, loss = 0.18967409\n",
            "Iteration 626, loss = 0.18935122\n",
            "Iteration 627, loss = 0.18903594\n",
            "Iteration 628, loss = 0.18874974\n",
            "Iteration 629, loss = 0.18841855\n",
            "Iteration 630, loss = 0.18808339\n",
            "Iteration 631, loss = 0.18780236\n",
            "Iteration 632, loss = 0.18748884\n",
            "Iteration 633, loss = 0.18717567\n",
            "Iteration 634, loss = 0.18707928\n",
            "Iteration 635, loss = 0.18653020\n",
            "Iteration 636, loss = 0.18637061\n",
            "Iteration 637, loss = 0.18597812\n",
            "Iteration 638, loss = 0.18559720\n",
            "Iteration 639, loss = 0.18528008\n",
            "Iteration 640, loss = 0.18497714\n",
            "Iteration 641, loss = 0.18464839\n",
            "Iteration 642, loss = 0.18433160\n",
            "Iteration 643, loss = 0.18407290\n",
            "Iteration 644, loss = 0.18377556\n",
            "Iteration 645, loss = 0.18347799\n",
            "Iteration 646, loss = 0.18318464\n",
            "Iteration 647, loss = 0.18289123\n",
            "Iteration 648, loss = 0.18255013\n",
            "Iteration 649, loss = 0.18233935\n",
            "Iteration 650, loss = 0.18196168\n",
            "Iteration 651, loss = 0.18164326\n",
            "Iteration 652, loss = 0.18137570\n",
            "Iteration 653, loss = 0.18112393\n",
            "Iteration 654, loss = 0.18082122\n",
            "Iteration 655, loss = 0.18051991\n",
            "Iteration 656, loss = 0.18022691\n",
            "Iteration 657, loss = 0.18002901\n",
            "Iteration 658, loss = 0.17965568\n",
            "Iteration 659, loss = 0.17927938\n",
            "Iteration 660, loss = 0.17897491\n",
            "Iteration 661, loss = 0.17876360\n",
            "Iteration 662, loss = 0.17844114\n",
            "Iteration 663, loss = 0.17814563\n",
            "Iteration 664, loss = 0.17785642\n",
            "Iteration 665, loss = 0.17754768\n",
            "Iteration 666, loss = 0.17724583\n",
            "Iteration 667, loss = 0.17692772\n",
            "Iteration 668, loss = 0.17659844\n",
            "Iteration 669, loss = 0.17639752\n",
            "Iteration 670, loss = 0.17604481\n",
            "Iteration 671, loss = 0.17591433\n",
            "Iteration 672, loss = 0.17547661\n",
            "Iteration 673, loss = 0.17518342\n",
            "Iteration 674, loss = 0.17489176\n",
            "Iteration 675, loss = 0.17462385\n",
            "Iteration 676, loss = 0.17426261\n",
            "Iteration 677, loss = 0.17397357\n",
            "Iteration 678, loss = 0.17367888\n",
            "Iteration 679, loss = 0.17345470\n",
            "Iteration 680, loss = 0.17336914\n",
            "Iteration 681, loss = 0.17300499\n",
            "Iteration 682, loss = 0.17295565\n",
            "Iteration 683, loss = 0.17233752\n",
            "Iteration 684, loss = 0.17206560\n",
            "Iteration 685, loss = 0.17181735\n",
            "Iteration 686, loss = 0.17151877\n",
            "Iteration 687, loss = 0.17120123\n",
            "Iteration 688, loss = 0.17092033\n",
            "Iteration 689, loss = 0.17089034\n",
            "Iteration 690, loss = 0.17047958\n",
            "Iteration 691, loss = 0.17009258\n",
            "Iteration 692, loss = 0.16987994\n",
            "Iteration 693, loss = 0.16959276\n",
            "Iteration 694, loss = 0.16923555\n",
            "Iteration 695, loss = 0.16902758\n",
            "Iteration 696, loss = 0.16870429\n",
            "Iteration 697, loss = 0.16844428\n",
            "Iteration 698, loss = 0.16830859\n",
            "Iteration 699, loss = 0.16791793\n",
            "Iteration 700, loss = 0.16768635\n",
            "Iteration 701, loss = 0.16737893\n",
            "Iteration 702, loss = 0.16708797\n",
            "Iteration 703, loss = 0.16678097\n",
            "Iteration 704, loss = 0.16656610\n",
            "Iteration 705, loss = 0.16638627\n",
            "Iteration 706, loss = 0.16607757\n",
            "Iteration 707, loss = 0.16574558\n",
            "Iteration 708, loss = 0.16566890\n",
            "Iteration 709, loss = 0.16520581\n",
            "Iteration 710, loss = 0.16504868\n",
            "Iteration 711, loss = 0.16470690\n",
            "Iteration 712, loss = 0.16448330\n",
            "Iteration 713, loss = 0.16424630\n",
            "Iteration 714, loss = 0.16392652\n",
            "Iteration 715, loss = 0.16366026\n",
            "Iteration 716, loss = 0.16342330\n",
            "Iteration 717, loss = 0.16316467\n",
            "Iteration 718, loss = 0.16286578\n",
            "Iteration 719, loss = 0.16263607\n",
            "Iteration 720, loss = 0.16233654\n",
            "Iteration 721, loss = 0.16214379\n",
            "Iteration 722, loss = 0.16184233\n",
            "Iteration 723, loss = 0.16161593\n",
            "Iteration 724, loss = 0.16129977\n",
            "Iteration 725, loss = 0.16108586\n",
            "Iteration 726, loss = 0.16079467\n",
            "Iteration 727, loss = 0.16049570\n",
            "Iteration 728, loss = 0.16026920\n",
            "Iteration 729, loss = 0.15995931\n",
            "Iteration 730, loss = 0.15980005\n",
            "Iteration 731, loss = 0.15956045\n",
            "Iteration 732, loss = 0.15931607\n",
            "Iteration 733, loss = 0.15908963\n",
            "Iteration 734, loss = 0.15874155\n",
            "Iteration 735, loss = 0.15848130\n",
            "Iteration 736, loss = 0.15820151\n",
            "Iteration 737, loss = 0.15802009\n",
            "Iteration 738, loss = 0.15776746\n",
            "Iteration 739, loss = 0.15749503\n",
            "Iteration 740, loss = 0.15728557\n",
            "Iteration 741, loss = 0.15698585\n",
            "Iteration 742, loss = 0.15681399\n",
            "Iteration 743, loss = 0.15647555\n",
            "Iteration 744, loss = 0.15619312\n",
            "Iteration 745, loss = 0.15606179\n",
            "Iteration 746, loss = 0.15565756\n",
            "Iteration 747, loss = 0.15546957\n",
            "Iteration 748, loss = 0.15532325\n",
            "Iteration 749, loss = 0.15515988\n",
            "Iteration 750, loss = 0.15485878\n",
            "Iteration 751, loss = 0.15457230\n",
            "Iteration 752, loss = 0.15433511\n",
            "Iteration 753, loss = 0.15406594\n",
            "Iteration 754, loss = 0.15377498\n",
            "Iteration 755, loss = 0.15352107\n",
            "Iteration 756, loss = 0.15335799\n",
            "Iteration 757, loss = 0.15310569\n",
            "Iteration 758, loss = 0.15285135\n",
            "Iteration 759, loss = 0.15259064\n",
            "Iteration 760, loss = 0.15234688\n",
            "Iteration 761, loss = 0.15222895\n",
            "Iteration 762, loss = 0.15183869\n",
            "Iteration 763, loss = 0.15155939\n",
            "Iteration 764, loss = 0.15140163\n",
            "Iteration 765, loss = 0.15106330\n",
            "Iteration 766, loss = 0.15082082\n",
            "Iteration 767, loss = 0.15059491\n",
            "Iteration 768, loss = 0.15030149\n",
            "Iteration 769, loss = 0.15017569\n",
            "Iteration 770, loss = 0.14989295\n",
            "Iteration 771, loss = 0.14965897\n",
            "Iteration 772, loss = 0.14944099\n",
            "Iteration 773, loss = 0.14938485\n",
            "Iteration 774, loss = 0.14898813\n",
            "Iteration 775, loss = 0.14868721\n",
            "Iteration 776, loss = 0.14854000\n",
            "Iteration 777, loss = 0.14843836\n",
            "Iteration 778, loss = 0.14818581\n",
            "Iteration 779, loss = 0.14774981\n",
            "Iteration 780, loss = 0.14753005\n",
            "Iteration 781, loss = 0.14728780\n",
            "Iteration 782, loss = 0.14700221\n",
            "Iteration 783, loss = 0.14680311\n",
            "Iteration 784, loss = 0.14652893\n",
            "Iteration 785, loss = 0.14633643\n",
            "Iteration 786, loss = 0.14603397\n",
            "Iteration 787, loss = 0.14585659\n",
            "Iteration 788, loss = 0.14554363\n",
            "Iteration 789, loss = 0.14536905\n",
            "Iteration 790, loss = 0.14513144\n",
            "Iteration 791, loss = 0.14489425\n",
            "Iteration 792, loss = 0.14458249\n",
            "Iteration 793, loss = 0.14433530\n",
            "Iteration 794, loss = 0.14403697\n",
            "Iteration 795, loss = 0.14379957\n",
            "Iteration 796, loss = 0.14364584\n",
            "Iteration 797, loss = 0.14325999\n",
            "Iteration 798, loss = 0.14303344\n",
            "Iteration 799, loss = 0.14280364\n",
            "Iteration 800, loss = 0.14275228\n",
            "Iteration 801, loss = 0.14235417\n",
            "Iteration 802, loss = 0.14207443\n",
            "Iteration 803, loss = 0.14187473\n",
            "Iteration 804, loss = 0.14159693\n",
            "Iteration 805, loss = 0.14143032\n",
            "Iteration 806, loss = 0.14108406\n",
            "Iteration 807, loss = 0.14087270\n",
            "Iteration 808, loss = 0.14062265\n",
            "Iteration 809, loss = 0.14036513\n",
            "Iteration 810, loss = 0.14017315\n",
            "Iteration 811, loss = 0.13991591\n",
            "Iteration 812, loss = 0.13961477\n",
            "Iteration 813, loss = 0.13939705\n",
            "Iteration 814, loss = 0.13920421\n",
            "Iteration 815, loss = 0.13907623\n",
            "Iteration 816, loss = 0.13880700\n",
            "Iteration 817, loss = 0.13853895\n",
            "Iteration 818, loss = 0.13827668\n",
            "Iteration 819, loss = 0.13807814\n",
            "Iteration 820, loss = 0.13778457\n",
            "Iteration 821, loss = 0.13756905\n",
            "Iteration 822, loss = 0.13740232\n",
            "Iteration 823, loss = 0.13710669\n",
            "Iteration 824, loss = 0.13686934\n",
            "Iteration 825, loss = 0.13666020\n",
            "Iteration 826, loss = 0.13644567\n",
            "Iteration 827, loss = 0.13622226\n",
            "Iteration 828, loss = 0.13604867\n",
            "Iteration 829, loss = 0.13574079\n",
            "Iteration 830, loss = 0.13549919\n",
            "Iteration 831, loss = 0.13532579\n",
            "Iteration 832, loss = 0.13511903\n",
            "Iteration 833, loss = 0.13493906\n",
            "Iteration 834, loss = 0.13495254\n",
            "Iteration 835, loss = 0.13440743\n",
            "Iteration 836, loss = 0.13419391\n",
            "Iteration 837, loss = 0.13401964\n",
            "Iteration 838, loss = 0.13389519\n",
            "Iteration 839, loss = 0.13370316\n",
            "Iteration 840, loss = 0.13344845\n",
            "Iteration 841, loss = 0.13314166\n",
            "Iteration 842, loss = 0.13295172\n",
            "Iteration 843, loss = 0.13282949\n",
            "Iteration 844, loss = 0.13259812\n",
            "Iteration 845, loss = 0.13229545\n",
            "Iteration 846, loss = 0.13212537\n",
            "Iteration 847, loss = 0.13195493\n",
            "Iteration 848, loss = 0.13176791\n",
            "Iteration 849, loss = 0.13158136\n",
            "Iteration 850, loss = 0.13129905\n",
            "Iteration 851, loss = 0.13103427\n",
            "Iteration 852, loss = 0.13086111\n",
            "Iteration 853, loss = 0.13066724\n",
            "Iteration 854, loss = 0.13046897\n",
            "Iteration 855, loss = 0.13020912\n",
            "Iteration 856, loss = 0.12997547\n",
            "Iteration 857, loss = 0.12984920\n",
            "Iteration 858, loss = 0.12960741\n",
            "Iteration 859, loss = 0.12956360\n",
            "Iteration 860, loss = 0.12917705\n",
            "Iteration 861, loss = 0.12899863\n",
            "Iteration 862, loss = 0.12884134\n",
            "Iteration 863, loss = 0.12863855\n",
            "Iteration 864, loss = 0.12836792\n",
            "Iteration 865, loss = 0.12812354\n",
            "Iteration 866, loss = 0.12815632\n",
            "Iteration 867, loss = 0.12782562\n",
            "Iteration 868, loss = 0.12757784\n",
            "Iteration 869, loss = 0.12745543\n",
            "Iteration 870, loss = 0.12720891\n",
            "Iteration 871, loss = 0.12695905\n",
            "Iteration 872, loss = 0.12677054\n",
            "Iteration 873, loss = 0.12661990\n",
            "Iteration 874, loss = 0.12642906\n",
            "Iteration 875, loss = 0.12625754\n",
            "Iteration 876, loss = 0.12600937\n",
            "Iteration 877, loss = 0.12583041\n",
            "Iteration 878, loss = 0.12560935\n",
            "Iteration 879, loss = 0.12542011\n",
            "Iteration 880, loss = 0.12526349\n",
            "Iteration 881, loss = 0.12499393\n",
            "Iteration 882, loss = 0.12479211\n",
            "Iteration 883, loss = 0.12458686\n",
            "Iteration 884, loss = 0.12443083\n",
            "Iteration 885, loss = 0.12428464\n",
            "Iteration 886, loss = 0.12405927\n",
            "Iteration 887, loss = 0.12380300\n",
            "Iteration 888, loss = 0.12366437\n",
            "Iteration 889, loss = 0.12341979\n",
            "Iteration 890, loss = 0.12321894\n",
            "Iteration 891, loss = 0.12301869\n",
            "Iteration 892, loss = 0.12277259\n",
            "Iteration 893, loss = 0.12263466\n",
            "Iteration 894, loss = 0.12240569\n",
            "Iteration 895, loss = 0.12220508\n",
            "Iteration 896, loss = 0.12202659\n",
            "Iteration 897, loss = 0.12178560\n",
            "Iteration 898, loss = 0.12159694\n",
            "Iteration 899, loss = 0.12136086\n",
            "Iteration 900, loss = 0.12119049\n",
            "Iteration 901, loss = 0.12095441\n",
            "Iteration 902, loss = 0.12075664\n",
            "Iteration 903, loss = 0.12055904\n",
            "Iteration 904, loss = 0.12038630\n",
            "Iteration 905, loss = 0.12016995\n",
            "Iteration 906, loss = 0.12003245\n",
            "Iteration 907, loss = 0.11983550\n",
            "Iteration 908, loss = 0.11963023\n",
            "Iteration 909, loss = 0.11944479\n",
            "Iteration 910, loss = 0.11922895\n",
            "Iteration 911, loss = 0.11910837\n",
            "Iteration 912, loss = 0.11890785\n",
            "Iteration 913, loss = 0.11872481\n",
            "Iteration 914, loss = 0.11857639\n",
            "Iteration 915, loss = 0.11835585\n",
            "Iteration 916, loss = 0.11821580\n",
            "Iteration 917, loss = 0.11807033\n",
            "Iteration 918, loss = 0.11783209\n",
            "Iteration 919, loss = 0.11774745\n",
            "Iteration 920, loss = 0.11762098\n",
            "Iteration 921, loss = 0.11736277\n",
            "Iteration 922, loss = 0.11719431\n",
            "Iteration 923, loss = 0.11696978\n",
            "Iteration 924, loss = 0.11678740\n",
            "Iteration 925, loss = 0.11664354\n",
            "Iteration 926, loss = 0.11647742\n",
            "Iteration 927, loss = 0.11639761\n",
            "Iteration 928, loss = 0.11622172\n",
            "Iteration 929, loss = 0.11593713\n",
            "Iteration 930, loss = 0.11579033\n",
            "Iteration 931, loss = 0.11572606\n",
            "Iteration 932, loss = 0.11536785\n",
            "Iteration 933, loss = 0.11524261\n",
            "Iteration 934, loss = 0.11511998\n",
            "Iteration 935, loss = 0.11494161\n",
            "Iteration 936, loss = 0.11480382\n",
            "Iteration 937, loss = 0.11458769\n",
            "Iteration 938, loss = 0.11450691\n",
            "Iteration 939, loss = 0.11421419\n",
            "Iteration 940, loss = 0.11405386\n",
            "Iteration 941, loss = 0.11394161\n",
            "Iteration 942, loss = 0.11382112\n",
            "Iteration 943, loss = 0.11371502\n",
            "Iteration 944, loss = 0.11354888\n",
            "Iteration 945, loss = 0.11326515\n",
            "Iteration 946, loss = 0.11311008\n",
            "Iteration 947, loss = 0.11317793\n",
            "Iteration 948, loss = 0.11287024\n",
            "Iteration 949, loss = 0.11272574\n",
            "Iteration 950, loss = 0.11247297\n",
            "Iteration 951, loss = 0.11229620\n",
            "Iteration 952, loss = 0.11217166\n",
            "Iteration 953, loss = 0.11198448\n",
            "Iteration 954, loss = 0.11181339\n",
            "Iteration 955, loss = 0.11165499\n",
            "Iteration 956, loss = 0.11166385\n",
            "Iteration 957, loss = 0.11154104\n",
            "Iteration 958, loss = 0.11118685\n",
            "Iteration 959, loss = 0.11102172\n",
            "Iteration 960, loss = 0.11088850\n",
            "Iteration 961, loss = 0.11079105\n",
            "Iteration 962, loss = 0.11059958\n",
            "Iteration 963, loss = 0.11050943\n",
            "Iteration 964, loss = 0.11023575\n",
            "Iteration 965, loss = 0.11007133\n",
            "Iteration 966, loss = 0.10990198\n",
            "Iteration 967, loss = 0.10973021\n",
            "Iteration 968, loss = 0.10961456\n",
            "Iteration 969, loss = 0.10945060\n",
            "Iteration 970, loss = 0.10925499\n",
            "Iteration 971, loss = 0.10916269\n",
            "Iteration 972, loss = 0.10901429\n",
            "Iteration 973, loss = 0.10884018\n",
            "Iteration 974, loss = 0.10867071\n",
            "Iteration 975, loss = 0.10851958\n",
            "Iteration 976, loss = 0.10833979\n",
            "Iteration 977, loss = 0.10809919\n",
            "Iteration 978, loss = 0.10796393\n",
            "Iteration 979, loss = 0.10785284\n",
            "Iteration 980, loss = 0.10768831\n",
            "Iteration 981, loss = 0.10749441\n",
            "Iteration 982, loss = 0.10732045\n",
            "Iteration 983, loss = 0.10718328\n",
            "Iteration 984, loss = 0.10704615\n",
            "Iteration 985, loss = 0.10685626\n",
            "Iteration 986, loss = 0.10672369\n",
            "Iteration 987, loss = 0.10653415\n",
            "Iteration 988, loss = 0.10637542\n",
            "Iteration 989, loss = 0.10628887\n",
            "Iteration 990, loss = 0.10611550\n",
            "Iteration 991, loss = 0.10596288\n",
            "Iteration 992, loss = 0.10577086\n",
            "Iteration 993, loss = 0.10561498\n",
            "Iteration 994, loss = 0.10548707\n",
            "Iteration 995, loss = 0.10531368\n",
            "Iteration 996, loss = 0.10518322\n",
            "Iteration 997, loss = 0.10500301\n",
            "Iteration 998, loss = 0.10488919\n",
            "Iteration 999, loss = 0.10477686\n",
            "Iteration 1000, loss = 0.10457769\n",
            "Acurácia: 0.7654320987654321\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.89      0.68      0.77        47\n",
            "   recurrence-events       0.67      0.88      0.76        34\n",
            "\n",
            "            accuracy                           0.77        81\n",
            "           macro avg       0.78      0.78      0.77        81\n",
            "        weighted avg       0.80      0.77      0.77        81\n",
            "\n",
            "[[32 15]\n",
            " [ 4 30]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Avaliando topologia: (30,)\n",
            "Iteration 1, loss = 0.71445153\n",
            "Iteration 2, loss = 0.70339797\n",
            "Iteration 3, loss = 0.69402646\n",
            "Iteration 4, loss = 0.68542020\n",
            "Iteration 5, loss = 0.67740751\n",
            "Iteration 6, loss = 0.67049346\n",
            "Iteration 7, loss = 0.66457760\n",
            "Iteration 8, loss = 0.65789115\n",
            "Iteration 9, loss = 0.65283408\n",
            "Iteration 10, loss = 0.64758829\n",
            "Iteration 11, loss = 0.64272653\n",
            "Iteration 12, loss = 0.63813235\n",
            "Iteration 13, loss = 0.63326411\n",
            "Iteration 14, loss = 0.62896995\n",
            "Iteration 15, loss = 0.62475658\n",
            "Iteration 16, loss = 0.62056654\n",
            "Iteration 17, loss = 0.61652890\n",
            "Iteration 18, loss = 0.61234608\n",
            "Iteration 19, loss = 0.60855084\n",
            "Iteration 20, loss = 0.60485416\n",
            "Iteration 21, loss = 0.60111163\n",
            "Iteration 22, loss = 0.59768505\n",
            "Iteration 23, loss = 0.59441969\n",
            "Iteration 24, loss = 0.59077880\n",
            "Iteration 25, loss = 0.58752553\n",
            "Iteration 26, loss = 0.58450979\n",
            "Iteration 27, loss = 0.58151792\n",
            "Iteration 28, loss = 0.57848405\n",
            "Iteration 29, loss = 0.57569515\n",
            "Iteration 30, loss = 0.57299007\n",
            "Iteration 31, loss = 0.57022173\n",
            "Iteration 32, loss = 0.56742984\n",
            "Iteration 33, loss = 0.56487137\n",
            "Iteration 34, loss = 0.56234290\n",
            "Iteration 35, loss = 0.55984580\n",
            "Iteration 36, loss = 0.55750311\n",
            "Iteration 37, loss = 0.55493004\n",
            "Iteration 38, loss = 0.55261927\n",
            "Iteration 39, loss = 0.55028205\n",
            "Iteration 40, loss = 0.54814925\n",
            "Iteration 41, loss = 0.54596315\n",
            "Iteration 42, loss = 0.54382941\n",
            "Iteration 43, loss = 0.54156508\n",
            "Iteration 44, loss = 0.53954483\n",
            "Iteration 45, loss = 0.53754896\n",
            "Iteration 46, loss = 0.53550432\n",
            "Iteration 47, loss = 0.53349076\n",
            "Iteration 48, loss = 0.53159307\n",
            "Iteration 49, loss = 0.52974699\n",
            "Iteration 50, loss = 0.52790068\n",
            "Iteration 51, loss = 0.52598669\n",
            "Iteration 52, loss = 0.52414160\n",
            "Iteration 53, loss = 0.52243412\n",
            "Iteration 54, loss = 0.52044358\n",
            "Iteration 55, loss = 0.51878478\n",
            "Iteration 56, loss = 0.51696876\n",
            "Iteration 57, loss = 0.51518935\n",
            "Iteration 58, loss = 0.51360259\n",
            "Iteration 59, loss = 0.51187373\n",
            "Iteration 60, loss = 0.51017251\n",
            "Iteration 61, loss = 0.50855025\n",
            "Iteration 62, loss = 0.50683278\n",
            "Iteration 63, loss = 0.50525438\n",
            "Iteration 64, loss = 0.50362856\n",
            "Iteration 65, loss = 0.50198439\n",
            "Iteration 66, loss = 0.50040049\n",
            "Iteration 67, loss = 0.49903928\n",
            "Iteration 68, loss = 0.49735317\n",
            "Iteration 69, loss = 0.49565680\n",
            "Iteration 70, loss = 0.49415079\n",
            "Iteration 71, loss = 0.49254971\n",
            "Iteration 72, loss = 0.49101982\n",
            "Iteration 73, loss = 0.48941970\n",
            "Iteration 74, loss = 0.48790662\n",
            "Iteration 75, loss = 0.48643882\n",
            "Iteration 76, loss = 0.48489048\n",
            "Iteration 77, loss = 0.48340882\n",
            "Iteration 78, loss = 0.48180811\n",
            "Iteration 79, loss = 0.48034875\n",
            "Iteration 80, loss = 0.47878079\n",
            "Iteration 81, loss = 0.47729640\n",
            "Iteration 82, loss = 0.47589052\n",
            "Iteration 83, loss = 0.47451645\n",
            "Iteration 84, loss = 0.47309166\n",
            "Iteration 85, loss = 0.47167494\n",
            "Iteration 86, loss = 0.47030272\n",
            "Iteration 87, loss = 0.46875923\n",
            "Iteration 88, loss = 0.46731020\n",
            "Iteration 89, loss = 0.46587060\n",
            "Iteration 90, loss = 0.46451839\n",
            "Iteration 91, loss = 0.46310228\n",
            "Iteration 92, loss = 0.46175147\n",
            "Iteration 93, loss = 0.46032478\n",
            "Iteration 94, loss = 0.45900843\n",
            "Iteration 95, loss = 0.45762146\n",
            "Iteration 96, loss = 0.45627292\n",
            "Iteration 97, loss = 0.45483937\n",
            "Iteration 98, loss = 0.45349506\n",
            "Iteration 99, loss = 0.45219371\n",
            "Iteration 100, loss = 0.45085944\n",
            "Iteration 101, loss = 0.44942756\n",
            "Iteration 102, loss = 0.44813940\n",
            "Iteration 103, loss = 0.44690844\n",
            "Iteration 104, loss = 0.44547571\n",
            "Iteration 105, loss = 0.44430929\n",
            "Iteration 106, loss = 0.44288794\n",
            "Iteration 107, loss = 0.44151453\n",
            "Iteration 108, loss = 0.44032489\n",
            "Iteration 109, loss = 0.43896956\n",
            "Iteration 110, loss = 0.43769725\n",
            "Iteration 111, loss = 0.43630658\n",
            "Iteration 112, loss = 0.43509559\n",
            "Iteration 113, loss = 0.43384708\n",
            "Iteration 114, loss = 0.43254063\n",
            "Iteration 115, loss = 0.43120971\n",
            "Iteration 116, loss = 0.42986773\n",
            "Iteration 117, loss = 0.42870071\n",
            "Iteration 118, loss = 0.42726607\n",
            "Iteration 119, loss = 0.42599096\n",
            "Iteration 120, loss = 0.42477903\n",
            "Iteration 121, loss = 0.42345437\n",
            "Iteration 122, loss = 0.42228899\n",
            "Iteration 123, loss = 0.42090497\n",
            "Iteration 124, loss = 0.41966222\n",
            "Iteration 125, loss = 0.41844365\n",
            "Iteration 126, loss = 0.41729542\n",
            "Iteration 127, loss = 0.41597729\n",
            "Iteration 128, loss = 0.41473570\n",
            "Iteration 129, loss = 0.41350020\n",
            "Iteration 130, loss = 0.41227018\n",
            "Iteration 131, loss = 0.41105903\n",
            "Iteration 132, loss = 0.40980869\n",
            "Iteration 133, loss = 0.40851441\n",
            "Iteration 134, loss = 0.40729872\n",
            "Iteration 135, loss = 0.40599856\n",
            "Iteration 136, loss = 0.40484389\n",
            "Iteration 137, loss = 0.40361314\n",
            "Iteration 138, loss = 0.40237746\n",
            "Iteration 139, loss = 0.40110312\n",
            "Iteration 140, loss = 0.39992105\n",
            "Iteration 141, loss = 0.39866828\n",
            "Iteration 142, loss = 0.39745714\n",
            "Iteration 143, loss = 0.39611149\n",
            "Iteration 144, loss = 0.39493668\n",
            "Iteration 145, loss = 0.39381243\n",
            "Iteration 146, loss = 0.39242855\n",
            "Iteration 147, loss = 0.39118272\n",
            "Iteration 148, loss = 0.39020050\n",
            "Iteration 149, loss = 0.38874041\n",
            "Iteration 150, loss = 0.38752283\n",
            "Iteration 151, loss = 0.38617627\n",
            "Iteration 152, loss = 0.38506198\n",
            "Iteration 153, loss = 0.38373649\n",
            "Iteration 154, loss = 0.38264754\n",
            "Iteration 155, loss = 0.38132439\n",
            "Iteration 156, loss = 0.38003065\n",
            "Iteration 157, loss = 0.37915131\n",
            "Iteration 158, loss = 0.37764837\n",
            "Iteration 159, loss = 0.37654082\n",
            "Iteration 160, loss = 0.37521342\n",
            "Iteration 161, loss = 0.37400218\n",
            "Iteration 162, loss = 0.37291644\n",
            "Iteration 163, loss = 0.37168353\n",
            "Iteration 164, loss = 0.37040929\n",
            "Iteration 165, loss = 0.36912707\n",
            "Iteration 166, loss = 0.36798325\n",
            "Iteration 167, loss = 0.36685080\n",
            "Iteration 168, loss = 0.36557598\n",
            "Iteration 169, loss = 0.36440482\n",
            "Iteration 170, loss = 0.36313221\n",
            "Iteration 171, loss = 0.36196165\n",
            "Iteration 172, loss = 0.36089449\n",
            "Iteration 173, loss = 0.35971104\n",
            "Iteration 174, loss = 0.35846852\n",
            "Iteration 175, loss = 0.35725790\n",
            "Iteration 176, loss = 0.35610422\n",
            "Iteration 177, loss = 0.35500574\n",
            "Iteration 178, loss = 0.35388588\n",
            "Iteration 179, loss = 0.35289716\n",
            "Iteration 180, loss = 0.35159951\n",
            "Iteration 181, loss = 0.35041099\n",
            "Iteration 182, loss = 0.34923329\n",
            "Iteration 183, loss = 0.34813898\n",
            "Iteration 184, loss = 0.34691159\n",
            "Iteration 185, loss = 0.34586082\n",
            "Iteration 186, loss = 0.34476067\n",
            "Iteration 187, loss = 0.34353391\n",
            "Iteration 188, loss = 0.34252522\n",
            "Iteration 189, loss = 0.34126613\n",
            "Iteration 190, loss = 0.34009111\n",
            "Iteration 191, loss = 0.33920682\n",
            "Iteration 192, loss = 0.33795883\n",
            "Iteration 193, loss = 0.33682357\n",
            "Iteration 194, loss = 0.33574202\n",
            "Iteration 195, loss = 0.33460440\n",
            "Iteration 196, loss = 0.33350144\n",
            "Iteration 197, loss = 0.33241552\n",
            "Iteration 198, loss = 0.33132322\n",
            "Iteration 199, loss = 0.33020896\n",
            "Iteration 200, loss = 0.32920967\n",
            "Iteration 201, loss = 0.32806359\n",
            "Iteration 202, loss = 0.32696271\n",
            "Iteration 203, loss = 0.32586071\n",
            "Iteration 204, loss = 0.32472927\n",
            "Iteration 205, loss = 0.32372918\n",
            "Iteration 206, loss = 0.32267657\n",
            "Iteration 207, loss = 0.32160749\n",
            "Iteration 208, loss = 0.32047058\n",
            "Iteration 209, loss = 0.31943464\n",
            "Iteration 210, loss = 0.31835597\n",
            "Iteration 211, loss = 0.31736727\n",
            "Iteration 212, loss = 0.31625264\n",
            "Iteration 213, loss = 0.31526781\n",
            "Iteration 214, loss = 0.31422292\n",
            "Iteration 215, loss = 0.31317009\n",
            "Iteration 216, loss = 0.31205102\n",
            "Iteration 217, loss = 0.31105518\n",
            "Iteration 218, loss = 0.30992756\n",
            "Iteration 219, loss = 0.30908430\n",
            "Iteration 220, loss = 0.30791276\n",
            "Iteration 221, loss = 0.30685198\n",
            "Iteration 222, loss = 0.30578795\n",
            "Iteration 223, loss = 0.30484875\n",
            "Iteration 224, loss = 0.30406294\n",
            "Iteration 225, loss = 0.30274870\n",
            "Iteration 226, loss = 0.30173910\n",
            "Iteration 227, loss = 0.30078083\n",
            "Iteration 228, loss = 0.29984553\n",
            "Iteration 229, loss = 0.29881313\n",
            "Iteration 230, loss = 0.29789592\n",
            "Iteration 231, loss = 0.29679881\n",
            "Iteration 232, loss = 0.29588254\n",
            "Iteration 233, loss = 0.29486304\n",
            "Iteration 234, loss = 0.29397308\n",
            "Iteration 235, loss = 0.29316686\n",
            "Iteration 236, loss = 0.29203512\n",
            "Iteration 237, loss = 0.29110499\n",
            "Iteration 238, loss = 0.29014239\n",
            "Iteration 239, loss = 0.28911697\n",
            "Iteration 240, loss = 0.28819705\n",
            "Iteration 241, loss = 0.28726299\n",
            "Iteration 242, loss = 0.28627432\n",
            "Iteration 243, loss = 0.28538440\n",
            "Iteration 244, loss = 0.28440907\n",
            "Iteration 245, loss = 0.28355192\n",
            "Iteration 246, loss = 0.28255292\n",
            "Iteration 247, loss = 0.28168742\n",
            "Iteration 248, loss = 0.28069577\n",
            "Iteration 249, loss = 0.27979441\n",
            "Iteration 250, loss = 0.27886013\n",
            "Iteration 251, loss = 0.27802060\n",
            "Iteration 252, loss = 0.27720805\n",
            "Iteration 253, loss = 0.27616451\n",
            "Iteration 254, loss = 0.27522075\n",
            "Iteration 255, loss = 0.27433254\n",
            "Iteration 256, loss = 0.27345235\n",
            "Iteration 257, loss = 0.27253687\n",
            "Iteration 258, loss = 0.27168425\n",
            "Iteration 259, loss = 0.27086787\n",
            "Iteration 260, loss = 0.26994303\n",
            "Iteration 261, loss = 0.26909390\n",
            "Iteration 262, loss = 0.26819921\n",
            "Iteration 263, loss = 0.26735019\n",
            "Iteration 264, loss = 0.26652206\n",
            "Iteration 265, loss = 0.26561818\n",
            "Iteration 266, loss = 0.26476297\n",
            "Iteration 267, loss = 0.26395694\n",
            "Iteration 268, loss = 0.26302652\n",
            "Iteration 269, loss = 0.26230313\n",
            "Iteration 270, loss = 0.26141564\n",
            "Iteration 271, loss = 0.26048450\n",
            "Iteration 272, loss = 0.25967770\n",
            "Iteration 273, loss = 0.25886639\n",
            "Iteration 274, loss = 0.25803393\n",
            "Iteration 275, loss = 0.25708319\n",
            "Iteration 276, loss = 0.25633520\n",
            "Iteration 277, loss = 0.25544806\n",
            "Iteration 278, loss = 0.25482766\n",
            "Iteration 279, loss = 0.25395521\n",
            "Iteration 280, loss = 0.25297461\n",
            "Iteration 281, loss = 0.25215219\n",
            "Iteration 282, loss = 0.25127176\n",
            "Iteration 283, loss = 0.25045130\n",
            "Iteration 284, loss = 0.24971061\n",
            "Iteration 285, loss = 0.24880995\n",
            "Iteration 286, loss = 0.24806044\n",
            "Iteration 287, loss = 0.24715054\n",
            "Iteration 288, loss = 0.24625807\n",
            "Iteration 289, loss = 0.24545785\n",
            "Iteration 290, loss = 0.24469398\n",
            "Iteration 291, loss = 0.24388388\n",
            "Iteration 292, loss = 0.24305527\n",
            "Iteration 293, loss = 0.24218857\n",
            "Iteration 294, loss = 0.24142531\n",
            "Iteration 295, loss = 0.24060405\n",
            "Iteration 296, loss = 0.23978880\n",
            "Iteration 297, loss = 0.23899689\n",
            "Iteration 298, loss = 0.23830912\n",
            "Iteration 299, loss = 0.23741231\n",
            "Iteration 300, loss = 0.23660603\n",
            "Iteration 301, loss = 0.23580154\n",
            "Iteration 302, loss = 0.23507524\n",
            "Iteration 303, loss = 0.23427345\n",
            "Iteration 304, loss = 0.23346576\n",
            "Iteration 305, loss = 0.23271610\n",
            "Iteration 306, loss = 0.23199933\n",
            "Iteration 307, loss = 0.23115180\n",
            "Iteration 308, loss = 0.23046804\n",
            "Iteration 309, loss = 0.22964513\n",
            "Iteration 310, loss = 0.22885926\n",
            "Iteration 311, loss = 0.22815138\n",
            "Iteration 312, loss = 0.22734099\n",
            "Iteration 313, loss = 0.22663800\n",
            "Iteration 314, loss = 0.22580679\n",
            "Iteration 315, loss = 0.22505158\n",
            "Iteration 316, loss = 0.22436431\n",
            "Iteration 317, loss = 0.22364155\n",
            "Iteration 318, loss = 0.22286218\n",
            "Iteration 319, loss = 0.22215028\n",
            "Iteration 320, loss = 0.22141782\n",
            "Iteration 321, loss = 0.22061742\n",
            "Iteration 322, loss = 0.21996440\n",
            "Iteration 323, loss = 0.21921389\n",
            "Iteration 324, loss = 0.21864917\n",
            "Iteration 325, loss = 0.21777021\n",
            "Iteration 326, loss = 0.21703295\n",
            "Iteration 327, loss = 0.21650959\n",
            "Iteration 328, loss = 0.21589978\n",
            "Iteration 329, loss = 0.21498806\n",
            "Iteration 330, loss = 0.21426619\n",
            "Iteration 331, loss = 0.21360548\n",
            "Iteration 332, loss = 0.21288695\n",
            "Iteration 333, loss = 0.21216114\n",
            "Iteration 334, loss = 0.21151380\n",
            "Iteration 335, loss = 0.21092388\n",
            "Iteration 336, loss = 0.21016482\n",
            "Iteration 337, loss = 0.20958292\n",
            "Iteration 338, loss = 0.20878226\n",
            "Iteration 339, loss = 0.20813658\n",
            "Iteration 340, loss = 0.20760647\n",
            "Iteration 341, loss = 0.20681665\n",
            "Iteration 342, loss = 0.20621441\n",
            "Iteration 343, loss = 0.20547534\n",
            "Iteration 344, loss = 0.20477599\n",
            "Iteration 345, loss = 0.20419575\n",
            "Iteration 346, loss = 0.20354808\n",
            "Iteration 347, loss = 0.20287095\n",
            "Iteration 348, loss = 0.20220828\n",
            "Iteration 349, loss = 0.20161152\n",
            "Iteration 350, loss = 0.20094177\n",
            "Iteration 351, loss = 0.20032478\n",
            "Iteration 352, loss = 0.19964482\n",
            "Iteration 353, loss = 0.19900838\n",
            "Iteration 354, loss = 0.19834276\n",
            "Iteration 355, loss = 0.19780640\n",
            "Iteration 356, loss = 0.19708996\n",
            "Iteration 357, loss = 0.19657722\n",
            "Iteration 358, loss = 0.19593816\n",
            "Iteration 359, loss = 0.19533876\n",
            "Iteration 360, loss = 0.19468966\n",
            "Iteration 361, loss = 0.19408202\n",
            "Iteration 362, loss = 0.19349963\n",
            "Iteration 363, loss = 0.19284221\n",
            "Iteration 364, loss = 0.19225083\n",
            "Iteration 365, loss = 0.19174333\n",
            "Iteration 366, loss = 0.19102673\n",
            "Iteration 367, loss = 0.19045842\n",
            "Iteration 368, loss = 0.19006337\n",
            "Iteration 369, loss = 0.18924046\n",
            "Iteration 370, loss = 0.18867832\n",
            "Iteration 371, loss = 0.18805671\n",
            "Iteration 372, loss = 0.18758175\n",
            "Iteration 373, loss = 0.18696584\n",
            "Iteration 374, loss = 0.18634652\n",
            "Iteration 375, loss = 0.18574376\n",
            "Iteration 376, loss = 0.18522802\n",
            "Iteration 377, loss = 0.18468242\n",
            "Iteration 378, loss = 0.18404212\n",
            "Iteration 379, loss = 0.18350311\n",
            "Iteration 380, loss = 0.18297945\n",
            "Iteration 381, loss = 0.18236708\n",
            "Iteration 382, loss = 0.18180932\n",
            "Iteration 383, loss = 0.18124374\n",
            "Iteration 384, loss = 0.18089183\n",
            "Iteration 385, loss = 0.18019189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 386, loss = 0.17969281\n",
            "Iteration 387, loss = 0.17905956\n",
            "Iteration 388, loss = 0.17854760\n",
            "Iteration 389, loss = 0.17813745\n",
            "Iteration 390, loss = 0.17750534\n",
            "Iteration 391, loss = 0.17701245\n",
            "Iteration 392, loss = 0.17639533\n",
            "Iteration 393, loss = 0.17585642\n",
            "Iteration 394, loss = 0.17536626\n",
            "Iteration 395, loss = 0.17489158\n",
            "Iteration 396, loss = 0.17427739\n",
            "Iteration 397, loss = 0.17383154\n",
            "Iteration 398, loss = 0.17327413\n",
            "Iteration 399, loss = 0.17276652\n",
            "Iteration 400, loss = 0.17227101\n",
            "Iteration 401, loss = 0.17182408\n",
            "Iteration 402, loss = 0.17118747\n",
            "Iteration 403, loss = 0.17068106\n",
            "Iteration 404, loss = 0.17046515\n",
            "Iteration 405, loss = 0.16979126\n",
            "Iteration 406, loss = 0.16928291\n",
            "Iteration 407, loss = 0.16869984\n",
            "Iteration 408, loss = 0.16827615\n",
            "Iteration 409, loss = 0.16775497\n",
            "Iteration 410, loss = 0.16725271\n",
            "Iteration 411, loss = 0.16675521\n",
            "Iteration 412, loss = 0.16626622\n",
            "Iteration 413, loss = 0.16575311\n",
            "Iteration 414, loss = 0.16521894\n",
            "Iteration 415, loss = 0.16505129\n",
            "Iteration 416, loss = 0.16424855\n",
            "Iteration 417, loss = 0.16384589\n",
            "Iteration 418, loss = 0.16329965\n",
            "Iteration 419, loss = 0.16286858\n",
            "Iteration 420, loss = 0.16236962\n",
            "Iteration 421, loss = 0.16200944\n",
            "Iteration 422, loss = 0.16150873\n",
            "Iteration 423, loss = 0.16099620\n",
            "Iteration 424, loss = 0.16049278\n",
            "Iteration 425, loss = 0.16003797\n",
            "Iteration 426, loss = 0.15961389\n",
            "Iteration 427, loss = 0.15918841\n",
            "Iteration 428, loss = 0.15865452\n",
            "Iteration 429, loss = 0.15843874\n",
            "Iteration 430, loss = 0.15773949\n",
            "Iteration 431, loss = 0.15741202\n",
            "Iteration 432, loss = 0.15682434\n",
            "Iteration 433, loss = 0.15635272\n",
            "Iteration 434, loss = 0.15602396\n",
            "Iteration 435, loss = 0.15548447\n",
            "Iteration 436, loss = 0.15505971\n",
            "Iteration 437, loss = 0.15472666\n",
            "Iteration 438, loss = 0.15444786\n",
            "Iteration 439, loss = 0.15378001\n",
            "Iteration 440, loss = 0.15341843\n",
            "Iteration 441, loss = 0.15298441\n",
            "Iteration 442, loss = 0.15254357\n",
            "Iteration 443, loss = 0.15210384\n",
            "Iteration 444, loss = 0.15163847\n",
            "Iteration 445, loss = 0.15121302\n",
            "Iteration 446, loss = 0.15080076\n",
            "Iteration 447, loss = 0.15058789\n",
            "Iteration 448, loss = 0.14988704\n",
            "Iteration 449, loss = 0.14951259\n",
            "Iteration 450, loss = 0.14915537\n",
            "Iteration 451, loss = 0.14876745\n",
            "Iteration 452, loss = 0.14831015\n",
            "Iteration 453, loss = 0.14781666\n",
            "Iteration 454, loss = 0.14741896\n",
            "Iteration 455, loss = 0.14715329\n",
            "Iteration 456, loss = 0.14673348\n",
            "Iteration 457, loss = 0.14634844\n",
            "Iteration 458, loss = 0.14587531\n",
            "Iteration 459, loss = 0.14535439\n",
            "Iteration 460, loss = 0.14499312\n",
            "Iteration 461, loss = 0.14460387\n",
            "Iteration 462, loss = 0.14427948\n",
            "Iteration 463, loss = 0.14387295\n",
            "Iteration 464, loss = 0.14345208\n",
            "Iteration 465, loss = 0.14306285\n",
            "Iteration 466, loss = 0.14260485\n",
            "Iteration 467, loss = 0.14220650\n",
            "Iteration 468, loss = 0.14180941\n",
            "Iteration 469, loss = 0.14145786\n",
            "Iteration 470, loss = 0.14106209\n",
            "Iteration 471, loss = 0.14068971\n",
            "Iteration 472, loss = 0.14031173\n",
            "Iteration 473, loss = 0.13996108\n",
            "Iteration 474, loss = 0.13958790\n",
            "Iteration 475, loss = 0.13915210\n",
            "Iteration 476, loss = 0.13872116\n",
            "Iteration 477, loss = 0.13840523\n",
            "Iteration 478, loss = 0.13805934\n",
            "Iteration 479, loss = 0.13770363\n",
            "Iteration 480, loss = 0.13725606\n",
            "Iteration 481, loss = 0.13690818\n",
            "Iteration 482, loss = 0.13659483\n",
            "Iteration 483, loss = 0.13616869\n",
            "Iteration 484, loss = 0.13591032\n",
            "Iteration 485, loss = 0.13546180\n",
            "Iteration 486, loss = 0.13510504\n",
            "Iteration 487, loss = 0.13465715\n",
            "Iteration 488, loss = 0.13431935\n",
            "Iteration 489, loss = 0.13399983\n",
            "Iteration 490, loss = 0.13359210\n",
            "Iteration 491, loss = 0.13327843\n",
            "Iteration 492, loss = 0.13290438\n",
            "Iteration 493, loss = 0.13250748\n",
            "Iteration 494, loss = 0.13217043\n",
            "Iteration 495, loss = 0.13179054\n",
            "Iteration 496, loss = 0.13145963\n",
            "Iteration 497, loss = 0.13119298\n",
            "Iteration 498, loss = 0.13073609\n",
            "Iteration 499, loss = 0.13046275\n",
            "Iteration 500, loss = 0.13005088\n",
            "Iteration 501, loss = 0.12975473\n",
            "Iteration 502, loss = 0.12940769\n",
            "Iteration 503, loss = 0.12898109\n",
            "Iteration 504, loss = 0.12865443\n",
            "Iteration 505, loss = 0.12833408\n",
            "Iteration 506, loss = 0.12793816\n",
            "Iteration 507, loss = 0.12764204\n",
            "Iteration 508, loss = 0.12730363\n",
            "Iteration 509, loss = 0.12697839\n",
            "Iteration 510, loss = 0.12664403\n",
            "Iteration 511, loss = 0.12623823\n",
            "Iteration 512, loss = 0.12590855\n",
            "Iteration 513, loss = 0.12561409\n",
            "Iteration 514, loss = 0.12533480\n",
            "Iteration 515, loss = 0.12496942\n",
            "Iteration 516, loss = 0.12472638\n",
            "Iteration 517, loss = 0.12435669\n",
            "Iteration 518, loss = 0.12399790\n",
            "Iteration 519, loss = 0.12368021\n",
            "Iteration 520, loss = 0.12335169\n",
            "Iteration 521, loss = 0.12320232\n",
            "Iteration 522, loss = 0.12283818\n",
            "Iteration 523, loss = 0.12238803\n",
            "Iteration 524, loss = 0.12205366\n",
            "Iteration 525, loss = 0.12172230\n",
            "Iteration 526, loss = 0.12145137\n",
            "Iteration 527, loss = 0.12105388\n",
            "Iteration 528, loss = 0.12082184\n",
            "Iteration 529, loss = 0.12075947\n",
            "Iteration 530, loss = 0.12041327\n",
            "Iteration 531, loss = 0.11987897\n",
            "Iteration 532, loss = 0.11955241\n",
            "Iteration 533, loss = 0.11932711\n",
            "Iteration 534, loss = 0.11899514\n",
            "Iteration 535, loss = 0.11873240\n",
            "Iteration 536, loss = 0.11835018\n",
            "Iteration 537, loss = 0.11805991\n",
            "Iteration 538, loss = 0.11778880\n",
            "Iteration 539, loss = 0.11744120\n",
            "Iteration 540, loss = 0.11712432\n",
            "Iteration 541, loss = 0.11694618\n",
            "Iteration 542, loss = 0.11664344\n",
            "Iteration 543, loss = 0.11636176\n",
            "Iteration 544, loss = 0.11595845\n",
            "Iteration 545, loss = 0.11561901\n",
            "Iteration 546, loss = 0.11543972\n",
            "Iteration 547, loss = 0.11508413\n",
            "Iteration 548, loss = 0.11479706\n",
            "Iteration 549, loss = 0.11450256\n",
            "Iteration 550, loss = 0.11421153\n",
            "Iteration 551, loss = 0.11404434\n",
            "Iteration 552, loss = 0.11375480\n",
            "Iteration 553, loss = 0.11345161\n",
            "Iteration 554, loss = 0.11310470\n",
            "Iteration 555, loss = 0.11282934\n",
            "Iteration 556, loss = 0.11251372\n",
            "Iteration 557, loss = 0.11228227\n",
            "Iteration 558, loss = 0.11196822\n",
            "Iteration 559, loss = 0.11170582\n",
            "Iteration 560, loss = 0.11158553\n",
            "Iteration 561, loss = 0.11120740\n",
            "Iteration 562, loss = 0.11089519\n",
            "Iteration 563, loss = 0.11063829\n",
            "Iteration 564, loss = 0.11037544\n",
            "Iteration 565, loss = 0.11009542\n",
            "Iteration 566, loss = 0.10984414\n",
            "Iteration 567, loss = 0.10959090\n",
            "Iteration 568, loss = 0.10929436\n",
            "Iteration 569, loss = 0.10904359\n",
            "Iteration 570, loss = 0.10877350\n",
            "Iteration 571, loss = 0.10848086\n",
            "Iteration 572, loss = 0.10818383\n",
            "Iteration 573, loss = 0.10790400\n",
            "Iteration 574, loss = 0.10762964\n",
            "Iteration 575, loss = 0.10743483\n",
            "Iteration 576, loss = 0.10716739\n",
            "Iteration 577, loss = 0.10701347\n",
            "Iteration 578, loss = 0.10663600\n",
            "Iteration 579, loss = 0.10636155\n",
            "Iteration 580, loss = 0.10607509\n",
            "Iteration 581, loss = 0.10582464\n",
            "Iteration 582, loss = 0.10559218\n",
            "Iteration 583, loss = 0.10532690\n",
            "Iteration 584, loss = 0.10506330\n",
            "Iteration 585, loss = 0.10475782\n",
            "Iteration 586, loss = 0.10461730\n",
            "Iteration 587, loss = 0.10422571\n",
            "Iteration 588, loss = 0.10399894\n",
            "Iteration 589, loss = 0.10383844\n",
            "Iteration 590, loss = 0.10361149\n",
            "Iteration 591, loss = 0.10324675\n",
            "Iteration 592, loss = 0.10301584\n",
            "Iteration 593, loss = 0.10274294\n",
            "Iteration 594, loss = 0.10255770\n",
            "Iteration 595, loss = 0.10233022\n",
            "Iteration 596, loss = 0.10198477\n",
            "Iteration 597, loss = 0.10174497\n",
            "Iteration 598, loss = 0.10152924\n",
            "Iteration 599, loss = 0.10132624\n",
            "Iteration 600, loss = 0.10101235\n",
            "Iteration 601, loss = 0.10080308\n",
            "Iteration 602, loss = 0.10055420\n",
            "Iteration 603, loss = 0.10037829\n",
            "Iteration 604, loss = 0.10006509\n",
            "Iteration 605, loss = 0.09982897\n",
            "Iteration 606, loss = 0.09962833\n",
            "Iteration 607, loss = 0.09930878\n",
            "Iteration 608, loss = 0.09912549\n",
            "Iteration 609, loss = 0.09894208\n",
            "Iteration 610, loss = 0.09875678\n",
            "Iteration 611, loss = 0.09849660\n",
            "Iteration 612, loss = 0.09814626\n",
            "Iteration 613, loss = 0.09805418\n",
            "Iteration 614, loss = 0.09799147\n",
            "Iteration 615, loss = 0.09775301\n",
            "Iteration 616, loss = 0.09748438\n",
            "Iteration 617, loss = 0.09716274\n",
            "Iteration 618, loss = 0.09683181\n",
            "Iteration 619, loss = 0.09670673\n",
            "Iteration 620, loss = 0.09646955\n",
            "Iteration 621, loss = 0.09626688\n",
            "Iteration 622, loss = 0.09610843\n",
            "Iteration 623, loss = 0.09574147\n",
            "Iteration 624, loss = 0.09548804\n",
            "Iteration 625, loss = 0.09536467\n",
            "Iteration 626, loss = 0.09505770\n",
            "Iteration 627, loss = 0.09482263\n",
            "Iteration 628, loss = 0.09463135\n",
            "Iteration 629, loss = 0.09454183\n",
            "Iteration 630, loss = 0.09424086\n",
            "Iteration 631, loss = 0.09403640\n",
            "Iteration 632, loss = 0.09377140\n",
            "Iteration 633, loss = 0.09353286\n",
            "Iteration 634, loss = 0.09334731\n",
            "Iteration 635, loss = 0.09309308\n",
            "Iteration 636, loss = 0.09289785\n",
            "Iteration 637, loss = 0.09265357\n",
            "Iteration 638, loss = 0.09243678\n",
            "Iteration 639, loss = 0.09225668\n",
            "Iteration 640, loss = 0.09205984\n",
            "Iteration 641, loss = 0.09187024\n",
            "Iteration 642, loss = 0.09167391\n",
            "Iteration 643, loss = 0.09171251\n",
            "Iteration 644, loss = 0.09122416\n",
            "Iteration 645, loss = 0.09102895\n",
            "Iteration 646, loss = 0.09083052\n",
            "Iteration 647, loss = 0.09059907\n",
            "Iteration 648, loss = 0.09038248\n",
            "Iteration 649, loss = 0.09026965\n",
            "Iteration 650, loss = 0.08995515\n",
            "Iteration 651, loss = 0.08976491\n",
            "Iteration 652, loss = 0.08958663\n",
            "Iteration 653, loss = 0.08943261\n",
            "Iteration 654, loss = 0.08919911\n",
            "Iteration 655, loss = 0.08900042\n",
            "Iteration 656, loss = 0.08878191\n",
            "Iteration 657, loss = 0.08857422\n",
            "Iteration 658, loss = 0.08850028\n",
            "Iteration 659, loss = 0.08819384\n",
            "Iteration 660, loss = 0.08828684\n",
            "Iteration 661, loss = 0.08779531\n",
            "Iteration 662, loss = 0.08761245\n",
            "Iteration 663, loss = 0.08741339\n",
            "Iteration 664, loss = 0.08737513\n",
            "Iteration 665, loss = 0.08706084\n",
            "Iteration 666, loss = 0.08693719\n",
            "Iteration 667, loss = 0.08675957\n",
            "Iteration 668, loss = 0.08648420\n",
            "Iteration 669, loss = 0.08640396\n",
            "Iteration 670, loss = 0.08616340\n",
            "Iteration 671, loss = 0.08608660\n",
            "Iteration 672, loss = 0.08575970\n",
            "Iteration 673, loss = 0.08563784\n",
            "Iteration 674, loss = 0.08536029\n",
            "Iteration 675, loss = 0.08517428\n",
            "Iteration 676, loss = 0.08507171\n",
            "Iteration 677, loss = 0.08482341\n",
            "Iteration 678, loss = 0.08467156\n",
            "Iteration 679, loss = 0.08463942\n",
            "Iteration 680, loss = 0.08447429\n",
            "Iteration 681, loss = 0.08418375\n",
            "Iteration 682, loss = 0.08402257\n",
            "Iteration 683, loss = 0.08377519\n",
            "Iteration 684, loss = 0.08364276\n",
            "Iteration 685, loss = 0.08346225\n",
            "Iteration 686, loss = 0.08328860\n",
            "Iteration 687, loss = 0.08311474\n",
            "Iteration 688, loss = 0.08294705\n",
            "Iteration 689, loss = 0.08275117\n",
            "Iteration 690, loss = 0.08259338\n",
            "Iteration 691, loss = 0.08242722\n",
            "Iteration 692, loss = 0.08224641\n",
            "Iteration 693, loss = 0.08202195\n",
            "Iteration 694, loss = 0.08190454\n",
            "Iteration 695, loss = 0.08183588\n",
            "Iteration 696, loss = 0.08157302\n",
            "Iteration 697, loss = 0.08138835\n",
            "Iteration 698, loss = 0.08121790\n",
            "Iteration 699, loss = 0.08100829\n",
            "Iteration 700, loss = 0.08098927\n",
            "Iteration 701, loss = 0.08067942\n",
            "Iteration 702, loss = 0.08070267\n",
            "Iteration 703, loss = 0.08054813\n",
            "Iteration 704, loss = 0.08028339\n",
            "Iteration 705, loss = 0.08007403\n",
            "Iteration 706, loss = 0.07986266\n",
            "Iteration 707, loss = 0.07973193\n",
            "Iteration 708, loss = 0.07956862\n",
            "Iteration 709, loss = 0.07946996\n",
            "Iteration 710, loss = 0.07934736\n",
            "Iteration 711, loss = 0.07910489\n",
            "Iteration 712, loss = 0.07904003\n",
            "Iteration 713, loss = 0.07879666\n",
            "Iteration 714, loss = 0.07868255\n",
            "Iteration 715, loss = 0.07848505\n",
            "Iteration 716, loss = 0.07834503\n",
            "Iteration 717, loss = 0.07820163\n",
            "Iteration 718, loss = 0.07798655\n",
            "Iteration 719, loss = 0.07784365\n",
            "Iteration 720, loss = 0.07767261\n",
            "Iteration 721, loss = 0.07754311\n",
            "Iteration 722, loss = 0.07753025\n",
            "Iteration 723, loss = 0.07723139\n",
            "Iteration 724, loss = 0.07710317\n",
            "Iteration 725, loss = 0.07701328\n",
            "Iteration 726, loss = 0.07683338\n",
            "Iteration 727, loss = 0.07665937\n",
            "Iteration 728, loss = 0.07655748\n",
            "Iteration 729, loss = 0.07652117\n",
            "Iteration 730, loss = 0.07629959\n",
            "Iteration 731, loss = 0.07615921\n",
            "Iteration 732, loss = 0.07596079\n",
            "Iteration 733, loss = 0.07582656\n",
            "Iteration 734, loss = 0.07567901\n",
            "Iteration 735, loss = 0.07553249\n",
            "Iteration 736, loss = 0.07546784\n",
            "Iteration 737, loss = 0.07519540\n",
            "Iteration 738, loss = 0.07509728\n",
            "Iteration 739, loss = 0.07491555\n",
            "Iteration 740, loss = 0.07476922\n",
            "Iteration 741, loss = 0.07465544\n",
            "Iteration 742, loss = 0.07447348\n",
            "Iteration 743, loss = 0.07445427\n",
            "Iteration 744, loss = 0.07430530\n",
            "Iteration 745, loss = 0.07414052\n",
            "Iteration 746, loss = 0.07400277\n",
            "Iteration 747, loss = 0.07383377\n",
            "Iteration 748, loss = 0.07371658\n",
            "Iteration 749, loss = 0.07356415\n",
            "Iteration 750, loss = 0.07344117\n",
            "Iteration 751, loss = 0.07326809\n",
            "Iteration 752, loss = 0.07316678\n",
            "Iteration 753, loss = 0.07311131\n",
            "Iteration 754, loss = 0.07280356\n",
            "Iteration 755, loss = 0.07265212\n",
            "Iteration 756, loss = 0.07269953\n",
            "Iteration 757, loss = 0.07247491\n",
            "Iteration 758, loss = 0.07234645\n",
            "Iteration 759, loss = 0.07222475\n",
            "Iteration 760, loss = 0.07201337\n",
            "Iteration 761, loss = 0.07202666\n",
            "Iteration 762, loss = 0.07189961\n",
            "Iteration 763, loss = 0.07174012\n",
            "Iteration 764, loss = 0.07144760\n",
            "Iteration 765, loss = 0.07132876\n",
            "Iteration 766, loss = 0.07118295\n",
            "Iteration 767, loss = 0.07121047\n",
            "Iteration 768, loss = 0.07102592\n",
            "Iteration 769, loss = 0.07078120\n",
            "Iteration 770, loss = 0.07065700\n",
            "Iteration 771, loss = 0.07050309\n",
            "Iteration 772, loss = 0.07034563\n",
            "Iteration 773, loss = 0.07026448\n",
            "Iteration 774, loss = 0.07008434\n",
            "Iteration 775, loss = 0.06998377\n",
            "Iteration 776, loss = 0.06987407\n",
            "Iteration 777, loss = 0.06984234\n",
            "Iteration 778, loss = 0.06957580\n",
            "Iteration 779, loss = 0.06970801\n",
            "Iteration 780, loss = 0.06943525\n",
            "Iteration 781, loss = 0.06921784\n",
            "Iteration 782, loss = 0.06935500\n",
            "Iteration 783, loss = 0.06906199\n",
            "Iteration 784, loss = 0.06890070\n",
            "Iteration 785, loss = 0.06874606\n",
            "Iteration 786, loss = 0.06857323\n",
            "Iteration 787, loss = 0.06848252\n",
            "Iteration 788, loss = 0.06836939\n",
            "Iteration 789, loss = 0.06824059\n",
            "Iteration 790, loss = 0.06805974\n",
            "Iteration 791, loss = 0.06800390\n",
            "Iteration 792, loss = 0.06784205\n",
            "Iteration 793, loss = 0.06774149\n",
            "Iteration 794, loss = 0.06757644\n",
            "Iteration 795, loss = 0.06751951\n",
            "Iteration 796, loss = 0.06736614\n",
            "Iteration 797, loss = 0.06717298\n",
            "Iteration 798, loss = 0.06706569\n",
            "Iteration 799, loss = 0.06705103\n",
            "Iteration 800, loss = 0.06683691\n",
            "Iteration 801, loss = 0.06669887\n",
            "Iteration 802, loss = 0.06654996\n",
            "Iteration 803, loss = 0.06661227\n",
            "Iteration 804, loss = 0.06641082\n",
            "Iteration 805, loss = 0.06626372\n",
            "Iteration 806, loss = 0.06614301\n",
            "Iteration 807, loss = 0.06593054\n",
            "Iteration 808, loss = 0.06587506\n",
            "Iteration 809, loss = 0.06572392\n",
            "Iteration 810, loss = 0.06562100\n",
            "Iteration 811, loss = 0.06547534\n",
            "Iteration 812, loss = 0.06533436\n",
            "Iteration 813, loss = 0.06519003\n",
            "Iteration 814, loss = 0.06508133\n",
            "Iteration 815, loss = 0.06504367\n",
            "Iteration 816, loss = 0.06487907\n",
            "Iteration 817, loss = 0.06472959\n",
            "Iteration 818, loss = 0.06462114\n",
            "Iteration 819, loss = 0.06450190\n",
            "Iteration 820, loss = 0.06444336\n",
            "Iteration 821, loss = 0.06427187\n",
            "Iteration 822, loss = 0.06422645\n",
            "Iteration 823, loss = 0.06408644\n",
            "Iteration 824, loss = 0.06394227\n",
            "Iteration 825, loss = 0.06388960\n",
            "Iteration 826, loss = 0.06383679\n",
            "Iteration 827, loss = 0.06359149\n",
            "Iteration 828, loss = 0.06353623\n",
            "Iteration 829, loss = 0.06340116\n",
            "Iteration 830, loss = 0.06338536\n",
            "Iteration 831, loss = 0.06327270\n",
            "Iteration 832, loss = 0.06310708\n",
            "Iteration 833, loss = 0.06302907\n",
            "Iteration 834, loss = 0.06291015\n",
            "Iteration 835, loss = 0.06275523\n",
            "Iteration 836, loss = 0.06262963\n",
            "Iteration 837, loss = 0.06253072\n",
            "Iteration 838, loss = 0.06243380\n",
            "Iteration 839, loss = 0.06230346\n",
            "Iteration 840, loss = 0.06220560\n",
            "Iteration 841, loss = 0.06210414\n",
            "Iteration 842, loss = 0.06201902\n",
            "Iteration 843, loss = 0.06188348\n",
            "Iteration 844, loss = 0.06194267\n",
            "Iteration 845, loss = 0.06168296\n",
            "Iteration 846, loss = 0.06160219\n",
            "Iteration 847, loss = 0.06152737\n",
            "Iteration 848, loss = 0.06147619\n",
            "Iteration 849, loss = 0.06129487\n",
            "Iteration 850, loss = 0.06124211\n",
            "Iteration 851, loss = 0.06119572\n",
            "Iteration 852, loss = 0.06107096\n",
            "Iteration 853, loss = 0.06101680\n",
            "Iteration 854, loss = 0.06087657\n",
            "Iteration 855, loss = 0.06074566\n",
            "Iteration 856, loss = 0.06068615\n",
            "Iteration 857, loss = 0.06055131\n",
            "Iteration 858, loss = 0.06042997\n",
            "Iteration 859, loss = 0.06033681\n",
            "Iteration 860, loss = 0.06028965\n",
            "Iteration 861, loss = 0.06025304\n",
            "Iteration 862, loss = 0.06009030\n",
            "Iteration 863, loss = 0.05996234\n",
            "Iteration 864, loss = 0.05985678\n",
            "Iteration 865, loss = 0.05977260\n",
            "Iteration 866, loss = 0.05963680\n",
            "Iteration 867, loss = 0.05958080\n",
            "Iteration 868, loss = 0.05945323\n",
            "Iteration 869, loss = 0.05940835\n",
            "Iteration 870, loss = 0.05930986\n",
            "Iteration 871, loss = 0.05928850\n",
            "Iteration 872, loss = 0.05905126\n",
            "Iteration 873, loss = 0.05901459\n",
            "Iteration 874, loss = 0.05895676\n",
            "Iteration 875, loss = 0.05889835\n",
            "Iteration 876, loss = 0.05888639\n",
            "Iteration 877, loss = 0.05862736\n",
            "Iteration 878, loss = 0.05853991\n",
            "Iteration 879, loss = 0.05846111\n",
            "Iteration 880, loss = 0.05835778\n",
            "Iteration 881, loss = 0.05830640\n",
            "Iteration 882, loss = 0.05817813\n",
            "Iteration 883, loss = 0.05819952\n",
            "Iteration 884, loss = 0.05802497\n",
            "Iteration 885, loss = 0.05800659\n",
            "Iteration 886, loss = 0.05783771\n",
            "Iteration 887, loss = 0.05780211\n",
            "Iteration 888, loss = 0.05770353\n",
            "Iteration 889, loss = 0.05759969\n",
            "Iteration 890, loss = 0.05756038\n",
            "Iteration 891, loss = 0.05746284\n",
            "Iteration 892, loss = 0.05730214\n",
            "Iteration 893, loss = 0.05729372\n",
            "Iteration 894, loss = 0.05718797\n",
            "Iteration 895, loss = 0.05712851\n",
            "Iteration 896, loss = 0.05702309\n",
            "Iteration 897, loss = 0.05692631\n",
            "Iteration 898, loss = 0.05683489\n",
            "Iteration 899, loss = 0.05673056\n",
            "Iteration 900, loss = 0.05665295\n",
            "Iteration 901, loss = 0.05650026\n",
            "Iteration 902, loss = 0.05644826\n",
            "Iteration 903, loss = 0.05631756\n",
            "Iteration 904, loss = 0.05630465\n",
            "Iteration 905, loss = 0.05620669\n",
            "Iteration 906, loss = 0.05607119\n",
            "Iteration 907, loss = 0.05607408\n",
            "Iteration 908, loss = 0.05591565\n",
            "Iteration 909, loss = 0.05611733\n",
            "Iteration 910, loss = 0.05583180\n",
            "Iteration 911, loss = 0.05567844\n",
            "Iteration 912, loss = 0.05559758\n",
            "Iteration 913, loss = 0.05549202\n",
            "Iteration 914, loss = 0.05545628\n",
            "Iteration 915, loss = 0.05534530\n",
            "Iteration 916, loss = 0.05528383\n",
            "Iteration 917, loss = 0.05517798\n",
            "Iteration 918, loss = 0.05511054\n",
            "Iteration 919, loss = 0.05506536\n",
            "Iteration 920, loss = 0.05496752\n",
            "Iteration 921, loss = 0.05485809\n",
            "Iteration 922, loss = 0.05482053\n",
            "Iteration 923, loss = 0.05477056\n",
            "Iteration 924, loss = 0.05469621\n",
            "Iteration 925, loss = 0.05456778\n",
            "Iteration 926, loss = 0.05443230\n",
            "Iteration 927, loss = 0.05466962\n",
            "Iteration 928, loss = 0.05431442\n",
            "Iteration 929, loss = 0.05418045\n",
            "Iteration 930, loss = 0.05408387\n",
            "Iteration 931, loss = 0.05410165\n",
            "Iteration 932, loss = 0.05396291\n",
            "Iteration 933, loss = 0.05387746\n",
            "Iteration 934, loss = 0.05380114\n",
            "Iteration 935, loss = 0.05378464\n",
            "Iteration 936, loss = 0.05361769\n",
            "Iteration 937, loss = 0.05358097\n",
            "Iteration 938, loss = 0.05350128\n",
            "Iteration 939, loss = 0.05342318\n",
            "Iteration 940, loss = 0.05333737\n",
            "Iteration 941, loss = 0.05324230\n",
            "Iteration 942, loss = 0.05322890\n",
            "Iteration 943, loss = 0.05309154\n",
            "Iteration 944, loss = 0.05309523\n",
            "Iteration 945, loss = 0.05298822\n",
            "Iteration 946, loss = 0.05289116\n",
            "Iteration 947, loss = 0.05279717\n",
            "Iteration 948, loss = 0.05272049\n",
            "Iteration 949, loss = 0.05265356\n",
            "Iteration 950, loss = 0.05255300\n",
            "Iteration 951, loss = 0.05249996\n",
            "Iteration 952, loss = 0.05244763\n",
            "Iteration 953, loss = 0.05232096\n",
            "Iteration 954, loss = 0.05232985\n",
            "Iteration 955, loss = 0.05224813\n",
            "Iteration 956, loss = 0.05213377\n",
            "Iteration 957, loss = 0.05209411\n",
            "Iteration 958, loss = 0.05197706\n",
            "Iteration 959, loss = 0.05190541\n",
            "Iteration 960, loss = 0.05184723\n",
            "Iteration 961, loss = 0.05178651\n",
            "Iteration 962, loss = 0.05173630\n",
            "Iteration 963, loss = 0.05159243\n",
            "Iteration 964, loss = 0.05153441\n",
            "Iteration 965, loss = 0.05151646\n",
            "Iteration 966, loss = 0.05152337\n",
            "Iteration 967, loss = 0.05144213\n",
            "Iteration 968, loss = 0.05129641\n",
            "Iteration 969, loss = 0.05123974\n",
            "Iteration 970, loss = 0.05113702\n",
            "Iteration 971, loss = 0.05112485\n",
            "Iteration 972, loss = 0.05102656\n",
            "Iteration 973, loss = 0.05093489\n",
            "Iteration 974, loss = 0.05086487\n",
            "Iteration 975, loss = 0.05077307\n",
            "Iteration 976, loss = 0.05076726\n",
            "Iteration 977, loss = 0.05072707\n",
            "Iteration 978, loss = 0.05061759\n",
            "Iteration 979, loss = 0.05062203\n",
            "Iteration 980, loss = 0.05048246\n",
            "Iteration 981, loss = 0.05037101\n",
            "Iteration 982, loss = 0.05027092\n",
            "Iteration 983, loss = 0.05023310\n",
            "Iteration 984, loss = 0.05013771\n",
            "Iteration 985, loss = 0.05008882\n",
            "Iteration 986, loss = 0.05001987\n",
            "Iteration 987, loss = 0.04995605\n",
            "Iteration 988, loss = 0.05008566\n",
            "Iteration 989, loss = 0.04992100\n",
            "Iteration 990, loss = 0.04981616\n",
            "Iteration 991, loss = 0.04977597\n",
            "Iteration 992, loss = 0.04963269\n",
            "Iteration 993, loss = 0.04954388\n",
            "Iteration 994, loss = 0.04947980\n",
            "Iteration 995, loss = 0.04947450\n",
            "Iteration 996, loss = 0.04937963\n",
            "Iteration 997, loss = 0.04940986\n",
            "Iteration 998, loss = 0.04925216\n",
            "Iteration 999, loss = 0.04912187\n",
            "Iteration 1000, loss = 0.04905444\n",
            "Acurácia: 0.7654320987654321\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.89      0.68      0.77        47\n",
            "   recurrence-events       0.67      0.88      0.76        34\n",
            "\n",
            "            accuracy                           0.77        81\n",
            "           macro avg       0.78      0.78      0.77        81\n",
            "        weighted avg       0.80      0.77      0.77        81\n",
            "\n",
            "[[32 15]\n",
            " [ 4 30]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Avaliando topologia: (10, 5)\n",
            "Iteration 1, loss = 0.69704508\n",
            "Iteration 2, loss = 0.69413507\n",
            "Iteration 3, loss = 0.69206345\n",
            "Iteration 4, loss = 0.68973619\n",
            "Iteration 5, loss = 0.68774916\n",
            "Iteration 6, loss = 0.68564772\n",
            "Iteration 7, loss = 0.68356818\n",
            "Iteration 8, loss = 0.68183985\n",
            "Iteration 9, loss = 0.67990038\n",
            "Iteration 10, loss = 0.67814125\n",
            "Iteration 11, loss = 0.67643939\n",
            "Iteration 12, loss = 0.67464683\n",
            "Iteration 13, loss = 0.67296584\n",
            "Iteration 14, loss = 0.67122694\n",
            "Iteration 15, loss = 0.66956552\n",
            "Iteration 16, loss = 0.66789567\n",
            "Iteration 17, loss = 0.66617436\n",
            "Iteration 18, loss = 0.66447482\n",
            "Iteration 19, loss = 0.66281607\n",
            "Iteration 20, loss = 0.66112741\n",
            "Iteration 21, loss = 0.65940420\n",
            "Iteration 22, loss = 0.65773219\n",
            "Iteration 23, loss = 0.65591389\n",
            "Iteration 24, loss = 0.65432023\n",
            "Iteration 25, loss = 0.65261731\n",
            "Iteration 26, loss = 0.65079569\n",
            "Iteration 27, loss = 0.64934447\n",
            "Iteration 28, loss = 0.64759700\n",
            "Iteration 29, loss = 0.64579700\n",
            "Iteration 30, loss = 0.64394282\n",
            "Iteration 31, loss = 0.64249301\n",
            "Iteration 32, loss = 0.64068119\n",
            "Iteration 33, loss = 0.63887264\n",
            "Iteration 34, loss = 0.63715687\n",
            "Iteration 35, loss = 0.63544844\n",
            "Iteration 36, loss = 0.63366437\n",
            "Iteration 37, loss = 0.63198294\n",
            "Iteration 38, loss = 0.63027897\n",
            "Iteration 39, loss = 0.62855887\n",
            "Iteration 40, loss = 0.62704679\n",
            "Iteration 41, loss = 0.62542171\n",
            "Iteration 42, loss = 0.62402746\n",
            "Iteration 43, loss = 0.62245571\n",
            "Iteration 44, loss = 0.62096923\n",
            "Iteration 45, loss = 0.61962134\n",
            "Iteration 46, loss = 0.61815450\n",
            "Iteration 47, loss = 0.61698136\n",
            "Iteration 48, loss = 0.61559638\n",
            "Iteration 49, loss = 0.61437354\n",
            "Iteration 50, loss = 0.61310434\n",
            "Iteration 51, loss = 0.61190641\n",
            "Iteration 52, loss = 0.61083475\n",
            "Iteration 53, loss = 0.60964238\n",
            "Iteration 54, loss = 0.60859016\n",
            "Iteration 55, loss = 0.60745527\n",
            "Iteration 56, loss = 0.60633576\n",
            "Iteration 57, loss = 0.60529309\n",
            "Iteration 58, loss = 0.60424457\n",
            "Iteration 59, loss = 0.60319494\n",
            "Iteration 60, loss = 0.60213212\n",
            "Iteration 61, loss = 0.60104204\n",
            "Iteration 62, loss = 0.59994995\n",
            "Iteration 63, loss = 0.59885142\n",
            "Iteration 64, loss = 0.59779989\n",
            "Iteration 65, loss = 0.59670682\n",
            "Iteration 66, loss = 0.59562827\n",
            "Iteration 67, loss = 0.59455670\n",
            "Iteration 68, loss = 0.59350137\n",
            "Iteration 69, loss = 0.59244958\n",
            "Iteration 70, loss = 0.59132100\n",
            "Iteration 71, loss = 0.59024629\n",
            "Iteration 72, loss = 0.58920277\n",
            "Iteration 73, loss = 0.58807280\n",
            "Iteration 74, loss = 0.58694648\n",
            "Iteration 75, loss = 0.58575599\n",
            "Iteration 76, loss = 0.58465668\n",
            "Iteration 77, loss = 0.58349442\n",
            "Iteration 78, loss = 0.58224872\n",
            "Iteration 79, loss = 0.58107951\n",
            "Iteration 80, loss = 0.57981775\n",
            "Iteration 81, loss = 0.57867653\n",
            "Iteration 82, loss = 0.57746291\n",
            "Iteration 83, loss = 0.57619571\n",
            "Iteration 84, loss = 0.57501440\n",
            "Iteration 85, loss = 0.57373544\n",
            "Iteration 86, loss = 0.57249320\n",
            "Iteration 87, loss = 0.57121106\n",
            "Iteration 88, loss = 0.56996334\n",
            "Iteration 89, loss = 0.56864003\n",
            "Iteration 90, loss = 0.56739688\n",
            "Iteration 91, loss = 0.56607307\n",
            "Iteration 92, loss = 0.56477474\n",
            "Iteration 93, loss = 0.56339805\n",
            "Iteration 94, loss = 0.56210022\n",
            "Iteration 95, loss = 0.56077136\n",
            "Iteration 96, loss = 0.55950685\n",
            "Iteration 97, loss = 0.55820535\n",
            "Iteration 98, loss = 0.55679869\n",
            "Iteration 99, loss = 0.55537593\n",
            "Iteration 100, loss = 0.55404094\n",
            "Iteration 101, loss = 0.55275944\n",
            "Iteration 102, loss = 0.55138471\n",
            "Iteration 103, loss = 0.55017145\n",
            "Iteration 104, loss = 0.54895836\n",
            "Iteration 105, loss = 0.54744234\n",
            "Iteration 106, loss = 0.54620394\n",
            "Iteration 107, loss = 0.54495812\n",
            "Iteration 108, loss = 0.54364358\n",
            "Iteration 109, loss = 0.54240317\n",
            "Iteration 110, loss = 0.54114055\n",
            "Iteration 111, loss = 0.53982385\n",
            "Iteration 112, loss = 0.53856921\n",
            "Iteration 113, loss = 0.53735604\n",
            "Iteration 114, loss = 0.53603473\n",
            "Iteration 115, loss = 0.53478394\n",
            "Iteration 116, loss = 0.53353538\n",
            "Iteration 117, loss = 0.53222972\n",
            "Iteration 118, loss = 0.53101811\n",
            "Iteration 119, loss = 0.52971950\n",
            "Iteration 120, loss = 0.52847886\n",
            "Iteration 121, loss = 0.52722585\n",
            "Iteration 122, loss = 0.52598696\n",
            "Iteration 123, loss = 0.52483974\n",
            "Iteration 124, loss = 0.52354469\n",
            "Iteration 125, loss = 0.52232236\n",
            "Iteration 126, loss = 0.52120427\n",
            "Iteration 127, loss = 0.51978765\n",
            "Iteration 128, loss = 0.51854076\n",
            "Iteration 129, loss = 0.51726906\n",
            "Iteration 130, loss = 0.51608659\n",
            "Iteration 131, loss = 0.51484528\n",
            "Iteration 132, loss = 0.51369370\n",
            "Iteration 133, loss = 0.51239382\n",
            "Iteration 134, loss = 0.51139135\n",
            "Iteration 135, loss = 0.50999051\n",
            "Iteration 136, loss = 0.50886091\n",
            "Iteration 137, loss = 0.50761320\n",
            "Iteration 138, loss = 0.50640671\n",
            "Iteration 139, loss = 0.50524737\n",
            "Iteration 140, loss = 0.50403563\n",
            "Iteration 141, loss = 0.50287134\n",
            "Iteration 142, loss = 0.50157565\n",
            "Iteration 143, loss = 0.50049027\n",
            "Iteration 144, loss = 0.49935964\n",
            "Iteration 145, loss = 0.49835114\n",
            "Iteration 146, loss = 0.49697171\n",
            "Iteration 147, loss = 0.49590355\n",
            "Iteration 148, loss = 0.49464674\n",
            "Iteration 149, loss = 0.49364998\n",
            "Iteration 150, loss = 0.49241421\n",
            "Iteration 151, loss = 0.49141324\n",
            "Iteration 152, loss = 0.49028428\n",
            "Iteration 153, loss = 0.48907121\n",
            "Iteration 154, loss = 0.48803153\n",
            "Iteration 155, loss = 0.48692557\n",
            "Iteration 156, loss = 0.48571334\n",
            "Iteration 157, loss = 0.48460553\n",
            "Iteration 158, loss = 0.48362125\n",
            "Iteration 159, loss = 0.48249238\n",
            "Iteration 160, loss = 0.48134536\n",
            "Iteration 161, loss = 0.48014093\n",
            "Iteration 162, loss = 0.47902195\n",
            "Iteration 163, loss = 0.47794337\n",
            "Iteration 164, loss = 0.47681409\n",
            "Iteration 165, loss = 0.47568985\n",
            "Iteration 166, loss = 0.47461220\n",
            "Iteration 167, loss = 0.47343245\n",
            "Iteration 168, loss = 0.47237439\n",
            "Iteration 169, loss = 0.47124092\n",
            "Iteration 170, loss = 0.47025875\n",
            "Iteration 171, loss = 0.46903185\n",
            "Iteration 172, loss = 0.46788097\n",
            "Iteration 173, loss = 0.46675071\n",
            "Iteration 174, loss = 0.46564063\n",
            "Iteration 175, loss = 0.46448688\n",
            "Iteration 176, loss = 0.46347549\n",
            "Iteration 177, loss = 0.46229610\n",
            "Iteration 178, loss = 0.46108450\n",
            "Iteration 179, loss = 0.45989988\n",
            "Iteration 180, loss = 0.45858135\n",
            "Iteration 181, loss = 0.45736424\n",
            "Iteration 182, loss = 0.45624009\n",
            "Iteration 183, loss = 0.45487616\n",
            "Iteration 184, loss = 0.45347786\n",
            "Iteration 185, loss = 0.45217198\n",
            "Iteration 186, loss = 0.45072928\n",
            "Iteration 187, loss = 0.44948756\n",
            "Iteration 188, loss = 0.44823100\n",
            "Iteration 189, loss = 0.44689400\n",
            "Iteration 190, loss = 0.44552447\n",
            "Iteration 191, loss = 0.44416749\n",
            "Iteration 192, loss = 0.44279576\n",
            "Iteration 193, loss = 0.44129098\n",
            "Iteration 194, loss = 0.43995305\n",
            "Iteration 195, loss = 0.43868029\n",
            "Iteration 196, loss = 0.43735549\n",
            "Iteration 197, loss = 0.43605119\n",
            "Iteration 198, loss = 0.43471808\n",
            "Iteration 199, loss = 0.43346258\n",
            "Iteration 200, loss = 0.43215659\n",
            "Iteration 201, loss = 0.43082248\n",
            "Iteration 202, loss = 0.42952989\n",
            "Iteration 203, loss = 0.42818439\n",
            "Iteration 204, loss = 0.42689274\n",
            "Iteration 205, loss = 0.42560310\n",
            "Iteration 206, loss = 0.42426268\n",
            "Iteration 207, loss = 0.42285699\n",
            "Iteration 208, loss = 0.42144008\n",
            "Iteration 209, loss = 0.42006102\n",
            "Iteration 210, loss = 0.41880735\n",
            "Iteration 211, loss = 0.41738041\n",
            "Iteration 212, loss = 0.41603588\n",
            "Iteration 213, loss = 0.41452978\n",
            "Iteration 214, loss = 0.41308704\n",
            "Iteration 215, loss = 0.41158614\n",
            "Iteration 216, loss = 0.41020609\n",
            "Iteration 217, loss = 0.40884202\n",
            "Iteration 218, loss = 0.40747709\n",
            "Iteration 219, loss = 0.40605431\n",
            "Iteration 220, loss = 0.40470318\n",
            "Iteration 221, loss = 0.40310478\n",
            "Iteration 222, loss = 0.40179713\n",
            "Iteration 223, loss = 0.40034056\n",
            "Iteration 224, loss = 0.39912248\n",
            "Iteration 225, loss = 0.39754933\n",
            "Iteration 226, loss = 0.39612034\n",
            "Iteration 227, loss = 0.39472883\n",
            "Iteration 228, loss = 0.39328323\n",
            "Iteration 229, loss = 0.39187319\n",
            "Iteration 230, loss = 0.39043877\n",
            "Iteration 231, loss = 0.38928139\n",
            "Iteration 232, loss = 0.38774624\n",
            "Iteration 233, loss = 0.38640267\n",
            "Iteration 234, loss = 0.38524958\n",
            "Iteration 235, loss = 0.38377794\n",
            "Iteration 236, loss = 0.38258115\n",
            "Iteration 237, loss = 0.38119399\n",
            "Iteration 238, loss = 0.37991824\n",
            "Iteration 239, loss = 0.37856220\n",
            "Iteration 240, loss = 0.37709310\n",
            "Iteration 241, loss = 0.37582017\n",
            "Iteration 242, loss = 0.37449430\n",
            "Iteration 243, loss = 0.37308498\n",
            "Iteration 244, loss = 0.37177557\n",
            "Iteration 245, loss = 0.37055078\n",
            "Iteration 246, loss = 0.36917676\n",
            "Iteration 247, loss = 0.36798175\n",
            "Iteration 248, loss = 0.36661250\n",
            "Iteration 249, loss = 0.36523803\n",
            "Iteration 250, loss = 0.36391986\n",
            "Iteration 251, loss = 0.36276353\n",
            "Iteration 252, loss = 0.36147468\n",
            "Iteration 253, loss = 0.36010680\n",
            "Iteration 254, loss = 0.35890389\n",
            "Iteration 255, loss = 0.35774061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 256, loss = 0.35658834\n",
            "Iteration 257, loss = 0.35529784\n",
            "Iteration 258, loss = 0.35409883\n",
            "Iteration 259, loss = 0.35284953\n",
            "Iteration 260, loss = 0.35167044\n",
            "Iteration 261, loss = 0.35051632\n",
            "Iteration 262, loss = 0.34917422\n",
            "Iteration 263, loss = 0.34812350\n",
            "Iteration 264, loss = 0.34697576\n",
            "Iteration 265, loss = 0.34601310\n",
            "Iteration 266, loss = 0.34474959\n",
            "Iteration 267, loss = 0.34367589\n",
            "Iteration 268, loss = 0.34245600\n",
            "Iteration 269, loss = 0.34118073\n",
            "Iteration 270, loss = 0.34034757\n",
            "Iteration 271, loss = 0.33902440\n",
            "Iteration 272, loss = 0.33787663\n",
            "Iteration 273, loss = 0.33700449\n",
            "Iteration 274, loss = 0.33589949\n",
            "Iteration 275, loss = 0.33490942\n",
            "Iteration 276, loss = 0.33370144\n",
            "Iteration 277, loss = 0.33269649\n",
            "Iteration 278, loss = 0.33165524\n",
            "Iteration 279, loss = 0.33055179\n",
            "Iteration 280, loss = 0.32958164\n",
            "Iteration 281, loss = 0.32842085\n",
            "Iteration 282, loss = 0.32748742\n",
            "Iteration 283, loss = 0.32642615\n",
            "Iteration 284, loss = 0.32545534\n",
            "Iteration 285, loss = 0.32453773\n",
            "Iteration 286, loss = 0.32362272\n",
            "Iteration 287, loss = 0.32256752\n",
            "Iteration 288, loss = 0.32149468\n",
            "Iteration 289, loss = 0.32054735\n",
            "Iteration 290, loss = 0.31977679\n",
            "Iteration 291, loss = 0.31876151\n",
            "Iteration 292, loss = 0.31756627\n",
            "Iteration 293, loss = 0.31655626\n",
            "Iteration 294, loss = 0.31553278\n",
            "Iteration 295, loss = 0.31464277\n",
            "Iteration 296, loss = 0.31377261\n",
            "Iteration 297, loss = 0.31268934\n",
            "Iteration 298, loss = 0.31178002\n",
            "Iteration 299, loss = 0.31074344\n",
            "Iteration 300, loss = 0.30995469\n",
            "Iteration 301, loss = 0.30911183\n",
            "Iteration 302, loss = 0.30824746\n",
            "Iteration 303, loss = 0.30757260\n",
            "Iteration 304, loss = 0.30647054\n",
            "Iteration 305, loss = 0.30559335\n",
            "Iteration 306, loss = 0.30454570\n",
            "Iteration 307, loss = 0.30385918\n",
            "Iteration 308, loss = 0.30288091\n",
            "Iteration 309, loss = 0.30212957\n",
            "Iteration 310, loss = 0.30114282\n",
            "Iteration 311, loss = 0.30031409\n",
            "Iteration 312, loss = 0.29956964\n",
            "Iteration 313, loss = 0.29868106\n",
            "Iteration 314, loss = 0.29783837\n",
            "Iteration 315, loss = 0.29701131\n",
            "Iteration 316, loss = 0.29617968\n",
            "Iteration 317, loss = 0.29550785\n",
            "Iteration 318, loss = 0.29461053\n",
            "Iteration 319, loss = 0.29381535\n",
            "Iteration 320, loss = 0.29313904\n",
            "Iteration 321, loss = 0.29238564\n",
            "Iteration 322, loss = 0.29189116\n",
            "Iteration 323, loss = 0.29098676\n",
            "Iteration 324, loss = 0.29013296\n",
            "Iteration 325, loss = 0.28947469\n",
            "Iteration 326, loss = 0.28871277\n",
            "Iteration 327, loss = 0.28806872\n",
            "Iteration 328, loss = 0.28729581\n",
            "Iteration 329, loss = 0.28660225\n",
            "Iteration 330, loss = 0.28586740\n",
            "Iteration 331, loss = 0.28526794\n",
            "Iteration 332, loss = 0.28445386\n",
            "Iteration 333, loss = 0.28380054\n",
            "Iteration 334, loss = 0.28300415\n",
            "Iteration 335, loss = 0.28247937\n",
            "Iteration 336, loss = 0.28180818\n",
            "Iteration 337, loss = 0.28109987\n",
            "Iteration 338, loss = 0.28057352\n",
            "Iteration 339, loss = 0.27987899\n",
            "Iteration 340, loss = 0.27916252\n",
            "Iteration 341, loss = 0.27850505\n",
            "Iteration 342, loss = 0.27783384\n",
            "Iteration 343, loss = 0.27707225\n",
            "Iteration 344, loss = 0.27659411\n",
            "Iteration 345, loss = 0.27589922\n",
            "Iteration 346, loss = 0.27527391\n",
            "Iteration 347, loss = 0.27470448\n",
            "Iteration 348, loss = 0.27409301\n",
            "Iteration 349, loss = 0.27339007\n",
            "Iteration 350, loss = 0.27278621\n",
            "Iteration 351, loss = 0.27208969\n",
            "Iteration 352, loss = 0.27135559\n",
            "Iteration 353, loss = 0.27085025\n",
            "Iteration 354, loss = 0.27015038\n",
            "Iteration 355, loss = 0.26988620\n",
            "Iteration 356, loss = 0.26910294\n",
            "Iteration 357, loss = 0.26835735\n",
            "Iteration 358, loss = 0.26775971\n",
            "Iteration 359, loss = 0.26718768\n",
            "Iteration 360, loss = 0.26653788\n",
            "Iteration 361, loss = 0.26592546\n",
            "Iteration 362, loss = 0.26536935\n",
            "Iteration 363, loss = 0.26493901\n",
            "Iteration 364, loss = 0.26437746\n",
            "Iteration 365, loss = 0.26364132\n",
            "Iteration 366, loss = 0.26306989\n",
            "Iteration 367, loss = 0.26243002\n",
            "Iteration 368, loss = 0.26181518\n",
            "Iteration 369, loss = 0.26122107\n",
            "Iteration 370, loss = 0.26052136\n",
            "Iteration 371, loss = 0.25996511\n",
            "Iteration 372, loss = 0.25937296\n",
            "Iteration 373, loss = 0.25879477\n",
            "Iteration 374, loss = 0.25838238\n",
            "Iteration 375, loss = 0.25793842\n",
            "Iteration 376, loss = 0.25717983\n",
            "Iteration 377, loss = 0.25665808\n",
            "Iteration 378, loss = 0.25595651\n",
            "Iteration 379, loss = 0.25539914\n",
            "Iteration 380, loss = 0.25483691\n",
            "Iteration 381, loss = 0.25450905\n",
            "Iteration 382, loss = 0.25376742\n",
            "Iteration 383, loss = 0.25356688\n",
            "Iteration 384, loss = 0.25336618\n",
            "Iteration 385, loss = 0.25216462\n",
            "Iteration 386, loss = 0.25252744\n",
            "Iteration 387, loss = 0.25156806\n",
            "Iteration 388, loss = 0.25071052\n",
            "Iteration 389, loss = 0.25052762\n",
            "Iteration 390, loss = 0.25013283\n",
            "Iteration 391, loss = 0.24986683\n",
            "Iteration 392, loss = 0.24873106\n",
            "Iteration 393, loss = 0.24841851\n",
            "Iteration 394, loss = 0.24769563\n",
            "Iteration 395, loss = 0.24714057\n",
            "Iteration 396, loss = 0.24701760\n",
            "Iteration 397, loss = 0.24644504\n",
            "Iteration 398, loss = 0.24590537\n",
            "Iteration 399, loss = 0.24541014\n",
            "Iteration 400, loss = 0.24490362\n",
            "Iteration 401, loss = 0.24437179\n",
            "Iteration 402, loss = 0.24387574\n",
            "Iteration 403, loss = 0.24332842\n",
            "Iteration 404, loss = 0.24283912\n",
            "Iteration 405, loss = 0.24260887\n",
            "Iteration 406, loss = 0.24182936\n",
            "Iteration 407, loss = 0.24152284\n",
            "Iteration 408, loss = 0.24114915\n",
            "Iteration 409, loss = 0.24052365\n",
            "Iteration 410, loss = 0.24002640\n",
            "Iteration 411, loss = 0.23972631\n",
            "Iteration 412, loss = 0.23931654\n",
            "Iteration 413, loss = 0.23887633\n",
            "Iteration 414, loss = 0.23828874\n",
            "Iteration 415, loss = 0.23770497\n",
            "Iteration 416, loss = 0.23735645\n",
            "Iteration 417, loss = 0.23705405\n",
            "Iteration 418, loss = 0.23708341\n",
            "Iteration 419, loss = 0.23620046\n",
            "Iteration 420, loss = 0.23595375\n",
            "Iteration 421, loss = 0.23550345\n",
            "Iteration 422, loss = 0.23493892\n",
            "Iteration 423, loss = 0.23437983\n",
            "Iteration 424, loss = 0.23416729\n",
            "Iteration 425, loss = 0.23391010\n",
            "Iteration 426, loss = 0.23340835\n",
            "Iteration 427, loss = 0.23276642\n",
            "Iteration 428, loss = 0.23238001\n",
            "Iteration 429, loss = 0.23177204\n",
            "Iteration 430, loss = 0.23153008\n",
            "Iteration 431, loss = 0.23086291\n",
            "Iteration 432, loss = 0.23051676\n",
            "Iteration 433, loss = 0.23064111\n",
            "Iteration 434, loss = 0.22995619\n",
            "Iteration 435, loss = 0.22921875\n",
            "Iteration 436, loss = 0.22862558\n",
            "Iteration 437, loss = 0.22821153\n",
            "Iteration 438, loss = 0.22771166\n",
            "Iteration 439, loss = 0.22728577\n",
            "Iteration 440, loss = 0.22680181\n",
            "Iteration 441, loss = 0.22621992\n",
            "Iteration 442, loss = 0.22579275\n",
            "Iteration 443, loss = 0.22565467\n",
            "Iteration 444, loss = 0.22496357\n",
            "Iteration 445, loss = 0.22435139\n",
            "Iteration 446, loss = 0.22387652\n",
            "Iteration 447, loss = 0.22343787\n",
            "Iteration 448, loss = 0.22284888\n",
            "Iteration 449, loss = 0.22235677\n",
            "Iteration 450, loss = 0.22204873\n",
            "Iteration 451, loss = 0.22160628\n",
            "Iteration 452, loss = 0.22095409\n",
            "Iteration 453, loss = 0.22064517\n",
            "Iteration 454, loss = 0.22002936\n",
            "Iteration 455, loss = 0.21964393\n",
            "Iteration 456, loss = 0.21905354\n",
            "Iteration 457, loss = 0.21865076\n",
            "Iteration 458, loss = 0.21821037\n",
            "Iteration 459, loss = 0.21789811\n",
            "Iteration 460, loss = 0.21723805\n",
            "Iteration 461, loss = 0.21682620\n",
            "Iteration 462, loss = 0.21643802\n",
            "Iteration 463, loss = 0.21589887\n",
            "Iteration 464, loss = 0.21555858\n",
            "Iteration 465, loss = 0.21509404\n",
            "Iteration 466, loss = 0.21471404\n",
            "Iteration 467, loss = 0.21454602\n",
            "Iteration 468, loss = 0.21373546\n",
            "Iteration 469, loss = 0.21339273\n",
            "Iteration 470, loss = 0.21317433\n",
            "Iteration 471, loss = 0.21253646\n",
            "Iteration 472, loss = 0.21231245\n",
            "Iteration 473, loss = 0.21184829\n",
            "Iteration 474, loss = 0.21142588\n",
            "Iteration 475, loss = 0.21102304\n",
            "Iteration 476, loss = 0.21065891\n",
            "Iteration 477, loss = 0.21015451\n",
            "Iteration 478, loss = 0.20976896\n",
            "Iteration 479, loss = 0.20938833\n",
            "Iteration 480, loss = 0.20898356\n",
            "Iteration 481, loss = 0.20846663\n",
            "Iteration 482, loss = 0.20813606\n",
            "Iteration 483, loss = 0.20767709\n",
            "Iteration 484, loss = 0.20739210\n",
            "Iteration 485, loss = 0.20698313\n",
            "Iteration 486, loss = 0.20647164\n",
            "Iteration 487, loss = 0.20616239\n",
            "Iteration 488, loss = 0.20589433\n",
            "Iteration 489, loss = 0.20566470\n",
            "Iteration 490, loss = 0.20521060\n",
            "Iteration 491, loss = 0.20468216\n",
            "Iteration 492, loss = 0.20425103\n",
            "Iteration 493, loss = 0.20387989\n",
            "Iteration 494, loss = 0.20347108\n",
            "Iteration 495, loss = 0.20320069\n",
            "Iteration 496, loss = 0.20286249\n",
            "Iteration 497, loss = 0.20239113\n",
            "Iteration 498, loss = 0.20204583\n",
            "Iteration 499, loss = 0.20173843\n",
            "Iteration 500, loss = 0.20141321\n",
            "Iteration 501, loss = 0.20093810\n",
            "Iteration 502, loss = 0.20059869\n",
            "Iteration 503, loss = 0.20037269\n",
            "Iteration 504, loss = 0.20015439\n",
            "Iteration 505, loss = 0.19976001\n",
            "Iteration 506, loss = 0.19918753\n",
            "Iteration 507, loss = 0.19905040\n",
            "Iteration 508, loss = 0.19859626\n",
            "Iteration 509, loss = 0.19800251\n",
            "Iteration 510, loss = 0.19765400\n",
            "Iteration 511, loss = 0.19738261\n",
            "Iteration 512, loss = 0.19696606\n",
            "Iteration 513, loss = 0.19648573\n",
            "Iteration 514, loss = 0.19624189\n",
            "Iteration 515, loss = 0.19586356\n",
            "Iteration 516, loss = 0.19532326\n",
            "Iteration 517, loss = 0.19499626\n",
            "Iteration 518, loss = 0.19471677\n",
            "Iteration 519, loss = 0.19441544\n",
            "Iteration 520, loss = 0.19364638\n",
            "Iteration 521, loss = 0.19381791\n",
            "Iteration 522, loss = 0.19327640\n",
            "Iteration 523, loss = 0.19263998\n",
            "Iteration 524, loss = 0.19219723\n",
            "Iteration 525, loss = 0.19195377\n",
            "Iteration 526, loss = 0.19147596\n",
            "Iteration 527, loss = 0.19122204\n",
            "Iteration 528, loss = 0.19094714\n",
            "Iteration 529, loss = 0.19058926\n",
            "Iteration 530, loss = 0.19012182\n",
            "Iteration 531, loss = 0.18974934\n",
            "Iteration 532, loss = 0.19017010\n",
            "Iteration 533, loss = 0.18897263\n",
            "Iteration 534, loss = 0.18853651\n",
            "Iteration 535, loss = 0.18813011\n",
            "Iteration 536, loss = 0.18790151\n",
            "Iteration 537, loss = 0.18734035\n",
            "Iteration 538, loss = 0.18702231\n",
            "Iteration 539, loss = 0.18662574\n",
            "Iteration 540, loss = 0.18624872\n",
            "Iteration 541, loss = 0.18582787\n",
            "Iteration 542, loss = 0.18578709\n",
            "Iteration 543, loss = 0.18532065\n",
            "Iteration 544, loss = 0.18491544\n",
            "Iteration 545, loss = 0.18477405\n",
            "Iteration 546, loss = 0.18428942\n",
            "Iteration 547, loss = 0.18376467\n",
            "Iteration 548, loss = 0.18356208\n",
            "Iteration 549, loss = 0.18315256\n",
            "Iteration 550, loss = 0.18276924\n",
            "Iteration 551, loss = 0.18242513\n",
            "Iteration 552, loss = 0.18197133\n",
            "Iteration 553, loss = 0.18176136\n",
            "Iteration 554, loss = 0.18125567\n",
            "Iteration 555, loss = 0.18089135\n",
            "Iteration 556, loss = 0.18078956\n",
            "Iteration 557, loss = 0.18020132\n",
            "Iteration 558, loss = 0.18002636\n",
            "Iteration 559, loss = 0.17961697\n",
            "Iteration 560, loss = 0.17936970\n",
            "Iteration 561, loss = 0.17903702\n",
            "Iteration 562, loss = 0.17854414\n",
            "Iteration 563, loss = 0.17812120\n",
            "Iteration 564, loss = 0.17777010\n",
            "Iteration 565, loss = 0.17740722\n",
            "Iteration 566, loss = 0.17702991\n",
            "Iteration 567, loss = 0.17663698\n",
            "Iteration 568, loss = 0.17610852\n",
            "Iteration 569, loss = 0.17589595\n",
            "Iteration 570, loss = 0.17539076\n",
            "Iteration 571, loss = 0.17509834\n",
            "Iteration 572, loss = 0.17478157\n",
            "Iteration 573, loss = 0.17427942\n",
            "Iteration 574, loss = 0.17391875\n",
            "Iteration 575, loss = 0.17366213\n",
            "Iteration 576, loss = 0.17352106\n",
            "Iteration 577, loss = 0.17292091\n",
            "Iteration 578, loss = 0.17261436\n",
            "Iteration 579, loss = 0.17231417\n",
            "Iteration 580, loss = 0.17206552\n",
            "Iteration 581, loss = 0.17149211\n",
            "Iteration 582, loss = 0.17105324\n",
            "Iteration 583, loss = 0.17065401\n",
            "Iteration 584, loss = 0.17032438\n",
            "Iteration 585, loss = 0.17018816\n",
            "Iteration 586, loss = 0.16975953\n",
            "Iteration 587, loss = 0.16959285\n",
            "Iteration 588, loss = 0.16932747\n",
            "Iteration 589, loss = 0.16876555\n",
            "Iteration 590, loss = 0.16838962\n",
            "Iteration 591, loss = 0.16871798\n",
            "Iteration 592, loss = 0.16779225\n",
            "Iteration 593, loss = 0.16759079\n",
            "Iteration 594, loss = 0.16707591\n",
            "Iteration 595, loss = 0.16693798\n",
            "Iteration 596, loss = 0.16644267\n",
            "Iteration 597, loss = 0.16583459\n",
            "Iteration 598, loss = 0.16592615\n",
            "Iteration 599, loss = 0.16558580\n",
            "Iteration 600, loss = 0.16499912\n",
            "Iteration 601, loss = 0.16433322\n",
            "Iteration 602, loss = 0.16471840\n",
            "Iteration 603, loss = 0.16435330\n",
            "Iteration 604, loss = 0.16405547\n",
            "Iteration 605, loss = 0.16303135\n",
            "Iteration 606, loss = 0.16284938\n",
            "Iteration 607, loss = 0.16301633\n",
            "Iteration 608, loss = 0.16261250\n",
            "Iteration 609, loss = 0.16197120\n",
            "Iteration 610, loss = 0.16170904\n",
            "Iteration 611, loss = 0.16194536\n",
            "Iteration 612, loss = 0.16103436\n",
            "Iteration 613, loss = 0.16069427\n",
            "Iteration 614, loss = 0.16044369\n",
            "Iteration 615, loss = 0.16018434\n",
            "Iteration 616, loss = 0.15969048\n",
            "Iteration 617, loss = 0.15927194\n",
            "Iteration 618, loss = 0.15909402\n",
            "Iteration 619, loss = 0.15884186\n",
            "Iteration 620, loss = 0.15840194\n",
            "Iteration 621, loss = 0.15800909\n",
            "Iteration 622, loss = 0.15777275\n",
            "Iteration 623, loss = 0.15758040\n",
            "Iteration 624, loss = 0.15739044\n",
            "Iteration 625, loss = 0.15684952\n",
            "Iteration 626, loss = 0.15650073\n",
            "Iteration 627, loss = 0.15614184\n",
            "Iteration 628, loss = 0.15617630\n",
            "Iteration 629, loss = 0.15589364\n",
            "Iteration 630, loss = 0.15548007\n",
            "Iteration 631, loss = 0.15473369\n",
            "Iteration 632, loss = 0.15458752\n",
            "Iteration 633, loss = 0.15426954\n",
            "Iteration 634, loss = 0.15382849\n",
            "Iteration 635, loss = 0.15353874\n",
            "Iteration 636, loss = 0.15337395\n",
            "Iteration 637, loss = 0.15288460\n",
            "Iteration 638, loss = 0.15254052\n",
            "Iteration 639, loss = 0.15220673\n",
            "Iteration 640, loss = 0.15184353\n",
            "Iteration 641, loss = 0.15159832\n",
            "Iteration 642, loss = 0.15120670\n",
            "Iteration 643, loss = 0.15093771\n",
            "Iteration 644, loss = 0.15053874\n",
            "Iteration 645, loss = 0.15019601\n",
            "Iteration 646, loss = 0.14983870\n",
            "Iteration 647, loss = 0.14949976\n",
            "Iteration 648, loss = 0.14909379\n",
            "Iteration 649, loss = 0.14886180\n",
            "Iteration 650, loss = 0.14840544\n",
            "Iteration 651, loss = 0.14812941\n",
            "Iteration 652, loss = 0.14768954\n",
            "Iteration 653, loss = 0.14729113\n",
            "Iteration 654, loss = 0.14718279\n",
            "Iteration 655, loss = 0.14653594\n",
            "Iteration 656, loss = 0.14620474\n",
            "Iteration 657, loss = 0.14581765\n",
            "Iteration 658, loss = 0.14539719\n",
            "Iteration 659, loss = 0.14517633\n",
            "Iteration 660, loss = 0.14454887\n",
            "Iteration 661, loss = 0.14442100\n",
            "Iteration 662, loss = 0.14389964\n",
            "Iteration 663, loss = 0.14350886\n",
            "Iteration 664, loss = 0.14312208\n",
            "Iteration 665, loss = 0.14277535\n",
            "Iteration 666, loss = 0.14231714\n",
            "Iteration 667, loss = 0.14200976\n",
            "Iteration 668, loss = 0.14160989\n",
            "Iteration 669, loss = 0.14129602\n",
            "Iteration 670, loss = 0.14097415\n",
            "Iteration 671, loss = 0.14051730\n",
            "Iteration 672, loss = 0.13999519\n",
            "Iteration 673, loss = 0.14039847\n",
            "Iteration 674, loss = 0.13968120\n",
            "Iteration 675, loss = 0.13912447\n",
            "Iteration 676, loss = 0.13879008\n",
            "Iteration 677, loss = 0.13860166\n",
            "Iteration 678, loss = 0.13816915\n",
            "Iteration 679, loss = 0.13827442\n",
            "Iteration 680, loss = 0.13744833\n",
            "Iteration 681, loss = 0.13710583\n",
            "Iteration 682, loss = 0.13683689\n",
            "Iteration 683, loss = 0.13637986\n",
            "Iteration 684, loss = 0.13625519\n",
            "Iteration 685, loss = 0.13601179\n",
            "Iteration 686, loss = 0.13532172\n",
            "Iteration 687, loss = 0.13557333\n",
            "Iteration 688, loss = 0.13496271\n",
            "Iteration 689, loss = 0.13450229\n",
            "Iteration 690, loss = 0.13419986\n",
            "Iteration 691, loss = 0.13380503\n",
            "Iteration 692, loss = 0.13351652\n",
            "Iteration 693, loss = 0.13323341\n",
            "Iteration 694, loss = 0.13297456\n",
            "Iteration 695, loss = 0.13252889\n",
            "Iteration 696, loss = 0.13229885\n",
            "Iteration 697, loss = 0.13197893\n",
            "Iteration 698, loss = 0.13167539\n",
            "Iteration 699, loss = 0.13164526\n",
            "Iteration 700, loss = 0.13114201\n",
            "Iteration 701, loss = 0.13132662\n",
            "Iteration 702, loss = 0.13044175\n",
            "Iteration 703, loss = 0.13014670\n",
            "Iteration 704, loss = 0.13021546\n",
            "Iteration 705, loss = 0.12982451\n",
            "Iteration 706, loss = 0.12930114\n",
            "Iteration 707, loss = 0.12941119\n",
            "Iteration 708, loss = 0.12899797\n",
            "Iteration 709, loss = 0.12841381\n",
            "Iteration 710, loss = 0.12822850\n",
            "Iteration 711, loss = 0.12836672\n",
            "Iteration 712, loss = 0.12801492\n",
            "Iteration 713, loss = 0.12750354\n",
            "Iteration 714, loss = 0.12706306\n",
            "Iteration 715, loss = 0.12684478\n",
            "Iteration 716, loss = 0.12678795\n",
            "Iteration 717, loss = 0.12643690\n",
            "Iteration 718, loss = 0.12609309\n",
            "Iteration 719, loss = 0.12585695\n",
            "Iteration 720, loss = 0.12550189\n",
            "Iteration 721, loss = 0.12516310\n",
            "Iteration 722, loss = 0.12495069\n",
            "Iteration 723, loss = 0.12473456\n",
            "Iteration 724, loss = 0.12457387\n",
            "Iteration 725, loss = 0.12404935\n",
            "Iteration 726, loss = 0.12377930\n",
            "Iteration 727, loss = 0.12364253\n",
            "Iteration 728, loss = 0.12329621\n",
            "Iteration 729, loss = 0.12293618\n",
            "Iteration 730, loss = 0.12271313\n",
            "Iteration 731, loss = 0.12247259\n",
            "Iteration 732, loss = 0.12219946\n",
            "Iteration 733, loss = 0.12188721\n",
            "Iteration 734, loss = 0.12172746\n",
            "Iteration 735, loss = 0.12156818\n",
            "Iteration 736, loss = 0.12103705\n",
            "Iteration 737, loss = 0.12082616\n",
            "Iteration 738, loss = 0.12075190\n",
            "Iteration 739, loss = 0.12023856\n",
            "Iteration 740, loss = 0.12013965\n",
            "Iteration 741, loss = 0.11977371\n",
            "Iteration 742, loss = 0.11940407\n",
            "Iteration 743, loss = 0.11930315\n",
            "Iteration 744, loss = 0.11924544\n",
            "Iteration 745, loss = 0.11886716\n",
            "Iteration 746, loss = 0.11845001\n",
            "Iteration 747, loss = 0.11844013\n",
            "Iteration 748, loss = 0.11823094\n",
            "Iteration 749, loss = 0.11789347\n",
            "Iteration 750, loss = 0.11777220\n",
            "Iteration 751, loss = 0.11720541\n",
            "Iteration 752, loss = 0.11695197\n",
            "Iteration 753, loss = 0.11677785\n",
            "Iteration 754, loss = 0.11666588\n",
            "Iteration 755, loss = 0.11613958\n",
            "Iteration 756, loss = 0.11593586\n",
            "Iteration 757, loss = 0.11576444\n",
            "Iteration 758, loss = 0.11584923\n",
            "Iteration 759, loss = 0.11521930\n",
            "Iteration 760, loss = 0.11478495\n",
            "Iteration 761, loss = 0.11491313\n",
            "Iteration 762, loss = 0.11505131\n",
            "Iteration 763, loss = 0.11462419\n",
            "Iteration 764, loss = 0.11425517\n",
            "Iteration 765, loss = 0.11397611\n",
            "Iteration 766, loss = 0.11367031\n",
            "Iteration 767, loss = 0.11324132\n",
            "Iteration 768, loss = 0.11302404\n",
            "Iteration 769, loss = 0.11280567\n",
            "Iteration 770, loss = 0.11250327\n",
            "Iteration 771, loss = 0.11223444\n",
            "Iteration 772, loss = 0.11197319\n",
            "Iteration 773, loss = 0.11177811\n",
            "Iteration 774, loss = 0.11145428\n",
            "Iteration 775, loss = 0.11140242\n",
            "Iteration 776, loss = 0.11106045\n",
            "Iteration 777, loss = 0.11075356\n",
            "Iteration 778, loss = 0.11050551\n",
            "Iteration 779, loss = 0.11038862\n",
            "Iteration 780, loss = 0.11005112\n",
            "Iteration 781, loss = 0.10980763\n",
            "Iteration 782, loss = 0.10946311\n",
            "Iteration 783, loss = 0.10929153\n",
            "Iteration 784, loss = 0.10901788\n",
            "Iteration 785, loss = 0.10881338\n",
            "Iteration 786, loss = 0.10845449\n",
            "Iteration 787, loss = 0.10827988\n",
            "Iteration 788, loss = 0.10808100\n",
            "Iteration 789, loss = 0.10785800\n",
            "Iteration 790, loss = 0.10756505\n",
            "Iteration 791, loss = 0.10730682\n",
            "Iteration 792, loss = 0.10699244\n",
            "Iteration 793, loss = 0.10673666\n",
            "Iteration 794, loss = 0.10659751\n",
            "Iteration 795, loss = 0.10636612\n",
            "Iteration 796, loss = 0.10618935\n",
            "Iteration 797, loss = 0.10581834\n",
            "Iteration 798, loss = 0.10596392\n",
            "Iteration 799, loss = 0.10562916\n",
            "Iteration 800, loss = 0.10521091\n",
            "Iteration 801, loss = 0.10511293\n",
            "Iteration 802, loss = 0.10495535\n",
            "Iteration 803, loss = 0.10468030\n",
            "Iteration 804, loss = 0.10433093\n",
            "Iteration 805, loss = 0.10421490\n",
            "Iteration 806, loss = 0.10411218\n",
            "Iteration 807, loss = 0.10367634\n",
            "Iteration 808, loss = 0.10328417\n",
            "Iteration 809, loss = 0.10323722\n",
            "Iteration 810, loss = 0.10280358\n",
            "Iteration 811, loss = 0.10279703\n",
            "Iteration 812, loss = 0.10261904\n",
            "Iteration 813, loss = 0.10225653\n",
            "Iteration 814, loss = 0.10212894\n",
            "Iteration 815, loss = 0.10185847\n",
            "Iteration 816, loss = 0.10159415\n",
            "Iteration 817, loss = 0.10145888\n",
            "Iteration 818, loss = 0.10109996\n",
            "Iteration 819, loss = 0.10090767\n",
            "Iteration 820, loss = 0.10093089\n",
            "Iteration 821, loss = 0.10052127\n",
            "Iteration 822, loss = 0.10086613\n",
            "Iteration 823, loss = 0.10011091\n",
            "Iteration 824, loss = 0.09980029\n",
            "Iteration 825, loss = 0.09958626\n",
            "Iteration 826, loss = 0.09947339\n",
            "Iteration 827, loss = 0.09950551\n",
            "Iteration 828, loss = 0.09893106\n",
            "Iteration 829, loss = 0.09875340\n",
            "Iteration 830, loss = 0.09849997\n",
            "Iteration 831, loss = 0.09827002\n",
            "Iteration 832, loss = 0.09809680\n",
            "Iteration 833, loss = 0.09799372\n",
            "Iteration 834, loss = 0.09775696\n",
            "Iteration 835, loss = 0.09740848\n",
            "Iteration 836, loss = 0.09727173\n",
            "Iteration 837, loss = 0.09696826\n",
            "Iteration 838, loss = 0.09683123\n",
            "Iteration 839, loss = 0.09657841\n",
            "Iteration 840, loss = 0.09625637\n",
            "Iteration 841, loss = 0.09599975\n",
            "Iteration 842, loss = 0.09577449\n",
            "Iteration 843, loss = 0.09555392\n",
            "Iteration 844, loss = 0.09537009\n",
            "Iteration 845, loss = 0.09502561\n",
            "Iteration 846, loss = 0.09485659\n",
            "Iteration 847, loss = 0.09471050\n",
            "Iteration 848, loss = 0.09445693\n",
            "Iteration 849, loss = 0.09416688\n",
            "Iteration 850, loss = 0.09387218\n",
            "Iteration 851, loss = 0.09375686\n",
            "Iteration 852, loss = 0.09353281\n",
            "Iteration 853, loss = 0.09325329\n",
            "Iteration 854, loss = 0.09295097\n",
            "Iteration 855, loss = 0.09275925\n",
            "Iteration 856, loss = 0.09262268\n",
            "Iteration 857, loss = 0.09248768\n",
            "Iteration 858, loss = 0.09207893\n",
            "Iteration 859, loss = 0.09175055\n",
            "Iteration 860, loss = 0.09176583\n",
            "Iteration 861, loss = 0.09156366\n",
            "Iteration 862, loss = 0.09125999\n",
            "Iteration 863, loss = 0.09085186\n",
            "Iteration 864, loss = 0.09091332\n",
            "Iteration 865, loss = 0.09045965\n",
            "Iteration 866, loss = 0.09020416\n",
            "Iteration 867, loss = 0.09000205\n",
            "Iteration 868, loss = 0.08982377\n",
            "Iteration 869, loss = 0.08962134\n",
            "Iteration 870, loss = 0.08939504\n",
            "Iteration 871, loss = 0.08908680\n",
            "Iteration 872, loss = 0.08880731\n",
            "Iteration 873, loss = 0.08877641\n",
            "Iteration 874, loss = 0.08853723\n",
            "Iteration 875, loss = 0.08820207\n",
            "Iteration 876, loss = 0.08794037\n",
            "Iteration 877, loss = 0.08777305\n",
            "Iteration 878, loss = 0.08779821\n",
            "Iteration 879, loss = 0.08736741\n",
            "Iteration 880, loss = 0.08707931\n",
            "Iteration 881, loss = 0.08696513\n",
            "Iteration 882, loss = 0.08685531\n",
            "Iteration 883, loss = 0.08650594\n",
            "Iteration 884, loss = 0.08635572\n",
            "Iteration 885, loss = 0.08623729\n",
            "Iteration 886, loss = 0.08587389\n",
            "Iteration 887, loss = 0.08571669\n",
            "Iteration 888, loss = 0.08544485\n",
            "Iteration 889, loss = 0.08531363\n",
            "Iteration 890, loss = 0.08522123\n",
            "Iteration 891, loss = 0.08486240\n",
            "Iteration 892, loss = 0.08472927\n",
            "Iteration 893, loss = 0.08432923\n",
            "Iteration 894, loss = 0.08433190\n",
            "Iteration 895, loss = 0.08405129\n",
            "Iteration 896, loss = 0.08385765\n",
            "Iteration 897, loss = 0.08359698\n",
            "Iteration 898, loss = 0.08339180\n",
            "Iteration 899, loss = 0.08303083\n",
            "Iteration 900, loss = 0.08279368\n",
            "Iteration 901, loss = 0.08271225\n",
            "Iteration 902, loss = 0.08241088\n",
            "Iteration 903, loss = 0.08234986\n",
            "Iteration 904, loss = 0.08192406\n",
            "Iteration 905, loss = 0.08205803\n",
            "Iteration 906, loss = 0.08147598\n",
            "Iteration 907, loss = 0.08131379\n",
            "Iteration 908, loss = 0.08122831\n",
            "Iteration 909, loss = 0.08083595\n",
            "Iteration 910, loss = 0.08054157\n",
            "Iteration 911, loss = 0.08047240\n",
            "Iteration 912, loss = 0.08046336\n",
            "Iteration 913, loss = 0.08008311\n",
            "Iteration 914, loss = 0.07978045\n",
            "Iteration 915, loss = 0.07949088\n",
            "Iteration 916, loss = 0.07941919\n",
            "Iteration 917, loss = 0.07913951\n",
            "Iteration 918, loss = 0.07891510\n",
            "Iteration 919, loss = 0.07852808\n",
            "Iteration 920, loss = 0.07845496\n",
            "Iteration 921, loss = 0.07834135\n",
            "Iteration 922, loss = 0.07809190\n",
            "Iteration 923, loss = 0.07788078\n",
            "Iteration 924, loss = 0.07764275\n",
            "Iteration 925, loss = 0.07735608\n",
            "Iteration 926, loss = 0.07710944\n",
            "Iteration 927, loss = 0.07691289\n",
            "Iteration 928, loss = 0.07678007\n",
            "Iteration 929, loss = 0.07667705\n",
            "Iteration 930, loss = 0.07628227\n",
            "Iteration 931, loss = 0.07611206\n",
            "Iteration 932, loss = 0.07601313\n",
            "Iteration 933, loss = 0.07596733\n",
            "Iteration 934, loss = 0.07546659\n",
            "Iteration 935, loss = 0.07563381\n",
            "Iteration 936, loss = 0.07536463\n",
            "Iteration 937, loss = 0.07493348\n",
            "Iteration 938, loss = 0.07491292\n",
            "Iteration 939, loss = 0.07467933\n",
            "Iteration 940, loss = 0.07447187\n",
            "Iteration 941, loss = 0.07432615\n",
            "Iteration 942, loss = 0.07413856\n",
            "Iteration 943, loss = 0.07386810\n",
            "Iteration 944, loss = 0.07389924\n",
            "Iteration 945, loss = 0.07339372\n",
            "Iteration 946, loss = 0.07327236\n",
            "Iteration 947, loss = 0.07308204\n",
            "Iteration 948, loss = 0.07286970\n",
            "Iteration 949, loss = 0.07267438\n",
            "Iteration 950, loss = 0.07250342\n",
            "Iteration 951, loss = 0.07234265\n",
            "Iteration 952, loss = 0.07219459\n",
            "Iteration 953, loss = 0.07236680\n",
            "Iteration 954, loss = 0.07191974\n",
            "Iteration 955, loss = 0.07175952\n",
            "Iteration 956, loss = 0.07146890\n",
            "Iteration 957, loss = 0.07146116\n",
            "Iteration 958, loss = 0.07118217\n",
            "Iteration 959, loss = 0.07114451\n",
            "Iteration 960, loss = 0.07081028\n",
            "Iteration 961, loss = 0.07058525\n",
            "Iteration 962, loss = 0.07038913\n",
            "Iteration 963, loss = 0.07023810\n",
            "Iteration 964, loss = 0.07025172\n",
            "Iteration 965, loss = 0.07000073\n",
            "Iteration 966, loss = 0.06992553\n",
            "Iteration 967, loss = 0.06972234\n",
            "Iteration 968, loss = 0.06953030\n",
            "Iteration 969, loss = 0.06929530\n",
            "Iteration 970, loss = 0.06908703\n",
            "Iteration 971, loss = 0.06899997\n",
            "Iteration 972, loss = 0.06880506\n",
            "Iteration 973, loss = 0.06858362\n",
            "Iteration 974, loss = 0.06844067\n",
            "Iteration 975, loss = 0.06843386\n",
            "Iteration 976, loss = 0.06823474\n",
            "Iteration 977, loss = 0.06832170\n",
            "Iteration 978, loss = 0.06816898\n",
            "Iteration 979, loss = 0.06770990\n",
            "Iteration 980, loss = 0.06786044\n",
            "Iteration 981, loss = 0.06769575\n",
            "Iteration 982, loss = 0.06732898\n",
            "Iteration 983, loss = 0.06712849\n",
            "Iteration 984, loss = 0.06695108\n",
            "Iteration 985, loss = 0.06687495\n",
            "Iteration 986, loss = 0.06674036\n",
            "Iteration 987, loss = 0.06659259\n",
            "Iteration 988, loss = 0.06652913\n",
            "Iteration 989, loss = 0.06628506\n",
            "Iteration 990, loss = 0.06617459\n",
            "Iteration 991, loss = 0.06613556\n",
            "Iteration 992, loss = 0.06576154\n",
            "Iteration 993, loss = 0.06558337\n",
            "Iteration 994, loss = 0.06543366\n",
            "Iteration 995, loss = 0.06543870\n",
            "Iteration 996, loss = 0.06534315\n",
            "Iteration 997, loss = 0.06515011\n",
            "Iteration 998, loss = 0.06508328\n",
            "Iteration 999, loss = 0.06532493\n",
            "Iteration 1000, loss = 0.06484457\n",
            "Acurácia: 0.6666666666666666\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.81      0.55      0.66        47\n",
            "   recurrence-events       0.57      0.82      0.67        34\n",
            "\n",
            "            accuracy                           0.67        81\n",
            "           macro avg       0.69      0.69      0.67        81\n",
            "        weighted avg       0.71      0.67      0.67        81\n",
            "\n",
            "[[26 21]\n",
            " [ 6 28]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Avaliando topologia: (20, 10)\n",
            "Iteration 1, loss = 0.70436254\n",
            "Iteration 2, loss = 0.69689538\n",
            "Iteration 3, loss = 0.69097477\n",
            "Iteration 4, loss = 0.68449368\n",
            "Iteration 5, loss = 0.67930924\n",
            "Iteration 6, loss = 0.67314548\n",
            "Iteration 7, loss = 0.66803460\n",
            "Iteration 8, loss = 0.66337107\n",
            "Iteration 9, loss = 0.65832578\n",
            "Iteration 10, loss = 0.65427347\n",
            "Iteration 11, loss = 0.65033085\n",
            "Iteration 12, loss = 0.64617882\n",
            "Iteration 13, loss = 0.64239190\n",
            "Iteration 14, loss = 0.63894234\n",
            "Iteration 15, loss = 0.63551258\n",
            "Iteration 16, loss = 0.63208505\n",
            "Iteration 17, loss = 0.62895523\n",
            "Iteration 18, loss = 0.62548421\n",
            "Iteration 19, loss = 0.62228711\n",
            "Iteration 20, loss = 0.61886206\n",
            "Iteration 21, loss = 0.61551007\n",
            "Iteration 22, loss = 0.61209406\n",
            "Iteration 23, loss = 0.60854495\n",
            "Iteration 24, loss = 0.60518619\n",
            "Iteration 25, loss = 0.60170479\n",
            "Iteration 26, loss = 0.59829495\n",
            "Iteration 27, loss = 0.59508638\n",
            "Iteration 28, loss = 0.59163760\n",
            "Iteration 29, loss = 0.58840781\n",
            "Iteration 30, loss = 0.58499547\n",
            "Iteration 31, loss = 0.58213504\n",
            "Iteration 32, loss = 0.57919823\n",
            "Iteration 33, loss = 0.57608220\n",
            "Iteration 34, loss = 0.57286240\n",
            "Iteration 35, loss = 0.56974939\n",
            "Iteration 36, loss = 0.56686577\n",
            "Iteration 37, loss = 0.56391620\n",
            "Iteration 38, loss = 0.56115472\n",
            "Iteration 39, loss = 0.55826106\n",
            "Iteration 40, loss = 0.55555063\n",
            "Iteration 41, loss = 0.55295443\n",
            "Iteration 42, loss = 0.55024275\n",
            "Iteration 43, loss = 0.54761174\n",
            "Iteration 44, loss = 0.54499522\n",
            "Iteration 45, loss = 0.54243066\n",
            "Iteration 46, loss = 0.53997937\n",
            "Iteration 47, loss = 0.53745717\n",
            "Iteration 48, loss = 0.53488859\n",
            "Iteration 49, loss = 0.53251699\n",
            "Iteration 50, loss = 0.53003596\n",
            "Iteration 51, loss = 0.52789946\n",
            "Iteration 52, loss = 0.52556203\n",
            "Iteration 53, loss = 0.52321114\n",
            "Iteration 54, loss = 0.52095598\n",
            "Iteration 55, loss = 0.51890408\n",
            "Iteration 56, loss = 0.51668751\n",
            "Iteration 57, loss = 0.51477145\n",
            "Iteration 58, loss = 0.51242789\n",
            "Iteration 59, loss = 0.51038054\n",
            "Iteration 60, loss = 0.50831851\n",
            "Iteration 61, loss = 0.50624890\n",
            "Iteration 62, loss = 0.50422004\n",
            "Iteration 63, loss = 0.50233048\n",
            "Iteration 64, loss = 0.50033345\n",
            "Iteration 65, loss = 0.49846803\n",
            "Iteration 66, loss = 0.49643514\n",
            "Iteration 67, loss = 0.49443157\n",
            "Iteration 68, loss = 0.49258081\n",
            "Iteration 69, loss = 0.49093175\n",
            "Iteration 70, loss = 0.48853846\n",
            "Iteration 71, loss = 0.48660567\n",
            "Iteration 72, loss = 0.48478440\n",
            "Iteration 73, loss = 0.48291283\n",
            "Iteration 74, loss = 0.48105480\n",
            "Iteration 75, loss = 0.47926042\n",
            "Iteration 76, loss = 0.47728560\n",
            "Iteration 77, loss = 0.47537928\n",
            "Iteration 78, loss = 0.47321533\n",
            "Iteration 79, loss = 0.47150961\n",
            "Iteration 80, loss = 0.47022443\n",
            "Iteration 81, loss = 0.46799918\n",
            "Iteration 82, loss = 0.46595152\n",
            "Iteration 83, loss = 0.46436273\n",
            "Iteration 84, loss = 0.46200770\n",
            "Iteration 85, loss = 0.46017805\n",
            "Iteration 86, loss = 0.45835988\n",
            "Iteration 87, loss = 0.45649112\n",
            "Iteration 88, loss = 0.45457733\n",
            "Iteration 89, loss = 0.45294658\n",
            "Iteration 90, loss = 0.45086044\n",
            "Iteration 91, loss = 0.44895591\n",
            "Iteration 92, loss = 0.44715308\n",
            "Iteration 93, loss = 0.44546224\n",
            "Iteration 94, loss = 0.44360186\n",
            "Iteration 95, loss = 0.44186523\n",
            "Iteration 96, loss = 0.44012451\n",
            "Iteration 97, loss = 0.43825407\n",
            "Iteration 98, loss = 0.43649079\n",
            "Iteration 99, loss = 0.43469604\n",
            "Iteration 100, loss = 0.43281732\n",
            "Iteration 101, loss = 0.43123691\n",
            "Iteration 102, loss = 0.42944469\n",
            "Iteration 103, loss = 0.42759317\n",
            "Iteration 104, loss = 0.42577697\n",
            "Iteration 105, loss = 0.42402979\n",
            "Iteration 106, loss = 0.42224194\n",
            "Iteration 107, loss = 0.42041405\n",
            "Iteration 108, loss = 0.41872030\n",
            "Iteration 109, loss = 0.41690956\n",
            "Iteration 110, loss = 0.41506804\n",
            "Iteration 111, loss = 0.41332407\n",
            "Iteration 112, loss = 0.41162627\n",
            "Iteration 113, loss = 0.40968242\n",
            "Iteration 114, loss = 0.40792442\n",
            "Iteration 115, loss = 0.40606156\n",
            "Iteration 116, loss = 0.40428786\n",
            "Iteration 117, loss = 0.40234983\n",
            "Iteration 118, loss = 0.40050574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 119, loss = 0.39848128\n",
            "Iteration 120, loss = 0.39666010\n",
            "Iteration 121, loss = 0.39472911\n",
            "Iteration 122, loss = 0.39282888\n",
            "Iteration 123, loss = 0.39068969\n",
            "Iteration 124, loss = 0.38848855\n",
            "Iteration 125, loss = 0.38641040\n",
            "Iteration 126, loss = 0.38437923\n",
            "Iteration 127, loss = 0.38224743\n",
            "Iteration 128, loss = 0.38028960\n",
            "Iteration 129, loss = 0.37825269\n",
            "Iteration 130, loss = 0.37602649\n",
            "Iteration 131, loss = 0.37416376\n",
            "Iteration 132, loss = 0.37191379\n",
            "Iteration 133, loss = 0.36994773\n",
            "Iteration 134, loss = 0.36790274\n",
            "Iteration 135, loss = 0.36574506\n",
            "Iteration 136, loss = 0.36379416\n",
            "Iteration 137, loss = 0.36151464\n",
            "Iteration 138, loss = 0.35931970\n",
            "Iteration 139, loss = 0.35755650\n",
            "Iteration 140, loss = 0.35533815\n",
            "Iteration 141, loss = 0.35318417\n",
            "Iteration 142, loss = 0.35088092\n",
            "Iteration 143, loss = 0.34877155\n",
            "Iteration 144, loss = 0.34677567\n",
            "Iteration 145, loss = 0.34473635\n",
            "Iteration 146, loss = 0.34256713\n",
            "Iteration 147, loss = 0.34035384\n",
            "Iteration 148, loss = 0.33802748\n",
            "Iteration 149, loss = 0.33601291\n",
            "Iteration 150, loss = 0.33387535\n",
            "Iteration 151, loss = 0.33209211\n",
            "Iteration 152, loss = 0.32998810\n",
            "Iteration 153, loss = 0.32750496\n",
            "Iteration 154, loss = 0.32562537\n",
            "Iteration 155, loss = 0.32364819\n",
            "Iteration 156, loss = 0.32102678\n",
            "Iteration 157, loss = 0.31934240\n",
            "Iteration 158, loss = 0.31811344\n",
            "Iteration 159, loss = 0.31523963\n",
            "Iteration 160, loss = 0.31340420\n",
            "Iteration 161, loss = 0.31113297\n",
            "Iteration 162, loss = 0.30914886\n",
            "Iteration 163, loss = 0.30792574\n",
            "Iteration 164, loss = 0.30581374\n",
            "Iteration 165, loss = 0.30353779\n",
            "Iteration 166, loss = 0.30139981\n",
            "Iteration 167, loss = 0.29971029\n",
            "Iteration 168, loss = 0.29790151\n",
            "Iteration 169, loss = 0.29617985\n",
            "Iteration 170, loss = 0.29380660\n",
            "Iteration 171, loss = 0.29224678\n",
            "Iteration 172, loss = 0.29002628\n",
            "Iteration 173, loss = 0.28815082\n",
            "Iteration 174, loss = 0.28631901\n",
            "Iteration 175, loss = 0.28463735\n",
            "Iteration 176, loss = 0.28255471\n",
            "Iteration 177, loss = 0.28070350\n",
            "Iteration 178, loss = 0.27894808\n",
            "Iteration 179, loss = 0.27715835\n",
            "Iteration 180, loss = 0.27532272\n",
            "Iteration 181, loss = 0.27344647\n",
            "Iteration 182, loss = 0.27169831\n",
            "Iteration 183, loss = 0.26991907\n",
            "Iteration 184, loss = 0.26790290\n",
            "Iteration 185, loss = 0.26618522\n",
            "Iteration 186, loss = 0.26468679\n",
            "Iteration 187, loss = 0.26264340\n",
            "Iteration 188, loss = 0.26103330\n",
            "Iteration 189, loss = 0.25932689\n",
            "Iteration 190, loss = 0.25733390\n",
            "Iteration 191, loss = 0.25564144\n",
            "Iteration 192, loss = 0.25404385\n",
            "Iteration 193, loss = 0.25249397\n",
            "Iteration 194, loss = 0.25058657\n",
            "Iteration 195, loss = 0.24890038\n",
            "Iteration 196, loss = 0.24730899\n",
            "Iteration 197, loss = 0.24569401\n",
            "Iteration 198, loss = 0.24406313\n",
            "Iteration 199, loss = 0.24286688\n",
            "Iteration 200, loss = 0.24079109\n",
            "Iteration 201, loss = 0.23925594\n",
            "Iteration 202, loss = 0.23796256\n",
            "Iteration 203, loss = 0.23632460\n",
            "Iteration 204, loss = 0.23483975\n",
            "Iteration 205, loss = 0.23309751\n",
            "Iteration 206, loss = 0.23160019\n",
            "Iteration 207, loss = 0.23005379\n",
            "Iteration 208, loss = 0.22864760\n",
            "Iteration 209, loss = 0.22698654\n",
            "Iteration 210, loss = 0.22572317\n",
            "Iteration 211, loss = 0.22400115\n",
            "Iteration 212, loss = 0.22248247\n",
            "Iteration 213, loss = 0.22102737\n",
            "Iteration 214, loss = 0.21971427\n",
            "Iteration 215, loss = 0.21821395\n",
            "Iteration 216, loss = 0.21683451\n",
            "Iteration 217, loss = 0.21564172\n",
            "Iteration 218, loss = 0.21415553\n",
            "Iteration 219, loss = 0.21276260\n",
            "Iteration 220, loss = 0.21140254\n",
            "Iteration 221, loss = 0.20999580\n",
            "Iteration 222, loss = 0.20887857\n",
            "Iteration 223, loss = 0.20734493\n",
            "Iteration 224, loss = 0.20597051\n",
            "Iteration 225, loss = 0.20490408\n",
            "Iteration 226, loss = 0.20362669\n",
            "Iteration 227, loss = 0.20218344\n",
            "Iteration 228, loss = 0.20086111\n",
            "Iteration 229, loss = 0.19956178\n",
            "Iteration 230, loss = 0.19816109\n",
            "Iteration 231, loss = 0.19706053\n",
            "Iteration 232, loss = 0.19577565\n",
            "Iteration 233, loss = 0.19458304\n",
            "Iteration 234, loss = 0.19329600\n",
            "Iteration 235, loss = 0.19200648\n",
            "Iteration 236, loss = 0.19086085\n",
            "Iteration 237, loss = 0.18960228\n",
            "Iteration 238, loss = 0.18855023\n",
            "Iteration 239, loss = 0.18719381\n",
            "Iteration 240, loss = 0.18605120\n",
            "Iteration 241, loss = 0.18482496\n",
            "Iteration 242, loss = 0.18381511\n",
            "Iteration 243, loss = 0.18280716\n",
            "Iteration 244, loss = 0.18130416\n",
            "Iteration 245, loss = 0.18027036\n",
            "Iteration 246, loss = 0.17918251\n",
            "Iteration 247, loss = 0.17799758\n",
            "Iteration 248, loss = 0.17725000\n",
            "Iteration 249, loss = 0.17613238\n",
            "Iteration 250, loss = 0.17503635\n",
            "Iteration 251, loss = 0.17387732\n",
            "Iteration 252, loss = 0.17316972\n",
            "Iteration 253, loss = 0.17190766\n",
            "Iteration 254, loss = 0.17084218\n",
            "Iteration 255, loss = 0.16977326\n",
            "Iteration 256, loss = 0.16868940\n",
            "Iteration 257, loss = 0.16824705\n",
            "Iteration 258, loss = 0.16682028\n",
            "Iteration 259, loss = 0.16563506\n",
            "Iteration 260, loss = 0.16456109\n",
            "Iteration 261, loss = 0.16403344\n",
            "Iteration 262, loss = 0.16322370\n",
            "Iteration 263, loss = 0.16181493\n",
            "Iteration 264, loss = 0.16117152\n",
            "Iteration 265, loss = 0.15998037\n",
            "Iteration 266, loss = 0.15886384\n",
            "Iteration 267, loss = 0.15792907\n",
            "Iteration 268, loss = 0.15700071\n",
            "Iteration 269, loss = 0.15597112\n",
            "Iteration 270, loss = 0.15512430\n",
            "Iteration 271, loss = 0.15423249\n",
            "Iteration 272, loss = 0.15324245\n",
            "Iteration 273, loss = 0.15236148\n",
            "Iteration 274, loss = 0.15146353\n",
            "Iteration 275, loss = 0.15067564\n",
            "Iteration 276, loss = 0.14980939\n",
            "Iteration 277, loss = 0.14864463\n",
            "Iteration 278, loss = 0.14784265\n",
            "Iteration 279, loss = 0.14700036\n",
            "Iteration 280, loss = 0.14618306\n",
            "Iteration 281, loss = 0.14521770\n",
            "Iteration 282, loss = 0.14443395\n",
            "Iteration 283, loss = 0.14355083\n",
            "Iteration 284, loss = 0.14275906\n",
            "Iteration 285, loss = 0.14198796\n",
            "Iteration 286, loss = 0.14120814\n",
            "Iteration 287, loss = 0.14058777\n",
            "Iteration 288, loss = 0.13954332\n",
            "Iteration 289, loss = 0.13876572\n",
            "Iteration 290, loss = 0.13802688\n",
            "Iteration 291, loss = 0.13746629\n",
            "Iteration 292, loss = 0.13652778\n",
            "Iteration 293, loss = 0.13570299\n",
            "Iteration 294, loss = 0.13518546\n",
            "Iteration 295, loss = 0.13431087\n",
            "Iteration 296, loss = 0.13342021\n",
            "Iteration 297, loss = 0.13273010\n",
            "Iteration 298, loss = 0.13208914\n",
            "Iteration 299, loss = 0.13122737\n",
            "Iteration 300, loss = 0.13042055\n",
            "Iteration 301, loss = 0.12990286\n",
            "Iteration 302, loss = 0.12941041\n",
            "Iteration 303, loss = 0.12864792\n",
            "Iteration 304, loss = 0.12761128\n",
            "Iteration 305, loss = 0.12695169\n",
            "Iteration 306, loss = 0.12618515\n",
            "Iteration 307, loss = 0.12557074\n",
            "Iteration 308, loss = 0.12501589\n",
            "Iteration 309, loss = 0.12429977\n",
            "Iteration 310, loss = 0.12346329\n",
            "Iteration 311, loss = 0.12311228\n",
            "Iteration 312, loss = 0.12192223\n",
            "Iteration 313, loss = 0.12149289\n",
            "Iteration 314, loss = 0.12100313\n",
            "Iteration 315, loss = 0.12008858\n",
            "Iteration 316, loss = 0.11939726\n",
            "Iteration 317, loss = 0.11880174\n",
            "Iteration 318, loss = 0.11837033\n",
            "Iteration 319, loss = 0.11765345\n",
            "Iteration 320, loss = 0.11737875\n",
            "Iteration 321, loss = 0.11629454\n",
            "Iteration 322, loss = 0.11566707\n",
            "Iteration 323, loss = 0.11534820\n",
            "Iteration 324, loss = 0.11454720\n",
            "Iteration 325, loss = 0.11374237\n",
            "Iteration 326, loss = 0.11361802\n",
            "Iteration 327, loss = 0.11282957\n",
            "Iteration 328, loss = 0.11224817\n",
            "Iteration 329, loss = 0.11133068\n",
            "Iteration 330, loss = 0.11064541\n",
            "Iteration 331, loss = 0.11009037\n",
            "Iteration 332, loss = 0.10971987\n",
            "Iteration 333, loss = 0.10897190\n",
            "Iteration 334, loss = 0.10834564\n",
            "Iteration 335, loss = 0.10792521\n",
            "Iteration 336, loss = 0.10753542\n",
            "Iteration 337, loss = 0.10687192\n",
            "Iteration 338, loss = 0.10623064\n",
            "Iteration 339, loss = 0.10575642\n",
            "Iteration 340, loss = 0.10508410\n",
            "Iteration 341, loss = 0.10460306\n",
            "Iteration 342, loss = 0.10398483\n",
            "Iteration 343, loss = 0.10367312\n",
            "Iteration 344, loss = 0.10303197\n",
            "Iteration 345, loss = 0.10271963\n",
            "Iteration 346, loss = 0.10196133\n",
            "Iteration 347, loss = 0.10132915\n",
            "Iteration 348, loss = 0.10085622\n",
            "Iteration 349, loss = 0.10046252\n",
            "Iteration 350, loss = 0.10010192\n",
            "Iteration 351, loss = 0.09931164\n",
            "Iteration 352, loss = 0.09891617\n",
            "Iteration 353, loss = 0.09822794\n",
            "Iteration 354, loss = 0.09785442\n",
            "Iteration 355, loss = 0.09761762\n",
            "Iteration 356, loss = 0.09694583\n",
            "Iteration 357, loss = 0.09648767\n",
            "Iteration 358, loss = 0.09626533\n",
            "Iteration 359, loss = 0.09589267\n",
            "Iteration 360, loss = 0.09509776\n",
            "Iteration 361, loss = 0.09449746\n",
            "Iteration 362, loss = 0.09412032\n",
            "Iteration 363, loss = 0.09370608\n",
            "Iteration 364, loss = 0.09307933\n",
            "Iteration 365, loss = 0.09295408\n",
            "Iteration 366, loss = 0.09232140\n",
            "Iteration 367, loss = 0.09193921\n",
            "Iteration 368, loss = 0.09120718\n",
            "Iteration 369, loss = 0.09078418\n",
            "Iteration 370, loss = 0.09032923\n",
            "Iteration 371, loss = 0.08985904\n",
            "Iteration 372, loss = 0.08951437\n",
            "Iteration 373, loss = 0.08902778\n",
            "Iteration 374, loss = 0.08848636\n",
            "Iteration 375, loss = 0.08793978\n",
            "Iteration 376, loss = 0.08753717\n",
            "Iteration 377, loss = 0.08721468\n",
            "Iteration 378, loss = 0.08661602\n",
            "Iteration 379, loss = 0.08620672\n",
            "Iteration 380, loss = 0.08578867\n",
            "Iteration 381, loss = 0.08531452\n",
            "Iteration 382, loss = 0.08478612\n",
            "Iteration 383, loss = 0.08452529\n",
            "Iteration 384, loss = 0.08411799\n",
            "Iteration 385, loss = 0.08342376\n",
            "Iteration 386, loss = 0.08343685\n",
            "Iteration 387, loss = 0.08271397\n",
            "Iteration 388, loss = 0.08209045\n",
            "Iteration 389, loss = 0.08183370\n",
            "Iteration 390, loss = 0.08112084\n",
            "Iteration 391, loss = 0.08089209\n",
            "Iteration 392, loss = 0.08066751\n",
            "Iteration 393, loss = 0.07994211\n",
            "Iteration 394, loss = 0.07946974\n",
            "Iteration 395, loss = 0.07897776\n",
            "Iteration 396, loss = 0.07922705\n",
            "Iteration 397, loss = 0.07818394\n",
            "Iteration 398, loss = 0.07827100\n",
            "Iteration 399, loss = 0.07788563\n",
            "Iteration 400, loss = 0.07726473\n",
            "Iteration 401, loss = 0.07654020\n",
            "Iteration 402, loss = 0.07665968\n",
            "Iteration 403, loss = 0.07572026\n",
            "Iteration 404, loss = 0.07619300\n",
            "Iteration 405, loss = 0.07510752\n",
            "Iteration 406, loss = 0.07458396\n",
            "Iteration 407, loss = 0.07459450\n",
            "Iteration 408, loss = 0.07411424\n",
            "Iteration 409, loss = 0.07355035\n",
            "Iteration 410, loss = 0.07341754\n",
            "Iteration 411, loss = 0.07329974\n",
            "Iteration 412, loss = 0.07257501\n",
            "Iteration 413, loss = 0.07208495\n",
            "Iteration 414, loss = 0.07195766\n",
            "Iteration 415, loss = 0.07161296\n",
            "Iteration 416, loss = 0.07158190\n",
            "Iteration 417, loss = 0.07122625\n",
            "Iteration 418, loss = 0.07069256\n",
            "Iteration 419, loss = 0.07043048\n",
            "Iteration 420, loss = 0.07012532\n",
            "Iteration 421, loss = 0.06974660\n",
            "Iteration 422, loss = 0.06922083\n",
            "Iteration 423, loss = 0.06924624\n",
            "Iteration 424, loss = 0.06880091\n",
            "Iteration 425, loss = 0.06885179\n",
            "Iteration 426, loss = 0.06831779\n",
            "Iteration 427, loss = 0.06781628\n",
            "Iteration 428, loss = 0.06747947\n",
            "Iteration 429, loss = 0.06727406\n",
            "Iteration 430, loss = 0.06688465\n",
            "Iteration 431, loss = 0.06657770\n",
            "Iteration 432, loss = 0.06619462\n",
            "Iteration 433, loss = 0.06610533\n",
            "Iteration 434, loss = 0.06576582\n",
            "Iteration 435, loss = 0.06550215\n",
            "Iteration 436, loss = 0.06519361\n",
            "Iteration 437, loss = 0.06481772\n",
            "Iteration 438, loss = 0.06461925\n",
            "Iteration 439, loss = 0.06461102\n",
            "Iteration 440, loss = 0.06428412\n",
            "Iteration 441, loss = 0.06384098\n",
            "Iteration 442, loss = 0.06371796\n",
            "Iteration 443, loss = 0.06332466\n",
            "Iteration 444, loss = 0.06297693\n",
            "Iteration 445, loss = 0.06295977\n",
            "Iteration 446, loss = 0.06238291\n",
            "Iteration 447, loss = 0.06265179\n",
            "Iteration 448, loss = 0.06211046\n",
            "Iteration 449, loss = 0.06166951\n",
            "Iteration 450, loss = 0.06157611\n",
            "Iteration 451, loss = 0.06132869\n",
            "Iteration 452, loss = 0.06096601\n",
            "Iteration 453, loss = 0.06077125\n",
            "Iteration 454, loss = 0.06063021\n",
            "Iteration 455, loss = 0.06025113\n",
            "Iteration 456, loss = 0.05996416\n",
            "Iteration 457, loss = 0.06005206\n",
            "Iteration 458, loss = 0.06000987\n",
            "Iteration 459, loss = 0.05928121\n",
            "Iteration 460, loss = 0.05910439\n",
            "Iteration 461, loss = 0.05876460\n",
            "Iteration 462, loss = 0.05865358\n",
            "Iteration 463, loss = 0.05836697\n",
            "Iteration 464, loss = 0.05815742\n",
            "Iteration 465, loss = 0.05795785\n",
            "Iteration 466, loss = 0.05797466\n",
            "Iteration 467, loss = 0.05739343\n",
            "Iteration 468, loss = 0.05720592\n",
            "Iteration 469, loss = 0.05723427\n",
            "Iteration 470, loss = 0.05691503\n",
            "Iteration 471, loss = 0.05661032\n",
            "Iteration 472, loss = 0.05631985\n",
            "Iteration 473, loss = 0.05614712\n",
            "Iteration 474, loss = 0.05609188\n",
            "Iteration 475, loss = 0.05572857\n",
            "Iteration 476, loss = 0.05570018\n",
            "Iteration 477, loss = 0.05537106\n",
            "Iteration 478, loss = 0.05514997\n",
            "Iteration 479, loss = 0.05498900\n",
            "Iteration 480, loss = 0.05484661\n",
            "Iteration 481, loss = 0.05446670\n",
            "Iteration 482, loss = 0.05431846\n",
            "Iteration 483, loss = 0.05418100\n",
            "Iteration 484, loss = 0.05395465\n",
            "Iteration 485, loss = 0.05367238\n",
            "Iteration 486, loss = 0.05368886\n",
            "Iteration 487, loss = 0.05343524\n",
            "Iteration 488, loss = 0.05322081\n",
            "Iteration 489, loss = 0.05326091\n",
            "Iteration 490, loss = 0.05283385\n",
            "Iteration 491, loss = 0.05265683\n",
            "Iteration 492, loss = 0.05280200\n",
            "Iteration 493, loss = 0.05238925\n",
            "Iteration 494, loss = 0.05217355\n",
            "Iteration 495, loss = 0.05187138\n",
            "Iteration 496, loss = 0.05182757\n",
            "Iteration 497, loss = 0.05145046\n",
            "Iteration 498, loss = 0.05125367\n",
            "Iteration 499, loss = 0.05112895\n",
            "Iteration 500, loss = 0.05118464\n",
            "Iteration 501, loss = 0.05084301\n",
            "Iteration 502, loss = 0.05097240\n",
            "Iteration 503, loss = 0.05054377\n",
            "Iteration 504, loss = 0.05079285\n",
            "Iteration 505, loss = 0.05011107\n",
            "Iteration 506, loss = 0.05013284\n",
            "Iteration 507, loss = 0.04976501\n",
            "Iteration 508, loss = 0.04953658\n",
            "Iteration 509, loss = 0.04954046\n",
            "Iteration 510, loss = 0.04978299\n",
            "Iteration 511, loss = 0.04912602\n",
            "Iteration 512, loss = 0.04903848\n",
            "Iteration 513, loss = 0.04910971\n",
            "Iteration 514, loss = 0.04874284\n",
            "Iteration 515, loss = 0.04860635\n",
            "Iteration 516, loss = 0.04855493\n",
            "Iteration 517, loss = 0.04819652\n",
            "Iteration 518, loss = 0.04788813\n",
            "Iteration 519, loss = 0.04797517\n",
            "Iteration 520, loss = 0.04810533\n",
            "Iteration 521, loss = 0.04773449\n",
            "Iteration 522, loss = 0.04751085\n",
            "Iteration 523, loss = 0.04740292\n",
            "Iteration 524, loss = 0.04716671\n",
            "Iteration 525, loss = 0.04683789\n",
            "Iteration 526, loss = 0.04681977\n",
            "Iteration 527, loss = 0.04656016\n",
            "Iteration 528, loss = 0.04648599\n",
            "Iteration 529, loss = 0.04622410\n",
            "Iteration 530, loss = 0.04628298\n",
            "Iteration 531, loss = 0.04592253\n",
            "Iteration 532, loss = 0.04606109\n",
            "Iteration 533, loss = 0.04591475\n",
            "Iteration 534, loss = 0.04606151\n",
            "Iteration 535, loss = 0.04575916\n",
            "Iteration 536, loss = 0.04560372\n",
            "Iteration 537, loss = 0.04546995\n",
            "Iteration 538, loss = 0.04507669\n",
            "Iteration 539, loss = 0.04501542\n",
            "Iteration 540, loss = 0.04484388\n",
            "Iteration 541, loss = 0.04461817\n",
            "Iteration 542, loss = 0.04463739\n",
            "Iteration 543, loss = 0.04435530\n",
            "Iteration 544, loss = 0.04424053\n",
            "Iteration 545, loss = 0.04430625\n",
            "Iteration 546, loss = 0.04424254\n",
            "Iteration 547, loss = 0.04392565\n",
            "Iteration 548, loss = 0.04371705\n",
            "Iteration 549, loss = 0.04399694\n",
            "Iteration 550, loss = 0.04384358\n",
            "Iteration 551, loss = 0.04379241\n",
            "Iteration 552, loss = 0.04325132\n",
            "Iteration 553, loss = 0.04316461\n",
            "Iteration 554, loss = 0.04308592\n",
            "Iteration 555, loss = 0.04290629\n",
            "Iteration 556, loss = 0.04274207\n",
            "Iteration 557, loss = 0.04271225\n",
            "Iteration 558, loss = 0.04260680\n",
            "Iteration 559, loss = 0.04253649\n",
            "Iteration 560, loss = 0.04234544\n",
            "Iteration 561, loss = 0.04230375\n",
            "Iteration 562, loss = 0.04225294\n",
            "Iteration 563, loss = 0.04199772\n",
            "Iteration 564, loss = 0.04202103\n",
            "Iteration 565, loss = 0.04201062\n",
            "Iteration 566, loss = 0.04166901\n",
            "Iteration 567, loss = 0.04143910\n",
            "Iteration 568, loss = 0.04153304\n",
            "Iteration 569, loss = 0.04144351\n",
            "Iteration 570, loss = 0.04114754\n",
            "Iteration 571, loss = 0.04114545\n",
            "Iteration 572, loss = 0.04093747\n",
            "Iteration 573, loss = 0.04098901\n",
            "Iteration 574, loss = 0.04122805\n",
            "Iteration 575, loss = 0.04079265\n",
            "Iteration 576, loss = 0.04078677\n",
            "Iteration 577, loss = 0.04056951\n",
            "Iteration 578, loss = 0.04040910\n",
            "Iteration 579, loss = 0.04048170\n",
            "Iteration 580, loss = 0.04044700\n",
            "Iteration 581, loss = 0.03997525\n",
            "Iteration 582, loss = 0.03987920\n",
            "Iteration 583, loss = 0.04000016\n",
            "Iteration 584, loss = 0.03983132\n",
            "Iteration 585, loss = 0.03976543\n",
            "Iteration 586, loss = 0.04030961\n",
            "Iteration 587, loss = 0.03989651\n",
            "Iteration 588, loss = 0.03955243\n",
            "Iteration 589, loss = 0.03936309\n",
            "Iteration 590, loss = 0.03907747\n",
            "Iteration 591, loss = 0.03954560\n",
            "Iteration 592, loss = 0.03959563\n",
            "Iteration 593, loss = 0.03912093\n",
            "Iteration 594, loss = 0.03901826\n",
            "Iteration 595, loss = 0.03917723\n",
            "Iteration 596, loss = 0.03887268\n",
            "Iteration 597, loss = 0.03849503\n",
            "Iteration 598, loss = 0.03887493\n",
            "Iteration 599, loss = 0.03849622\n",
            "Iteration 600, loss = 0.03838891\n",
            "Iteration 601, loss = 0.03822225\n",
            "Iteration 602, loss = 0.03797786\n",
            "Iteration 603, loss = 0.03805718\n",
            "Iteration 604, loss = 0.03793357\n",
            "Iteration 605, loss = 0.03783010\n",
            "Iteration 606, loss = 0.03767130\n",
            "Iteration 607, loss = 0.03768980\n",
            "Iteration 608, loss = 0.03761723\n",
            "Iteration 609, loss = 0.03772386\n",
            "Iteration 610, loss = 0.03738092\n",
            "Iteration 611, loss = 0.03737159\n",
            "Iteration 612, loss = 0.03727850\n",
            "Iteration 613, loss = 0.03714605\n",
            "Iteration 614, loss = 0.03695823\n",
            "Iteration 615, loss = 0.03702646\n",
            "Iteration 616, loss = 0.03700811\n",
            "Iteration 617, loss = 0.03713160\n",
            "Iteration 618, loss = 0.03654924\n",
            "Iteration 619, loss = 0.03685441\n",
            "Iteration 620, loss = 0.03693010\n",
            "Iteration 621, loss = 0.03656924\n",
            "Iteration 622, loss = 0.03634697\n",
            "Iteration 623, loss = 0.03624037\n",
            "Iteration 624, loss = 0.03628377\n",
            "Iteration 625, loss = 0.03627092\n",
            "Iteration 626, loss = 0.03627458\n",
            "Iteration 627, loss = 0.03611531\n",
            "Iteration 628, loss = 0.03585608\n",
            "Iteration 629, loss = 0.03593926\n",
            "Iteration 630, loss = 0.03578496\n",
            "Iteration 631, loss = 0.03563590\n",
            "Iteration 632, loss = 0.03563263\n",
            "Iteration 633, loss = 0.03554802\n",
            "Iteration 634, loss = 0.03542755\n",
            "Iteration 635, loss = 0.03535991\n",
            "Iteration 636, loss = 0.03527959\n",
            "Iteration 637, loss = 0.03564047\n",
            "Iteration 638, loss = 0.03550537\n",
            "Iteration 639, loss = 0.03529549\n",
            "Iteration 640, loss = 0.03489186\n",
            "Iteration 641, loss = 0.03534505\n",
            "Iteration 642, loss = 0.03560880\n",
            "Iteration 643, loss = 0.03561136\n",
            "Iteration 644, loss = 0.03484591\n",
            "Iteration 645, loss = 0.03470964\n",
            "Iteration 646, loss = 0.03489829\n",
            "Iteration 647, loss = 0.03479043\n",
            "Iteration 648, loss = 0.03470080\n",
            "Iteration 649, loss = 0.03435200\n",
            "Iteration 650, loss = 0.03457966\n",
            "Iteration 651, loss = 0.03439449\n",
            "Iteration 652, loss = 0.03431329\n",
            "Iteration 653, loss = 0.03440889\n",
            "Iteration 654, loss = 0.03404492\n",
            "Iteration 655, loss = 0.03402829\n",
            "Iteration 656, loss = 0.03404118\n",
            "Iteration 657, loss = 0.03403855\n",
            "Iteration 658, loss = 0.03428738\n",
            "Iteration 659, loss = 0.03378021\n",
            "Iteration 660, loss = 0.03351884\n",
            "Iteration 661, loss = 0.03381292\n",
            "Iteration 662, loss = 0.03372466\n",
            "Iteration 663, loss = 0.03368436\n",
            "Iteration 664, loss = 0.03367733\n",
            "Iteration 665, loss = 0.03344313\n",
            "Iteration 666, loss = 0.03369936\n",
            "Iteration 667, loss = 0.03369594\n",
            "Iteration 668, loss = 0.03338999\n",
            "Iteration 669, loss = 0.03280855\n",
            "Iteration 670, loss = 0.03294249\n",
            "Iteration 671, loss = 0.03453787\n",
            "Iteration 672, loss = 0.03413027\n",
            "Iteration 673, loss = 0.03303956\n",
            "Iteration 674, loss = 0.03325275\n",
            "Iteration 675, loss = 0.03302973\n",
            "Iteration 676, loss = 0.03294653\n",
            "Iteration 677, loss = 0.03271691\n",
            "Iteration 678, loss = 0.03261670\n",
            "Iteration 679, loss = 0.03268178\n",
            "Iteration 680, loss = 0.03287658\n",
            "Iteration 681, loss = 0.03263916\n",
            "Iteration 682, loss = 0.03237684\n",
            "Iteration 683, loss = 0.03215282\n",
            "Iteration 684, loss = 0.03237851\n",
            "Iteration 685, loss = 0.03220214\n",
            "Iteration 686, loss = 0.03188074\n",
            "Iteration 687, loss = 0.03186537\n",
            "Iteration 688, loss = 0.03225275\n",
            "Iteration 689, loss = 0.03207752\n",
            "Iteration 690, loss = 0.03211235\n",
            "Iteration 691, loss = 0.03157735\n",
            "Iteration 692, loss = 0.03153559\n",
            "Iteration 693, loss = 0.03147107\n",
            "Iteration 694, loss = 0.03137653\n",
            "Iteration 695, loss = 0.03137020\n",
            "Iteration 696, loss = 0.03118698\n",
            "Iteration 697, loss = 0.03109529\n",
            "Iteration 698, loss = 0.03117481\n",
            "Iteration 699, loss = 0.03099274\n",
            "Iteration 700, loss = 0.03082050\n",
            "Iteration 701, loss = 0.03076728\n",
            "Iteration 702, loss = 0.03089289\n",
            "Iteration 703, loss = 0.03078379\n",
            "Iteration 704, loss = 0.03053512\n",
            "Iteration 705, loss = 0.03058055\n",
            "Iteration 706, loss = 0.03048661\n",
            "Iteration 707, loss = 0.03070324\n",
            "Iteration 708, loss = 0.03050801\n",
            "Iteration 709, loss = 0.03048739\n",
            "Iteration 710, loss = 0.03020055\n",
            "Iteration 711, loss = 0.03081878\n",
            "Iteration 712, loss = 0.03041998\n",
            "Iteration 713, loss = 0.03005382\n",
            "Iteration 714, loss = 0.02974660\n",
            "Iteration 715, loss = 0.02994880\n",
            "Iteration 716, loss = 0.03020204\n",
            "Iteration 717, loss = 0.03016955\n",
            "Iteration 718, loss = 0.03022068\n",
            "Iteration 719, loss = 0.03044315\n",
            "Iteration 720, loss = 0.02970112\n",
            "Iteration 721, loss = 0.02966565\n",
            "Iteration 722, loss = 0.03050068\n",
            "Iteration 723, loss = 0.02949217\n",
            "Iteration 724, loss = 0.02944304\n",
            "Iteration 725, loss = 0.02978450\n",
            "Iteration 726, loss = 0.03045250\n",
            "Iteration 727, loss = 0.02994313\n",
            "Iteration 728, loss = 0.02953478\n",
            "Iteration 729, loss = 0.02940882\n",
            "Iteration 730, loss = 0.02964264\n",
            "Iteration 731, loss = 0.02955565\n",
            "Iteration 732, loss = 0.02912303\n",
            "Iteration 733, loss = 0.02937659\n",
            "Iteration 734, loss = 0.02957848\n",
            "Iteration 735, loss = 0.02933009\n",
            "Iteration 736, loss = 0.02870581\n",
            "Iteration 737, loss = 0.02932543\n",
            "Iteration 738, loss = 0.02931196\n",
            "Iteration 739, loss = 0.02926684\n",
            "Iteration 740, loss = 0.02898883\n",
            "Iteration 741, loss = 0.02867704\n",
            "Iteration 742, loss = 0.02886614\n",
            "Iteration 743, loss = 0.02873454\n",
            "Iteration 744, loss = 0.02866412\n",
            "Iteration 745, loss = 0.02844421\n",
            "Iteration 746, loss = 0.02915370\n",
            "Iteration 747, loss = 0.02865775\n",
            "Iteration 748, loss = 0.02887589\n",
            "Iteration 749, loss = 0.02852475\n",
            "Iteration 750, loss = 0.02876316\n",
            "Iteration 751, loss = 0.02835711\n",
            "Iteration 752, loss = 0.02835626\n",
            "Iteration 753, loss = 0.02825579\n",
            "Iteration 754, loss = 0.02808642\n",
            "Iteration 755, loss = 0.02807817\n",
            "Iteration 756, loss = 0.02832095\n",
            "Iteration 757, loss = 0.02796009\n",
            "Iteration 758, loss = 0.02785630\n",
            "Iteration 759, loss = 0.02794238\n",
            "Iteration 760, loss = 0.02819086\n",
            "Iteration 761, loss = 0.02836916\n",
            "Iteration 762, loss = 0.02799460\n",
            "Iteration 763, loss = 0.02801640\n",
            "Iteration 764, loss = 0.02842525\n",
            "Iteration 765, loss = 0.02778094\n",
            "Iteration 766, loss = 0.02771259\n",
            "Iteration 767, loss = 0.02806974\n",
            "Iteration 768, loss = 0.02784254\n",
            "Iteration 769, loss = 0.02767416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Acurácia: 0.7901234567901234\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.92      0.70      0.80        47\n",
            "   recurrence-events       0.69      0.91      0.78        34\n",
            "\n",
            "            accuracy                           0.79        81\n",
            "           macro avg       0.80      0.81      0.79        81\n",
            "        weighted avg       0.82      0.79      0.79        81\n",
            "\n",
            "[[33 14]\n",
            " [ 3 31]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Avaliando topologia: (30, 15)\n",
            "Iteration 1, loss = 0.71831802\n",
            "Iteration 2, loss = 0.70901913\n",
            "Iteration 3, loss = 0.70030484\n",
            "Iteration 4, loss = 0.69285478\n",
            "Iteration 5, loss = 0.68565540\n",
            "Iteration 6, loss = 0.67959472\n",
            "Iteration 7, loss = 0.67350666\n",
            "Iteration 8, loss = 0.66768038\n",
            "Iteration 9, loss = 0.66196458\n",
            "Iteration 10, loss = 0.65627697\n",
            "Iteration 11, loss = 0.65086945\n",
            "Iteration 12, loss = 0.64553062\n",
            "Iteration 13, loss = 0.64029283\n",
            "Iteration 14, loss = 0.63492663\n",
            "Iteration 15, loss = 0.62995740\n",
            "Iteration 16, loss = 0.62494761\n",
            "Iteration 17, loss = 0.62024915\n",
            "Iteration 18, loss = 0.61591232\n",
            "Iteration 19, loss = 0.61091917\n",
            "Iteration 20, loss = 0.60671001\n",
            "Iteration 21, loss = 0.60260520\n",
            "Iteration 22, loss = 0.59794523\n",
            "Iteration 23, loss = 0.59400046\n",
            "Iteration 24, loss = 0.58956028\n",
            "Iteration 25, loss = 0.58550344\n",
            "Iteration 26, loss = 0.58146794\n",
            "Iteration 27, loss = 0.57737758\n",
            "Iteration 28, loss = 0.57336783\n",
            "Iteration 29, loss = 0.56942077\n",
            "Iteration 30, loss = 0.56556171\n",
            "Iteration 31, loss = 0.56124065\n",
            "Iteration 32, loss = 0.55751426\n",
            "Iteration 33, loss = 0.55359420\n",
            "Iteration 34, loss = 0.54989255\n",
            "Iteration 35, loss = 0.54611545\n",
            "Iteration 36, loss = 0.54234888\n",
            "Iteration 37, loss = 0.53867747\n",
            "Iteration 38, loss = 0.53525969\n",
            "Iteration 39, loss = 0.53142082\n",
            "Iteration 40, loss = 0.52762236\n",
            "Iteration 41, loss = 0.52404120\n",
            "Iteration 42, loss = 0.52069662\n",
            "Iteration 43, loss = 0.51687153\n",
            "Iteration 44, loss = 0.51317476\n",
            "Iteration 45, loss = 0.50954263\n",
            "Iteration 46, loss = 0.50591736\n",
            "Iteration 47, loss = 0.50229357\n",
            "Iteration 48, loss = 0.49886666\n",
            "Iteration 49, loss = 0.49531275\n",
            "Iteration 50, loss = 0.49166020\n",
            "Iteration 51, loss = 0.48803225\n",
            "Iteration 52, loss = 0.48432065\n",
            "Iteration 53, loss = 0.48072422\n",
            "Iteration 54, loss = 0.47724418\n",
            "Iteration 55, loss = 0.47345662\n",
            "Iteration 56, loss = 0.46963797\n",
            "Iteration 57, loss = 0.46573090\n",
            "Iteration 58, loss = 0.46194525\n",
            "Iteration 59, loss = 0.45810158\n",
            "Iteration 60, loss = 0.45413847\n",
            "Iteration 61, loss = 0.45020821\n",
            "Iteration 62, loss = 0.44619577\n",
            "Iteration 63, loss = 0.44214045\n",
            "Iteration 64, loss = 0.43804594\n",
            "Iteration 65, loss = 0.43432445\n",
            "Iteration 66, loss = 0.43053475\n",
            "Iteration 67, loss = 0.42649468\n",
            "Iteration 68, loss = 0.42285535\n",
            "Iteration 69, loss = 0.41910818\n",
            "Iteration 70, loss = 0.41506669\n",
            "Iteration 71, loss = 0.41126993\n",
            "Iteration 72, loss = 0.40757141\n",
            "Iteration 73, loss = 0.40386371\n",
            "Iteration 74, loss = 0.39994515\n",
            "Iteration 75, loss = 0.39597335\n",
            "Iteration 76, loss = 0.39229375\n",
            "Iteration 77, loss = 0.38845376\n",
            "Iteration 78, loss = 0.38506255\n",
            "Iteration 79, loss = 0.38166037\n",
            "Iteration 80, loss = 0.37769143\n",
            "Iteration 81, loss = 0.37380403\n",
            "Iteration 82, loss = 0.37019429\n",
            "Iteration 83, loss = 0.36656892\n",
            "Iteration 84, loss = 0.36309420\n",
            "Iteration 85, loss = 0.35958855\n",
            "Iteration 86, loss = 0.35640262\n",
            "Iteration 87, loss = 0.35248545\n",
            "Iteration 88, loss = 0.34905797\n",
            "Iteration 89, loss = 0.34567306\n",
            "Iteration 90, loss = 0.34209899\n",
            "Iteration 91, loss = 0.33917031\n",
            "Iteration 92, loss = 0.33513819\n",
            "Iteration 93, loss = 0.33155877\n",
            "Iteration 94, loss = 0.32851681\n",
            "Iteration 95, loss = 0.32516564\n",
            "Iteration 96, loss = 0.32150924\n",
            "Iteration 97, loss = 0.31808699\n",
            "Iteration 98, loss = 0.31507029\n",
            "Iteration 99, loss = 0.31194580\n",
            "Iteration 100, loss = 0.30846798\n",
            "Iteration 101, loss = 0.30547273\n",
            "Iteration 102, loss = 0.30241878\n",
            "Iteration 103, loss = 0.29931305\n",
            "Iteration 104, loss = 0.29587118\n",
            "Iteration 105, loss = 0.29258010\n",
            "Iteration 106, loss = 0.28949840\n",
            "Iteration 107, loss = 0.28654117\n",
            "Iteration 108, loss = 0.28370309\n",
            "Iteration 109, loss = 0.28051132\n",
            "Iteration 110, loss = 0.27748424\n",
            "Iteration 111, loss = 0.27476327\n",
            "Iteration 112, loss = 0.27182200\n",
            "Iteration 113, loss = 0.26929124\n",
            "Iteration 114, loss = 0.26622539\n",
            "Iteration 115, loss = 0.26349234\n",
            "Iteration 116, loss = 0.26062183\n",
            "Iteration 117, loss = 0.25805893\n",
            "Iteration 118, loss = 0.25538115\n",
            "Iteration 119, loss = 0.25268085\n",
            "Iteration 120, loss = 0.25016649\n",
            "Iteration 121, loss = 0.24749886\n",
            "Iteration 122, loss = 0.24506094\n",
            "Iteration 123, loss = 0.24258490\n",
            "Iteration 124, loss = 0.23986599\n",
            "Iteration 125, loss = 0.23733705\n",
            "Iteration 126, loss = 0.23462435\n",
            "Iteration 127, loss = 0.23228132\n",
            "Iteration 128, loss = 0.22986390\n",
            "Iteration 129, loss = 0.22756757\n",
            "Iteration 130, loss = 0.22500151\n",
            "Iteration 131, loss = 0.22261648\n",
            "Iteration 132, loss = 0.22079240\n",
            "Iteration 133, loss = 0.21770374\n",
            "Iteration 134, loss = 0.21575191\n",
            "Iteration 135, loss = 0.21381486\n",
            "Iteration 136, loss = 0.21122970\n",
            "Iteration 137, loss = 0.20892612\n",
            "Iteration 138, loss = 0.20678594\n",
            "Iteration 139, loss = 0.20461063\n",
            "Iteration 140, loss = 0.20244898\n",
            "Iteration 141, loss = 0.20028551\n",
            "Iteration 142, loss = 0.19850184\n",
            "Iteration 143, loss = 0.19611387\n",
            "Iteration 144, loss = 0.19406127\n",
            "Iteration 145, loss = 0.19179464\n",
            "Iteration 146, loss = 0.18994562\n",
            "Iteration 147, loss = 0.18800333\n",
            "Iteration 148, loss = 0.18634713\n",
            "Iteration 149, loss = 0.18432135\n",
            "Iteration 150, loss = 0.18222360\n",
            "Iteration 151, loss = 0.18005878\n",
            "Iteration 152, loss = 0.17818250\n",
            "Iteration 153, loss = 0.17631036\n",
            "Iteration 154, loss = 0.17475373\n",
            "Iteration 155, loss = 0.17256177\n",
            "Iteration 156, loss = 0.17085366\n",
            "Iteration 157, loss = 0.16910050\n",
            "Iteration 158, loss = 0.16722648\n",
            "Iteration 159, loss = 0.16563532\n",
            "Iteration 160, loss = 0.16362013\n",
            "Iteration 161, loss = 0.16180189\n",
            "Iteration 162, loss = 0.16027899\n",
            "Iteration 163, loss = 0.15836688\n",
            "Iteration 164, loss = 0.15670766\n",
            "Iteration 165, loss = 0.15490745\n",
            "Iteration 166, loss = 0.15339872\n",
            "Iteration 167, loss = 0.15152640\n",
            "Iteration 168, loss = 0.14999332\n",
            "Iteration 169, loss = 0.14851700\n",
            "Iteration 170, loss = 0.14661499\n",
            "Iteration 171, loss = 0.14500728\n",
            "Iteration 172, loss = 0.14332372\n",
            "Iteration 173, loss = 0.14188046\n",
            "Iteration 174, loss = 0.14033971\n",
            "Iteration 175, loss = 0.13873598\n",
            "Iteration 176, loss = 0.13722824\n",
            "Iteration 177, loss = 0.13643431\n",
            "Iteration 178, loss = 0.13427062\n",
            "Iteration 179, loss = 0.13325102\n",
            "Iteration 180, loss = 0.13196188\n",
            "Iteration 181, loss = 0.13012251\n",
            "Iteration 182, loss = 0.12885646\n",
            "Iteration 183, loss = 0.12759340\n",
            "Iteration 184, loss = 0.12600842\n",
            "Iteration 185, loss = 0.12446059\n",
            "Iteration 186, loss = 0.12317179\n",
            "Iteration 187, loss = 0.12203314\n",
            "Iteration 188, loss = 0.12054586\n",
            "Iteration 189, loss = 0.11993392\n",
            "Iteration 190, loss = 0.11783424\n",
            "Iteration 191, loss = 0.11692313\n",
            "Iteration 192, loss = 0.11621823\n",
            "Iteration 193, loss = 0.11461782\n",
            "Iteration 194, loss = 0.11306658\n",
            "Iteration 195, loss = 0.11208681\n",
            "Iteration 196, loss = 0.11113914\n",
            "Iteration 197, loss = 0.10967453\n",
            "Iteration 198, loss = 0.10835896\n",
            "Iteration 199, loss = 0.10744334\n",
            "Iteration 200, loss = 0.10653117\n",
            "Iteration 201, loss = 0.10542314\n",
            "Iteration 202, loss = 0.10379347\n",
            "Iteration 203, loss = 0.10476558\n",
            "Iteration 204, loss = 0.10287150\n",
            "Iteration 205, loss = 0.10119123\n",
            "Iteration 206, loss = 0.10040774\n",
            "Iteration 207, loss = 0.09945017\n",
            "Iteration 208, loss = 0.09789737\n",
            "Iteration 209, loss = 0.09739937\n",
            "Iteration 210, loss = 0.09637316\n",
            "Iteration 211, loss = 0.09520399\n",
            "Iteration 212, loss = 0.09409055\n",
            "Iteration 213, loss = 0.09334897\n",
            "Iteration 214, loss = 0.09240873\n",
            "Iteration 215, loss = 0.09137552\n",
            "Iteration 216, loss = 0.09049322\n",
            "Iteration 217, loss = 0.08964366\n",
            "Iteration 218, loss = 0.08907331\n",
            "Iteration 219, loss = 0.08818223\n",
            "Iteration 220, loss = 0.08707455\n",
            "Iteration 221, loss = 0.08657787\n",
            "Iteration 222, loss = 0.08548984\n",
            "Iteration 223, loss = 0.08494950\n",
            "Iteration 224, loss = 0.08447098\n",
            "Iteration 225, loss = 0.08343361\n",
            "Iteration 226, loss = 0.08265013\n",
            "Iteration 227, loss = 0.08228565\n",
            "Iteration 228, loss = 0.08150982\n",
            "Iteration 229, loss = 0.08039357\n",
            "Iteration 230, loss = 0.07963308\n",
            "Iteration 231, loss = 0.07882544\n",
            "Iteration 232, loss = 0.07813675\n",
            "Iteration 233, loss = 0.07752414\n",
            "Iteration 234, loss = 0.07731704\n",
            "Iteration 235, loss = 0.07624659\n",
            "Iteration 236, loss = 0.07565513\n",
            "Iteration 237, loss = 0.07494029\n",
            "Iteration 238, loss = 0.07430713\n",
            "Iteration 239, loss = 0.07360608\n",
            "Iteration 240, loss = 0.07309659\n",
            "Iteration 241, loss = 0.07255222\n",
            "Iteration 242, loss = 0.07200536\n",
            "Iteration 243, loss = 0.07163539\n",
            "Iteration 244, loss = 0.07079124\n",
            "Iteration 245, loss = 0.06997100\n",
            "Iteration 246, loss = 0.06954192\n",
            "Iteration 247, loss = 0.06941765\n",
            "Iteration 248, loss = 0.06883030\n",
            "Iteration 249, loss = 0.06790447\n",
            "Iteration 250, loss = 0.06747687\n",
            "Iteration 251, loss = 0.06714720\n",
            "Iteration 252, loss = 0.06648321\n",
            "Iteration 253, loss = 0.06563388\n",
            "Iteration 254, loss = 0.06536745\n",
            "Iteration 255, loss = 0.06521254\n",
            "Iteration 256, loss = 0.06430085\n",
            "Iteration 257, loss = 0.06378258\n",
            "Iteration 258, loss = 0.06319762\n",
            "Iteration 259, loss = 0.06277132\n",
            "Iteration 260, loss = 0.06259057\n",
            "Iteration 261, loss = 0.06193530\n",
            "Iteration 262, loss = 0.06133276\n",
            "Iteration 263, loss = 0.06085287\n",
            "Iteration 264, loss = 0.06043604\n",
            "Iteration 265, loss = 0.05990971\n",
            "Iteration 266, loss = 0.05953961\n",
            "Iteration 267, loss = 0.05898294\n",
            "Iteration 268, loss = 0.05878341\n",
            "Iteration 269, loss = 0.05810525\n",
            "Iteration 270, loss = 0.05781358\n",
            "Iteration 271, loss = 0.05745783\n",
            "Iteration 272, loss = 0.05696189\n",
            "Iteration 273, loss = 0.05661212\n",
            "Iteration 274, loss = 0.05634443\n",
            "Iteration 275, loss = 0.05612572\n",
            "Iteration 276, loss = 0.05549727\n",
            "Iteration 277, loss = 0.05499720\n",
            "Iteration 278, loss = 0.05457363\n",
            "Iteration 279, loss = 0.05448983\n",
            "Iteration 280, loss = 0.05372298\n",
            "Iteration 281, loss = 0.05345028\n",
            "Iteration 282, loss = 0.05343944\n",
            "Iteration 283, loss = 0.05307141\n",
            "Iteration 284, loss = 0.05297601\n",
            "Iteration 285, loss = 0.05217028\n",
            "Iteration 286, loss = 0.05196813\n",
            "Iteration 287, loss = 0.05154898\n",
            "Iteration 288, loss = 0.05125001\n",
            "Iteration 289, loss = 0.05080241\n",
            "Iteration 290, loss = 0.05043395\n",
            "Iteration 291, loss = 0.05010498\n",
            "Iteration 292, loss = 0.04980766\n",
            "Iteration 293, loss = 0.04951171\n",
            "Iteration 294, loss = 0.04903696\n",
            "Iteration 295, loss = 0.04873507\n",
            "Iteration 296, loss = 0.04866321\n",
            "Iteration 297, loss = 0.04831243\n",
            "Iteration 298, loss = 0.04785014\n",
            "Iteration 299, loss = 0.04753760\n",
            "Iteration 300, loss = 0.04724165\n",
            "Iteration 301, loss = 0.04708889\n",
            "Iteration 302, loss = 0.04673105\n",
            "Iteration 303, loss = 0.04660658\n",
            "Iteration 304, loss = 0.04627361\n",
            "Iteration 305, loss = 0.04590607\n",
            "Iteration 306, loss = 0.04565038\n",
            "Iteration 307, loss = 0.04542653\n",
            "Iteration 308, loss = 0.04554489\n",
            "Iteration 309, loss = 0.04490262\n",
            "Iteration 310, loss = 0.04476284\n",
            "Iteration 311, loss = 0.04441368\n",
            "Iteration 312, loss = 0.04426414\n",
            "Iteration 313, loss = 0.04397674\n",
            "Iteration 314, loss = 0.04377136\n",
            "Iteration 315, loss = 0.04383636\n",
            "Iteration 316, loss = 0.04351552\n",
            "Iteration 317, loss = 0.04322147\n",
            "Iteration 318, loss = 0.04295194\n",
            "Iteration 319, loss = 0.04270160\n",
            "Iteration 320, loss = 0.04243638\n",
            "Iteration 321, loss = 0.04198937\n",
            "Iteration 322, loss = 0.04189495\n",
            "Iteration 323, loss = 0.04180235\n",
            "Iteration 324, loss = 0.04162379\n",
            "Iteration 325, loss = 0.04158248\n",
            "Iteration 326, loss = 0.04109795\n",
            "Iteration 327, loss = 0.04090633\n",
            "Iteration 328, loss = 0.04049662\n",
            "Iteration 329, loss = 0.04088249\n",
            "Iteration 330, loss = 0.04032792\n",
            "Iteration 331, loss = 0.03981906\n",
            "Iteration 332, loss = 0.03988860\n",
            "Iteration 333, loss = 0.03999970\n",
            "Iteration 334, loss = 0.03953748\n",
            "Iteration 335, loss = 0.03905116\n",
            "Iteration 336, loss = 0.03912155\n",
            "Iteration 337, loss = 0.03923535\n",
            "Iteration 338, loss = 0.03907320\n",
            "Iteration 339, loss = 0.03864617\n",
            "Iteration 340, loss = 0.03855667\n",
            "Iteration 341, loss = 0.03826202\n",
            "Iteration 342, loss = 0.03809141\n",
            "Iteration 343, loss = 0.03788680\n",
            "Iteration 344, loss = 0.03783256\n",
            "Iteration 345, loss = 0.03761811\n",
            "Iteration 346, loss = 0.03752183\n",
            "Iteration 347, loss = 0.03730908\n",
            "Iteration 348, loss = 0.03721338\n",
            "Iteration 349, loss = 0.03684893\n",
            "Iteration 350, loss = 0.03688852\n",
            "Iteration 351, loss = 0.03692563\n",
            "Iteration 352, loss = 0.03672449\n",
            "Iteration 353, loss = 0.03627250\n",
            "Iteration 354, loss = 0.03610696\n",
            "Iteration 355, loss = 0.03600800\n",
            "Iteration 356, loss = 0.03595625\n",
            "Iteration 357, loss = 0.03597762\n",
            "Iteration 358, loss = 0.03558130\n",
            "Iteration 359, loss = 0.03545514\n",
            "Iteration 360, loss = 0.03533200\n",
            "Iteration 361, loss = 0.03529120\n",
            "Iteration 362, loss = 0.03522381\n",
            "Iteration 363, loss = 0.03499960\n",
            "Iteration 364, loss = 0.03464889\n",
            "Iteration 365, loss = 0.03464291\n",
            "Iteration 366, loss = 0.03476919\n",
            "Iteration 367, loss = 0.03494790\n",
            "Iteration 368, loss = 0.03411963\n",
            "Iteration 369, loss = 0.03407839\n",
            "Iteration 370, loss = 0.03383778\n",
            "Iteration 371, loss = 0.03381464\n",
            "Iteration 372, loss = 0.03388279\n",
            "Iteration 373, loss = 0.03396378\n",
            "Iteration 374, loss = 0.03366198\n",
            "Iteration 375, loss = 0.03333353\n",
            "Iteration 376, loss = 0.03345116\n",
            "Iteration 377, loss = 0.03332476\n",
            "Iteration 378, loss = 0.03297823\n",
            "Iteration 379, loss = 0.03336383\n",
            "Iteration 380, loss = 0.03290246\n",
            "Iteration 381, loss = 0.03281410\n",
            "Iteration 382, loss = 0.03279742\n",
            "Iteration 383, loss = 0.03249919\n",
            "Iteration 384, loss = 0.03252356\n",
            "Iteration 385, loss = 0.03242729\n",
            "Iteration 386, loss = 0.03233034\n",
            "Iteration 387, loss = 0.03208122\n",
            "Iteration 388, loss = 0.03211920\n",
            "Iteration 389, loss = 0.03195017\n",
            "Iteration 390, loss = 0.03154742\n",
            "Iteration 391, loss = 0.03205370\n",
            "Iteration 392, loss = 0.03211649\n",
            "Iteration 393, loss = 0.03164082\n",
            "Iteration 394, loss = 0.03145728\n",
            "Iteration 395, loss = 0.03133726\n",
            "Iteration 396, loss = 0.03126703\n",
            "Iteration 397, loss = 0.03113190\n",
            "Iteration 398, loss = 0.03106942\n",
            "Iteration 399, loss = 0.03086914\n",
            "Iteration 400, loss = 0.03099114\n",
            "Iteration 401, loss = 0.03092083\n",
            "Iteration 402, loss = 0.03060008\n",
            "Iteration 403, loss = 0.03035610\n",
            "Iteration 404, loss = 0.03033475\n",
            "Iteration 405, loss = 0.03030650\n",
            "Iteration 406, loss = 0.03035346\n",
            "Iteration 407, loss = 0.03027663\n",
            "Iteration 408, loss = 0.03007017\n",
            "Iteration 409, loss = 0.03010272\n",
            "Iteration 410, loss = 0.03009322\n",
            "Iteration 411, loss = 0.02980184\n",
            "Iteration 412, loss = 0.02965789\n",
            "Iteration 413, loss = 0.02998092\n",
            "Iteration 414, loss = 0.02985578\n",
            "Iteration 415, loss = 0.02962060\n",
            "Iteration 416, loss = 0.02948569\n",
            "Iteration 417, loss = 0.02980425\n",
            "Iteration 418, loss = 0.02944548\n",
            "Iteration 419, loss = 0.02920591\n",
            "Iteration 420, loss = 0.02909501\n",
            "Iteration 421, loss = 0.02905710\n",
            "Iteration 422, loss = 0.02930669\n",
            "Iteration 423, loss = 0.02909840\n",
            "Iteration 424, loss = 0.02949285\n",
            "Iteration 425, loss = 0.02884486\n",
            "Iteration 426, loss = 0.02879715\n",
            "Iteration 427, loss = 0.02893074\n",
            "Iteration 428, loss = 0.02853719\n",
            "Iteration 429, loss = 0.02852173\n",
            "Iteration 430, loss = 0.02835609\n",
            "Iteration 431, loss = 0.02830595\n",
            "Iteration 432, loss = 0.02830868\n",
            "Iteration 433, loss = 0.02857325\n",
            "Iteration 434, loss = 0.02793323\n",
            "Iteration 435, loss = 0.02936765\n",
            "Iteration 436, loss = 0.02850686\n",
            "Iteration 437, loss = 0.02756346\n",
            "Iteration 438, loss = 0.02832283\n",
            "Iteration 439, loss = 0.02889679\n",
            "Iteration 440, loss = 0.02886471\n",
            "Iteration 441, loss = 0.02784395\n",
            "Iteration 442, loss = 0.02800401\n",
            "Iteration 443, loss = 0.02771108\n",
            "Iteration 444, loss = 0.02778384\n",
            "Iteration 445, loss = 0.02741365\n",
            "Iteration 446, loss = 0.02743730\n",
            "Iteration 447, loss = 0.02749087\n",
            "Iteration 448, loss = 0.02766165\n",
            "Iteration 449, loss = 0.02726894\n",
            "Iteration 450, loss = 0.02723659\n",
            "Iteration 451, loss = 0.02733357\n",
            "Iteration 452, loss = 0.02718966\n",
            "Iteration 453, loss = 0.02698831\n",
            "Iteration 454, loss = 0.02704840\n",
            "Iteration 455, loss = 0.02722694\n",
            "Iteration 456, loss = 0.02698783\n",
            "Iteration 457, loss = 0.02681153\n",
            "Iteration 458, loss = 0.02676088\n",
            "Iteration 459, loss = 0.02720386\n",
            "Iteration 460, loss = 0.02654759\n",
            "Iteration 461, loss = 0.02706531\n",
            "Iteration 462, loss = 0.02693884\n",
            "Iteration 463, loss = 0.02661891\n",
            "Iteration 464, loss = 0.02676099\n",
            "Iteration 465, loss = 0.02680095\n",
            "Iteration 466, loss = 0.02667869\n",
            "Iteration 467, loss = 0.02622194\n",
            "Iteration 468, loss = 0.02616236\n",
            "Iteration 469, loss = 0.02642504\n",
            "Iteration 470, loss = 0.02672683\n",
            "Iteration 471, loss = 0.02652383\n",
            "Iteration 472, loss = 0.02598498\n",
            "Iteration 473, loss = 0.02576580\n",
            "Iteration 474, loss = 0.02637120\n",
            "Iteration 475, loss = 0.02630013\n",
            "Iteration 476, loss = 0.02630091\n",
            "Iteration 477, loss = 0.02599271\n",
            "Iteration 478, loss = 0.02573759\n",
            "Iteration 479, loss = 0.02581830\n",
            "Iteration 480, loss = 0.02575365\n",
            "Iteration 481, loss = 0.02579048\n",
            "Iteration 482, loss = 0.02557647\n",
            "Iteration 483, loss = 0.02539797\n",
            "Iteration 484, loss = 0.02547264\n",
            "Iteration 485, loss = 0.02546548\n",
            "Iteration 486, loss = 0.02547273\n",
            "Iteration 487, loss = 0.02520168\n",
            "Iteration 488, loss = 0.02549162\n",
            "Iteration 489, loss = 0.02541945\n",
            "Iteration 490, loss = 0.02560571\n",
            "Iteration 491, loss = 0.02554815\n",
            "Iteration 492, loss = 0.02512794\n",
            "Iteration 493, loss = 0.02532371\n",
            "Iteration 494, loss = 0.02541472\n",
            "Iteration 495, loss = 0.02509569\n",
            "Iteration 496, loss = 0.02500002\n",
            "Iteration 497, loss = 0.02502012\n",
            "Iteration 498, loss = 0.02550182\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Acurácia: 0.8271604938271605\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.90      0.79      0.84        47\n",
            "   recurrence-events       0.75      0.88      0.81        34\n",
            "\n",
            "            accuracy                           0.83        81\n",
            "           macro avg       0.83      0.83      0.83        81\n",
            "        weighted avg       0.84      0.83      0.83        81\n",
            "\n",
            "[[37 10]\n",
            " [ 4 30]]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#avaliação do desempenho do modelo a partir de diferentes topologias\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "def avaliar_topologia(X_treino, y_treino, X_teste, y_teste, topologia):\n",
        "    print(f\"Avaliando topologia: {topologia}\")\n",
        "    modelo = MLPClassifier(hidden_layer_sizes=topologia, max_iter=1000, verbose=True)\n",
        "    modelo.fit(X_treino, y_treino)\n",
        "    previsoes = modelo.predict(X_teste)\n",
        "    \n",
        "    acuracia = accuracy_score(y_teste, previsoes)\n",
        "    relatorio = classification_report(y_teste, previsoes)\n",
        "    \n",
        "    print(f\"Acurácia: {acuracia}\")\n",
        "    print(relatorio)\n",
        "    print(confusion_matrix(y_teste, previsoes))\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "# Lista de topologias a serem avaliadas\n",
        "topologias = [\n",
        "    \n",
        "    (10,), (20,), (30,), (10, 5), (20, 10), (30, 15) # 1 camada oculta com 20 neurônios\n",
        "    # ... Adicione mais topologias conforme necessário\n",
        "]\n",
        "\n",
        "# Avaliação de cada topologia\n",
        "for topologia in topologias:\n",
        "    avaliar_topologia(X_treino, y_treino, X_teste, y_teste, topologia)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avaliando modelo com taxa de aprendizado 0.001 e 200 épocas...\n",
            "Acurácia: 0.6049382716049383\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.001 e 400 épocas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 0.6666666666666666\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.001 e 600 épocas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 0.691358024691358\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.001 e 800 épocas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 0.6666666666666666\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.001 e 1000 épocas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 0.654320987654321\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.01 e 200 épocas...\n",
            "Acurácia: 0.7283950617283951\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.01 e 400 épocas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/caiolima/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acurácia: 0.7530864197530864\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.01 e 600 épocas...\n",
            "Acurácia: 0.8271604938271605\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.01 e 800 épocas...\n",
            "Acurácia: 0.7407407407407407\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.01 e 1000 épocas...\n",
            "Acurácia: 0.7160493827160493\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.1 e 200 épocas...\n",
            "Acurácia: 0.691358024691358\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.1 e 400 épocas...\n",
            "Acurácia: 0.691358024691358\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.1 e 600 épocas...\n",
            "Acurácia: 0.6790123456790124\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.1 e 800 épocas...\n",
            "Acurácia: 0.7037037037037037\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.1 e 1000 épocas...\n",
            "Acurácia: 0.7037037037037037\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.2 e 200 épocas...\n",
            "Acurácia: 0.6666666666666666\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.2 e 400 épocas...\n",
            "Acurácia: 0.6296296296296297\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.2 e 600 épocas...\n",
            "Acurácia: 0.6419753086419753\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.2 e 800 épocas...\n",
            "Acurácia: 0.41975308641975306\n",
            "\n",
            "Avaliando modelo com taxa de aprendizado 0.2 e 1000 épocas...\n",
            "Acurácia: 0.6790123456790124\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "#relação do parâmetro taxa de aprendizado com quantidade de épocas\n",
        "# Taxas de aprendizado e números de épocas para avaliação\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.2]\n",
        "epochs = [200, 400, 600, 800, 1000]\n",
        "\n",
        "# Avaliação das combinações\n",
        "for lr in learning_rates:\n",
        "    for epoch in epochs:\n",
        "        print(f\"Avaliando modelo com taxa de aprendizado {lr} e {epoch} épocas...\")\n",
        "        modelo = MLPClassifier(\n",
        "            hidden_layer_sizes=(10,), \n",
        "            max_iter=epoch, \n",
        "            learning_rate_init=lr, \n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        modelo.fit(X_treino, y_treino)\n",
        "        previsoes = modelo.predict(X_teste)\n",
        "        acuracia = accuracy_score(y_teste, previsoes)\n",
        "        print(f\"Acurácia: {acuracia}\\n\")\n",
        "        # Aqui você pode adicionar mais métricas ou visualizações conforme necessário\n",
        "\n",
        "# Pode-se também querer plotar as curvas de aprendizado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csRtFY7lKr0N"
      },
      "source": [
        "**Veja como implementar o backpropagation em python:**\n",
        "https://www.askpython.com/python/examples/backpropagation-in-python\n",
        "https://www.deeplearningbook.com.br/algoritmo-backpropagation-em-python/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
