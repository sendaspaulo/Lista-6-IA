{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Text;class-att\n",
      "'BAHIA COCOA REVIEW Showers continued throughou...  alleviating the drought since early\\nJanuary a... \\nalthough normal humidity levels have not been... \\nComissaria Smith said in its weekly review.\\n... 221 bags\\nof 60 kilos making a cumulative total... \\nmiddlemen                                         exporters and processors.\\n    There are doubt... 750 to\\n1                           780 dlrs per tonne to ports to be named.\\n    N... 850 and 1     880 dlrs and at 35 and 45 dlrs\\nunder New York ...  Aug/Sept at 1                                     870     1                                                 875 and 1 880 dlrs\\nper tonne FOB.\\n    Routine sales of ... 340  4  345 and 4 350 dlrs.\\n    April/May butter went at 2.27 ti...  June/July\\nat 4 400 and 4 415 dlrs  Aug/Sept at 4 351 to 4 450 dlrs and at\\n2.27 and 2.28 times New York S... 480 dlrs and\\n2.27 times New York Dec  Comissaria Smith said.\\n    Destinations were ...  Covertible currency areas \\nUruguay and open ports.\\n    Cake sales were ...  785 dlrs for May  753 dlrs for Aug and 0.39 times\\nNew York Dec ...  Argentina  Uruguay and convertible\\ncurrency areas.\\n    ... 325\\nand 2 380 dlrs  June/July at 2 375 dlrs and at 1.25 times New\\nYork July  Aug/Sept at 2 400 dlrs and at 1.25 times New York\\nSept and O...   Comissaria Smith\\nsaid.\\n    Total Bahia sale...\n",
      "'NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESER...  II and III have matured. Level IV reflects\\ngr...  1981 for feedgrain and after July\\n23              1981 for wheat. Level V wheat/barley after 5/1... \\ncorn/sorghum after 7/1/82. Level VI covers wh...  1984.  X-1986 rates. Y-dlrs per CWT (100 lbs).... NaN                                                NaN                                 NaN                                                NaN           NaN                                                NaN                                                NaN    NaN                                                NaN       NaN                                                NaN NaN NaN       NaN                                                NaN              NaN       NaN      NaN            NaN      NaN                                                NaN                                   NaN                                                NaN                        NaN                                                NaN               NaN                                                NaN        NaN                                                NaN        NaN      NaN             NaN                                       NaN            NaN                                                                                               NaN\n",
      "'CHAMPION PRODUCTS &lt.CH> APPROVES STOCK SPLIT...  1987.\\n    The company also said its board vot... NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                 NaN                                                NaN           NaN                                                NaN                                                NaN    NaN                                                NaN       NaN                                                NaN NaN NaN       NaN                                                NaN              NaN       NaN      NaN            NaN      NaN                                                NaN                                   NaN                                                NaN                        NaN                                                NaN               NaN                                                NaN        NaN                                                NaN        NaN      NaN             NaN                                       NaN            NaN                                                                                               NaN\n",
      "'COMPUTER TERMINAL SYSTEMS &lt.CPML> COMPLETES ... 000 shares of its common\\nstock                     and warrants to acquire an additional one mln ...  to\\n&lt.Sedio N.V.> of Lugano                      Switzerland for 50                                000 dlrs.\\n    The company said the warrants ar...  not to exceed 1.50 dlrs per share.\\n    Comput...  including any future\\nimprovements  to &lt.Woodco Inc> of Houston                      Tex. for 200 000\\ndlrs. But                                      it said it would continue to be the exclusive\\...  forms \\ntags and ticket printers and terminals.\\n Reu... NaN       NaN                                                NaN NaN NaN       NaN                                                NaN              NaN       NaN      NaN            NaN      NaN                                                NaN                                   NaN                                                NaN                        NaN                                                NaN               NaN                                                NaN        NaN                                                NaN        NaN      NaN             NaN                                       NaN            NaN                                                                                               NaN\n",
      "'COBANCO INC &lt.CBCO> YEAR NET Shr 34 cts vs 1... 000 vs 2                                           858                                                000\\n    Assets 510.2 mln vs 479.7 mln\\n    Dep... 000 dlrs                                            or\\nfive cts per shr.\\n Reuter\\n&#3.';0           NaN                                                NaN                                 NaN                                                NaN           NaN                                                NaN                                                NaN    NaN                                                NaN       NaN                                                NaN NaN NaN       NaN                                                NaN              NaN       NaN      NaN            NaN      NaN                                                NaN                                   NaN                                                NaN                        NaN                                                NaN               NaN                                                NaN        NaN                                                NaN        NaN      NaN             NaN                                       NaN            NaN                                                                                               NaN\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text;class-att\n",
      "'ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...  businessmen and officials said.\\n    They told...  in the short-term Tokyo\\'s loss might be\\nthei...  in\\nretaliation for Japan\\'s alleged failure t... \\\" said a spokesman for\\nleading Japanese elect... \\\" said Tom\\nMurtha                                 a stock analyst at the Tokyo office of broker ...  businessmen and officials are also worried.\\n ... \\\" said a senior\\nTaiwanese trade official who ...  95 pct of it with the U.S.\\n    The surplus he...  among the world\\'s largest.\\n    \\\"We must qui...  remove trade barriers and\\ncut import tariffs ...  if we\\nwant to defuse problems from possible U... \\\" said\\nPaul Sheen  chairman of textile exporters &ltTaiwan Safe G...  whose chief exports\\nare similar to those of J...  Up from 4.9 billion dlrs in 1985.\\n    In Mala...  trade officers and businessmen said tough\\ncur...  where newspapers have alleged Japan has been\\n...  some electronics\\nmanufacturers share that vie... \\\" said Lawrence Mills \\ndirector-general of the Federation of Hong Ko...  one day it will\\nbe extended to other sources.... \\\" he said.\\n    The U.S. Last year was Hong Ko... \\naccounting for over 30 pct of domestically pr... \\nIndustry Minister John Button said in Canberr... \\\" Button said.\\n    He said Australia\\'s conce... \\nAustralia\\'s two largest exports to Japan and...  Japan\\'s deputy minister of International Trad...   are due to meet in Washington this week in an...\n",
      "'CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...  the China Daily said.\\n    It also said that e...  or 25 pct                                          of\\nChina\\'s fruit output are left to rot          and 2.1 mln tonnes                                 or up\\nto 30 pct                                   of its vegetables. The paper blamed the waste ...  calling for improved technology in storage and...  and greater production of additives. The paper... NaN                                                NaN                                                NaN                                                NaN                                                NaN                 NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                    NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n",
      "'JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...  ministry officials said.\\n    MITI is expected...  they said.\\n    The decision follows the emerg...  the officials said.\\n    They said MITI will a...  including oil                                      nuclear                                            coal and natural gas.\\n    Nuclear energy prov...  supplying an estimated 27\\npct on a kilowatt/h...  followed by oil (23 pct) and\\nliquefied natura...  they noted.\\n REUTER\\n&#3';0                      NaN                                                NaN                                                NaN                                                NaN                 NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                    NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n",
      "'THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...  the Business Economics Department said.\\n    I...  however                                            fell 23 pct in the\\nfirst quarter due to lower...  maize 66 pct                                       sugar 45 pct                                       tin 26\\npct and canned pineapples seven pct.\\n...  clothing 57 pct and rubber 35 pct.\\n REUTER\\n&... NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                 NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                    NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n",
      "'INDONESIA SEES CPO PRICE RISING SHARPLY Indone...  Hasrul Harahap                                     junior minister for tree\\ncrops                    told Indonesian reporters.\\n    Prices of Mala...  traders said.\\n    Harahap said Indonesia woul...  despite\\nmaking recent palm oil purchases from...  so that it\\ncould possibly increase its intern...  the world\\'s second largest producer of palm o...  has been forced to import palm oil to ensure\\n... 500\\ntonnes                                         against 468                                       500 in 1985                                         according to central bank\\nfigures.\\n REUTER\\n... NaN                 NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                    NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                                                               NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar dados com opções adicionais para lidar com linhas problemáticas\n",
    "try:\n",
    "    train_data = pd.read_csv('ReutersGrain-train.csv', on_bad_lines='skip')\n",
    "    test_data = pd.read_csv('ReutersGrain-test.csv', on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os dados: {e}\")\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\phpal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReutersGrain-test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Aplicar pré-processamento nos dados de treino e teste\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(preprocess)\n\u001b[0;32m     49\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Vetorização TF-IDF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phpal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\phpal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Baixar os pacotes necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Função de pré-processamento\n",
    "def preprocess(text):\n",
    "    # Converte o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenização\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remoção de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Reúne os tokens em uma string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Carregar dados\n",
    "train_data = pd.read_csv('ReutersGrain-train.csv', on_bad_lines='skip')\n",
    "test_data = pd.read_csv('ReutersGrain-test.csv', on_bad_lines='skip')\n",
    "\n",
    "# Aplicar pré-processamento nos dados de treino e teste\n",
    "train_data['text'] = train_data['text'].apply(preprocess)\n",
    "test_data['text'] = test_data['text'].apply(preprocess)\n",
    "\n",
    "# Vetorização TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Rótulos\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliação\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 90.56%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Baixar os pacotes necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Função de pré-processamento\n",
    "def preprocess(text):\n",
    "    # Converte o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenização\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remoção de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Reúne os tokens em uma string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Carregar dados\n",
    "train_data = pd.read_csv('ReutersGrain-train.csv', sep=';', on_bad_lines='skip')\n",
    "test_data = pd.read_csv('ReutersGrain-test.csv', sep=';', on_bad_lines='skip')\n",
    "\n",
    "# Ajustar nomes das colunas\n",
    "train_data.rename(columns={'Text': 'content', 'classatt': 'label'}, inplace=True)\n",
    "test_data.rename(columns={'Text': 'content', 'classatt': 'label'}, inplace=True)\n",
    "\n",
    "# Aplicar pré-processamento nos dados de treino e teste\n",
    "train_data['content'] = train_data['content'].apply(preprocess)\n",
    "test_data['content'] = test_data['content'].apply(preprocess)\n",
    "\n",
    "# Vetorização TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['content'])\n",
    "X_test = vectorizer.transform(test_data['content'])\n",
    "\n",
    "# Rótulos\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliação\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\phpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da Floresta Aleatória: 93.87%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Baixar os pacotes necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Função de pré-processamento\n",
    "def preprocess(text):\n",
    "    # Converte o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenização\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remoção de stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Reúne os tokens em uma string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Carregar dados\n",
    "train_data = pd.read_csv('ReutersGrain-train.csv', sep=';', on_bad_lines='skip')\n",
    "test_data = pd.read_csv('ReutersGrain-test.csv', sep=';', on_bad_lines='skip')\n",
    "\n",
    "# Ajustar nomes das colunas\n",
    "train_data.rename(columns={'Text': 'content', 'classatt': 'label'}, inplace=True)\n",
    "test_data.rename(columns={'Text': 'content', 'classatt': 'label'}, inplace=True)\n",
    "\n",
    "# Aplicar pré-processamento nos dados de treino e teste\n",
    "train_data['content'] = train_data['content'].apply(preprocess)\n",
    "test_data['content'] = test_data['content'].apply(preprocess)\n",
    "\n",
    "# Vetorização TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['content'])\n",
    "X_test = vectorizer.transform(test_data['content'])\n",
    "\n",
    "# Rótulos\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Treinamento do modelo de Floresta Aleatória\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliação\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia da Floresta Aleatória: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
